Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 265, MAX_LENGTH = 252, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
42022
Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 0 	 loss 537.730842590332
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1051.0000      116.0       1167.0  ...     0.9006  0.6182  0.7332
Arg1         2164.0000      531.0       2695.0  ...     0.8030  0.5366  0.6433
Arg2          839.0000      542.0       1381.0  ...     0.6075  0.5189  0.5597
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV       75.0000       95.0        170.0  ...     0.4412  0.1765  0.2521
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        3.0000        3.0          6.0  ...     0.5000  0.0184  0.0355
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        8.0000       10.0         18.0  ...     0.4444  0.0690  0.1194
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG       39.0000        6.0         45.0  ...     0.8667  0.4937  0.6290
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       16.0000       26.0         42.0  ...     0.3810  0.0734  0.1231
sum          4195.0000     1329.0       5524.0  ...     4.9443  2.5046  3.0953
precision       0.7594        NaN          NaN  ...        NaN     NaN     NaN
recall          0.4677        NaN          NaN  ...        NaN     NaN     NaN
f1              0.5789        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.1997        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.5789 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 1 	 loss 304.1792175475857
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1312.0000      260.0       1572.0  ...     0.8346  0.7718  0.8020
Arg1         2725.0000      582.0       3307.0  ...     0.8240  0.6757  0.7425
Arg2          957.0000      440.0       1397.0  ...     0.6850  0.5918  0.6350
Arg3           14.0000        0.0         14.0  ...     1.0000  0.1359  0.2393
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      143.0000      123.0        266.0  ...     0.5376  0.3365  0.4139
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       10.0000        9.0         19.0  ...     0.5263  0.1429  0.2247
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       30.0000       26.0         56.0  ...     0.5357  0.1840  0.2740
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       21.0000       17.0         38.0  ...     0.5526  0.1810  0.2727
ArgM_MNS        2.0000        0.0          2.0  ...     1.0000  0.0667  0.1250
ArgM_NEG       58.0000       20.0         78.0  ...     0.7436  0.7342  0.7389
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX       15.0000        5.0         20.0  ...     0.7500  0.1705  0.2778
ArgM_REC       32.0000        1.0         33.0  ...     0.9697  0.5000  0.6598
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       73.0000       62.0        135.0  ...     0.5407  0.3349  0.4136
sum          5392.0000     1545.0       6937.0  ...     9.4999  4.8258  5.8192
precision       0.7773        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6012        NaN          NaN  ...        NaN     NaN     NaN
f1              0.6780        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.3754        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.678 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 2 	 loss 205.514308473852
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1358.0000      325.0       1683.0  ...     0.8069  0.7988  0.8028
Arg1         2931.0000      650.0       3581.0  ...     0.8185  0.7268  0.7699
Arg2         1000.0000      441.0       1441.0  ...     0.6940  0.6184  0.6540
Arg3           16.0000        4.0         20.0  ...     0.8000  0.1553  0.2602
Arg4            3.0000        0.0          3.0  ...     1.0000  0.0698  0.1304
Arg5            1.0000        1.0          2.0  ...     0.5000  0.0588  0.1053
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      167.0000      122.0        289.0  ...     0.5779  0.3929  0.4678
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       17.0000       23.0         40.0  ...     0.4250  0.2429  0.3091
ArgM_CMP        1.0000        0.0          1.0  ...     1.0000  0.0455  0.0870
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       44.0000       28.0         72.0  ...     0.6111  0.2699  0.3745
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       34.0000       28.0         62.0  ...     0.5484  0.2931  0.3820
ArgM_MNS        5.0000        9.0         14.0  ...     0.3571  0.1667  0.2273
ArgM_NEG       62.0000       17.0         79.0  ...     0.7848  0.7848  0.7848
ArgM_PRP        6.0000        0.0          6.0  ...     1.0000  0.2069  0.3429
ArgM_PRX       17.0000        7.0         24.0  ...     0.7083  0.1932  0.3036
ArgM_REC       41.0000        3.0         44.0  ...     0.9318  0.6406  0.7593
ArgM_SCP        4.0000        7.0         11.0  ...     0.3636  0.0870  0.1404
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       95.0000       76.0        171.0  ...     0.5556  0.4358  0.4884
sum          5802.0000     1741.0       7543.0  ...    12.4830  6.1871  7.3895
precision       0.7692        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6469        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7028        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.4767        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.7028 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 3 	 loss 153.51527459608042
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1406.0000      369.0       1775.0  ...     0.7921  0.8271  0.8092
Arg1         2912.0000      646.0       3558.0  ...     0.8184  0.7220  0.7672
Arg2          972.0000      364.0       1336.0  ...     0.7275  0.6011  0.6583
Arg3           18.0000        5.0         23.0  ...     0.7826  0.1748  0.2857
Arg4            4.0000        6.0         10.0  ...     0.4000  0.0930  0.1509
Arg5            2.0000        3.0          5.0  ...     0.4000  0.1176  0.1818
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      185.0000      112.0        297.0  ...     0.6229  0.4353  0.5125
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       24.0000       19.0         43.0  ...     0.5581  0.3429  0.4248
ArgM_CMP        3.0000        4.0          7.0  ...     0.4286  0.1364  0.2069
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        6.0000        0.0          6.0  ...     1.0000  0.2143  0.3529
ArgM_EXT        2.0000        2.0          4.0  ...     0.5000  0.0426  0.0784
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       51.0000       25.0         76.0  ...     0.6711  0.3129  0.4268
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       37.0000       50.0         87.0  ...     0.4253  0.3190  0.3645
ArgM_MNS        8.0000       17.0         25.0  ...     0.3200  0.2667  0.2909
ArgM_NEG       68.0000       20.0         88.0  ...     0.7727  0.8608  0.8144
ArgM_PRP        6.0000        2.0          8.0  ...     0.7500  0.2069  0.3243
ArgM_PRX       28.0000       12.0         40.0  ...     0.7000  0.3182  0.4375
ArgM_REC       42.0000        4.0         46.0  ...     0.9130  0.6562  0.7636
ArgM_SCP        5.0000       23.0         28.0  ...     0.1786  0.1087  0.1351
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      100.0000       53.0        153.0  ...     0.6536  0.4587  0.5391
sum          5879.0000     1736.0       7615.0  ...    12.4146  7.2150  8.5250
precision       0.7720        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6555        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7090        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.5500        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.709 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 4 	 loss 117.11897747701732
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1383.0000      305.0       1688.0  ...     0.8193  0.8135  0.8164
Arg1         2941.0000      662.0       3603.0  ...     0.8163  0.7292  0.7703
Arg2          990.0000      361.0       1351.0  ...     0.7328  0.6122  0.6671
Arg3           18.0000       12.0         30.0  ...     0.6000  0.1748  0.2707
Arg4            7.0000       10.0         17.0  ...     0.4118  0.1628  0.2333
Arg5            5.0000        1.0          6.0  ...     0.8333  0.2941  0.4348
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      221.0000      146.0        367.0  ...     0.6022  0.5200  0.5581
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       26.0000       23.0         49.0  ...     0.5306  0.3714  0.4370
ArgM_CMP        4.0000        6.0         10.0  ...     0.4000  0.1818  0.2500
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        7.0000        1.0          8.0  ...     0.8750  0.2500  0.3889
ArgM_EXT        4.0000        7.0         11.0  ...     0.3636  0.0851  0.1379
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       32.0000       15.0         47.0  ...     0.6809  0.1963  0.3048
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       41.0000       36.0         77.0  ...     0.5325  0.3534  0.4249
ArgM_MNS        7.0000       13.0         20.0  ...     0.3500  0.2333  0.2800
ArgM_NEG       67.0000       12.0         79.0  ...     0.8481  0.8481  0.8481
ArgM_PRP        7.0000        3.0         10.0  ...     0.7000  0.2414  0.3590
ArgM_PRX       26.0000       17.0         43.0  ...     0.6047  0.2955  0.3969
ArgM_REC       40.0000        3.0         43.0  ...     0.9302  0.6250  0.7477
ArgM_SCP        6.0000       21.0         27.0  ...     0.2222  0.1304  0.1644
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      126.0000       81.0        207.0  ...     0.6087  0.5780  0.5929
sum          5958.0000     1735.0       7693.0  ...    12.4621  7.6965  9.0831
precision       0.7745        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6643        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7152        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.5860        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.7152 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 5 	 loss 96.04506228857645
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1378.0000      298.0       1676.0  ...     0.8222  0.8106   0.8164
Arg1         3040.0000      631.0       3671.0  ...     0.8281  0.7538   0.7892
Arg2         1028.0000      377.0       1405.0  ...     0.7317  0.6357   0.6803
Arg3           23.0000       12.0         35.0  ...     0.6571  0.2233   0.3333
Arg4           11.0000        8.0         19.0  ...     0.5789  0.2558   0.3548
Arg5            5.0000        1.0          6.0  ...     0.8333  0.2941   0.4348
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      191.0000      107.0        298.0  ...     0.6409  0.4494   0.5284
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       24.0000       19.0         43.0  ...     0.5581  0.3429   0.4248
ArgM_CMP        5.0000       11.0         16.0  ...     0.3125  0.2273   0.2632
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        0.0          1.0  ...     1.0000  0.1250   0.2222
ArgM_DIR        9.0000        3.0         12.0  ...     0.7500  0.3214   0.4500
ArgM_EXT        8.0000       11.0         19.0  ...     0.4211  0.1702   0.2424
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       81.0000       39.0        120.0  ...     0.6750  0.4969   0.5724
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       47.0000       43.0         90.0  ...     0.5222  0.4052   0.4563
ArgM_MNS        8.0000       15.0         23.0  ...     0.3478  0.2667   0.3019
ArgM_NEG       68.0000       15.0         83.0  ...     0.8193  0.8608   0.8395
ArgM_PRP        9.0000        5.0         14.0  ...     0.6429  0.3103   0.4186
ArgM_PRX       30.0000       23.0         53.0  ...     0.5660  0.3409   0.4255
ArgM_REC       38.0000        2.0         40.0  ...     0.9500  0.5938   0.7308
ArgM_SCP        8.0000       27.0         35.0  ...     0.2286  0.1739   0.1975
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      127.0000       72.0        199.0  ...     0.6382  0.5826   0.6091
sum          6139.0000     1719.0       7858.0  ...    13.5240  8.6405  10.0915
precision       0.7812        NaN          NaN  ...        NaN     NaN      NaN
recall          0.6845        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7297        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6511        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7297 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 6 	 loss 78.0950909037274
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1412.0000      339.0       1751.0  ...     0.8064  0.8306   0.8183
Arg1         3106.0000      668.0       3774.0  ...     0.8230  0.7701   0.7957
Arg2         1062.0000      423.0       1485.0  ...     0.7152  0.6568   0.6847
Arg3           24.0000       16.0         40.0  ...     0.6000  0.2330   0.3357
Arg4           10.0000       10.0         20.0  ...     0.5000  0.2326   0.3175
Arg5            6.0000        2.0          8.0  ...     0.7500  0.3529   0.4800
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      201.0000      104.0        305.0  ...     0.6590  0.4729   0.5507
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       28.0000       30.0         58.0  ...     0.4828  0.4000   0.4375
ArgM_CMP        5.0000       11.0         16.0  ...     0.3125  0.2273   0.2632
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        2.0          3.0  ...     0.3333  0.1250   0.1818
ArgM_DIR        9.0000       10.0         19.0  ...     0.4737  0.3214   0.3830
ArgM_EXT       10.0000       16.0         26.0  ...     0.3846  0.2128   0.2740
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       63.0000       30.0         93.0  ...     0.6774  0.3865   0.4922
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       54.0000       64.0        118.0  ...     0.4576  0.4655   0.4615
ArgM_MNS        7.0000        8.0         15.0  ...     0.4667  0.2333   0.3111
ArgM_NEG       69.0000       20.0         89.0  ...     0.7753  0.8734   0.8214
ArgM_PRP        8.0000        6.0         14.0  ...     0.5714  0.2759   0.3721
ArgM_PRX       31.0000       27.0         58.0  ...     0.5345  0.3523   0.4247
ArgM_REC       44.0000        0.0         44.0  ...     1.0000  0.6875   0.8148
ArgM_SCP       10.0000       30.0         40.0  ...     0.2500  0.2174   0.2326
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      132.0000       83.0        215.0  ...     0.6140  0.6055   0.6097
sum          6292.0000     1899.0       8191.0  ...    12.1873  8.9327  10.0621
precision       0.7682        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7015        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7333        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6492        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7333 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 7 	 loss 72.59088859778512
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1434.0000      377.0       1811.0  ...     0.7918  0.8435   0.8169
Arg1         3091.0000      666.0       3757.0  ...     0.8227  0.7664   0.7936
Arg2         1071.0000      414.0       1485.0  ...     0.7212  0.6623   0.6905
Arg3           25.0000       14.0         39.0  ...     0.6410  0.2427   0.3521
Arg4           10.0000        8.0         18.0  ...     0.5556  0.2326   0.3279
Arg5            6.0000        0.0          6.0  ...     1.0000  0.3529   0.5217
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      220.0000      145.0        365.0  ...     0.6027  0.5176   0.5570
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       30.0000       32.0         62.0  ...     0.4839  0.4286   0.4545
ArgM_CMP        5.0000       12.0         17.0  ...     0.2941  0.2273   0.2564
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        1.0          2.0  ...     0.5000  0.1250   0.2000
ArgM_DIR       13.0000        6.0         19.0  ...     0.6842  0.4643   0.5532
ArgM_EXT        8.0000       13.0         21.0  ...     0.3810  0.1702   0.2353
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       82.0000       43.0        125.0  ...     0.6560  0.5031   0.5694
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       56.0000       51.0        107.0  ...     0.5234  0.4828   0.5022
ArgM_MNS        6.0000       12.0         18.0  ...     0.3333  0.2000   0.2500
ArgM_NEG       67.0000       18.0         85.0  ...     0.7882  0.8481   0.8171
ArgM_PRP       10.0000        6.0         16.0  ...     0.6250  0.3448   0.4444
ArgM_PRX       33.0000       29.0         62.0  ...     0.5323  0.3750   0.4400
ArgM_REC       44.0000        8.0         52.0  ...     0.8462  0.6875   0.7586
ArgM_SCP        9.0000       40.0         49.0  ...     0.1837  0.1957   0.1895
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      147.0000       82.0        229.0  ...     0.6419  0.6743   0.6577
sum          6368.0000     1977.0       8345.0  ...    12.6082  9.3447  10.3881
precision       0.7631        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7100        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7356        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6702        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7356 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 8 	 loss 61.23969194867868
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1447.0000      365.0       1812.0  ...     0.7986  0.8512   0.8240
Arg1         3120.0000      670.0       3790.0  ...     0.8232  0.7736   0.7976
Arg2         1074.0000      415.0       1489.0  ...     0.7213  0.6642   0.6916
Arg3           19.0000        7.0         26.0  ...     0.7308  0.1845   0.2946
Arg4            9.0000        7.0         16.0  ...     0.5625  0.2093   0.3051
Arg5            4.0000        1.0          5.0  ...     0.8000  0.2353   0.3636
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      226.0000      149.0        375.0  ...     0.6027  0.5318   0.5650
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       32.0000       34.0         66.0  ...     0.4848  0.4571   0.4706
ArgM_CMP        5.0000       12.0         17.0  ...     0.2941  0.2273   0.2564
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        0.0          2.0  ...     1.0000  0.2500   0.4000
ArgM_DIR       14.0000       11.0         25.0  ...     0.5600  0.5000   0.5283
ArgM_EXT       14.0000       25.0         39.0  ...     0.3590  0.2979   0.3256
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       77.0000       57.0        134.0  ...     0.5746  0.4724   0.5185
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       57.0000       63.0        120.0  ...     0.4750  0.4914   0.4831
ArgM_MNS        8.0000       14.0         22.0  ...     0.3636  0.2667   0.3077
ArgM_NEG       67.0000       18.0         85.0  ...     0.7882  0.8481   0.8171
ArgM_PRP       12.0000        7.0         19.0  ...     0.6316  0.4138   0.5000
ArgM_PRX       37.0000       29.0         66.0  ...     0.5606  0.4205   0.4805
ArgM_REC       42.0000        3.0         45.0  ...     0.9333  0.6562   0.7706
ArgM_SCP        7.0000       34.0         41.0  ...     0.1707  0.1522   0.1609
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      134.0000       88.0        222.0  ...     0.6036  0.6147   0.6091
sum          6408.0000     2010.0       8418.0  ...    13.8383  9.7180  10.8033
precision       0.7612        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7145        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7371        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6970        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7371 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 9 	 loss 56.59169330140503
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1360.0000      275.0       1635.0  ...     0.8318  0.8000  0.8156
Arg1         2911.0000      669.0       3580.0  ...     0.8131  0.7218  0.7647
Arg2         1050.0000      475.0       1525.0  ...     0.6885  0.6494  0.6684
Arg3           15.0000        7.0         22.0  ...     0.6818  0.1456  0.2400
Arg4           13.0000       11.0         24.0  ...     0.5417  0.3023  0.3881
Arg5            7.0000        0.0          7.0  ...     1.0000  0.4118  0.5833
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      166.0000       88.0        254.0  ...     0.6535  0.3906  0.4890
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       26.0000       21.0         47.0  ...     0.5532  0.3714  0.4444
ArgM_CMP        4.0000        8.0         12.0  ...     0.3333  0.1818  0.2353
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        1.0000        0.0          1.0  ...     1.0000  0.1250  0.2222
ArgM_DIR       12.0000        9.0         21.0  ...     0.5714  0.4286  0.4898
ArgM_EXT       14.0000       25.0         39.0  ...     0.3590  0.2979  0.3256
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       70.0000       34.0        104.0  ...     0.6731  0.4294  0.5243
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       48.0000       46.0         94.0  ...     0.5106  0.4138  0.4571
ArgM_MNS        5.0000       10.0         15.0  ...     0.3333  0.1667  0.2222
ArgM_NEG       64.0000       14.0         78.0  ...     0.8205  0.8101  0.8153
ArgM_PRP       11.0000        7.0         18.0  ...     0.6111  0.3793  0.4681
ArgM_PRX       30.0000       11.0         41.0  ...     0.7317  0.3409  0.4651
ArgM_REC       32.0000        0.0         32.0  ...     1.0000  0.5000  0.6667
ArgM_SCP        3.0000        9.0         12.0  ...     0.2500  0.0652  0.1034
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      117.0000       69.0        186.0  ...     0.6290  0.5367  0.5792
sum          5959.0000     1789.0       7748.0  ...    13.5868  8.4683  9.9679
precision       0.7691        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6644        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7129        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.6431        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 10 	 loss 56.215595457831114
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1412.0000      307.0       1719.0  ...     0.8214  0.8306   0.8260
Arg1         3097.0000      640.0       3737.0  ...     0.8287  0.7679   0.7972
Arg2         1051.0000      385.0       1436.0  ...     0.7319  0.6500   0.6885
Arg3           21.0000       19.0         40.0  ...     0.5250  0.2039   0.2937
Arg4           11.0000        9.0         20.0  ...     0.5500  0.2558   0.3492
Arg5            8.0000        0.0          8.0  ...     1.0000  0.4706   0.6400
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      210.0000      105.0        315.0  ...     0.6667  0.4941   0.5676
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       24.0000       22.0         46.0  ...     0.5217  0.3429   0.4138
ArgM_CMP        5.0000       13.0         18.0  ...     0.2778  0.2273   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000  0.2500   0.3333
ArgM_DIR       13.0000        8.0         21.0  ...     0.6190  0.4643   0.5306
ArgM_EXT       10.0000       21.0         31.0  ...     0.3226  0.2128   0.2564
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       87.0000       52.0        139.0  ...     0.6259  0.5337   0.5762
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       57.0000       50.0        107.0  ...     0.5327  0.4914   0.5112
ArgM_MNS        7.0000       17.0         24.0  ...     0.2917  0.2333   0.2593
ArgM_NEG       63.0000       14.0         77.0  ...     0.8182  0.7975   0.8077
ArgM_PRP       10.0000        6.0         16.0  ...     0.6250  0.3448   0.4444
ArgM_PRX       28.0000       29.0         57.0  ...     0.4912  0.3182   0.3862
ArgM_REC       49.0000        4.0         53.0  ...     0.9245  0.7656   0.8376
ArgM_SCP       13.0000       39.0         52.0  ...     0.2500  0.2826   0.2653
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      131.0000       74.0        205.0  ...     0.6390  0.6009   0.6194
sum          6310.0000     1816.0       8126.0  ...    13.5631  9.7381  10.9869
precision       0.7765        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7035        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7382        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.7088        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7382 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 11 	 loss 51.301266960780595
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1436.0000      309.0       1745.0  ...     0.8229   0.8447   0.8337
Arg1         3210.0000      649.0       3859.0  ...     0.8318   0.7959   0.8135
Arg2         1082.0000      407.0       1489.0  ...     0.7267   0.6691   0.6967
Arg3           27.0000       30.0         57.0  ...     0.4737   0.2621   0.3375
Arg4           12.0000       11.0         23.0  ...     0.5217   0.2791   0.3636
Arg5            7.0000        1.0          8.0  ...     0.8750   0.4118   0.5600
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      212.0000      120.0        332.0  ...     0.6386   0.4988   0.5601
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       24.0000       25.0         49.0  ...     0.4898   0.3429   0.4034
ArgM_CMP        5.0000       14.0         19.0  ...     0.2632   0.2273   0.2439
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000   0.2500   0.3333
ArgM_DIR       12.0000       12.0         24.0  ...     0.5000   0.4286   0.4615
ArgM_EXT       12.0000       32.0         44.0  ...     0.2727   0.2553   0.2637
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       88.0000       52.0        140.0  ...     0.6286   0.5399   0.5809
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       64.0000       75.0        139.0  ...     0.4604   0.5517   0.5020
ArgM_MNS        9.0000       23.0         32.0  ...     0.2812   0.3000   0.2903
ArgM_NEG       67.0000       15.0         82.0  ...     0.8171   0.8481   0.8323
ArgM_PRP       11.0000       10.0         21.0  ...     0.5238   0.3793   0.4400
ArgM_PRX       35.0000       35.0         70.0  ...     0.5000   0.3977   0.4430
ArgM_REC       43.0000        3.0         46.0  ...     0.9348   0.6719   0.7818
ArgM_SCP       20.0000       50.0         70.0  ...     0.2857   0.4348   0.3448
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      141.0000       76.0        217.0  ...     0.6498   0.6468   0.6483
sum          6520.0000     1954.0       8474.0  ...    12.4975  10.2358  11.0201
precision       0.7694        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7269        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7476        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7110        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7476 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 12 	 loss 45.524705889021334
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1451.0000      354.0       1805.0  ...     0.8039   0.8535   0.8280
Arg1         3124.0000      640.0       3764.0  ...     0.8300   0.7746   0.8013
Arg2         1107.0000      482.0       1589.0  ...     0.6967   0.6846   0.6906
Arg3           26.0000       12.0         38.0  ...     0.6842   0.2524   0.3688
Arg4           11.0000       10.0         21.0  ...     0.5238   0.2558   0.3438
Arg5            5.0000        2.0          7.0  ...     0.7143   0.2941   0.4167
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      218.0000      131.0        349.0  ...     0.6246   0.5129   0.5633
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       33.0         60.0  ...     0.4500   0.3857   0.4154
ArgM_CMP        5.0000       13.0         18.0  ...     0.2778   0.2273   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        2.0000        0.0          2.0  ...     1.0000   0.2500   0.4000
ArgM_DIR       12.0000        6.0         18.0  ...     0.6667   0.4286   0.5217
ArgM_EXT       14.0000       23.0         37.0  ...     0.3784   0.2979   0.3333
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       81.0000       49.0        130.0  ...     0.6231   0.4969   0.5529
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       57.0        118.0  ...     0.5169   0.5259   0.5214
ArgM_MNS       10.0000       18.0         28.0  ...     0.3571   0.3333   0.3448
ArgM_NEG       70.0000       18.0         88.0  ...     0.7955   0.8861   0.8383
ArgM_PRP       13.0000       10.0         23.0  ...     0.5652   0.4483   0.5000
ArgM_PRX       28.0000       18.0         46.0  ...     0.6087   0.3182   0.4179
ArgM_REC       46.0000        3.0         49.0  ...     0.9388   0.7188   0.8142
ArgM_SCP       13.0000       46.0         59.0  ...     0.2203   0.2826   0.2476
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      145.0000       79.0        224.0  ...     0.6473   0.6651   0.6561
sum          6470.0000     2008.0       8478.0  ...    13.9233  10.0926  11.1594
precision       0.7632        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7214        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7417        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7200        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 13 	 loss 40.963574874829646
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1461.0000      340.0       1801.0  ...     0.8112   0.8594   0.8346
Arg1         3227.0000      621.0       3848.0  ...     0.8386   0.8001   0.8189
Arg2         1071.0000      372.0       1443.0  ...     0.7422   0.6623   0.7000
Arg3           25.0000       26.0         51.0  ...     0.4902   0.2427   0.3247
Arg4            8.0000        9.0         17.0  ...     0.4706   0.1860   0.2667
Arg5            4.0000        2.0          6.0  ...     0.6667   0.2353   0.3478
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      205.0000      137.0        342.0  ...     0.5994   0.4824   0.5346
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       26.0000       28.0         54.0  ...     0.4815   0.3714   0.4194
ArgM_CMP        5.0000       16.0         21.0  ...     0.2381   0.2273   0.2326
ArgM_CND        1.0000        3.0          4.0  ...     0.2500   0.2000   0.2222
ArgM_CRT        2.0000        7.0          9.0  ...     0.2222   0.2500   0.2353
ArgM_DIR       12.0000       12.0         24.0  ...     0.5000   0.4286   0.4615
ArgM_EXT       15.0000       32.0         47.0  ...     0.3191   0.3191   0.3191
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC      100.0000       77.0        177.0  ...     0.5650   0.6135   0.5882
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       61.0000       72.0        133.0  ...     0.4586   0.5259   0.4900
ArgM_MNS        8.0000       17.0         25.0  ...     0.3200   0.2667   0.2909
ArgM_NEG       70.0000       10.0         80.0  ...     0.8750   0.8861   0.8805
ArgM_PRP       15.0000        9.0         24.0  ...     0.6250   0.5172   0.5660
ArgM_PRX       34.0000       41.0         75.0  ...     0.4533   0.3864   0.4172
ArgM_REC       51.0000       11.0         62.0  ...     0.8226   0.7969   0.8095
ArgM_SCP       15.0000       48.0         63.0  ...     0.2381   0.3261   0.2752
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      146.0000       86.0        232.0  ...     0.6293   0.6697   0.6489
sum          6562.0000     1977.0       8539.0  ...    11.6168  10.2531  10.6839
precision       0.7685        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7316        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7496        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.6893        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7496 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 14 	 loss 43.35606135417197
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1397.0000      266.0       1663.0  ...     0.8400   0.8218   0.8308
Arg1         3205.0000      700.0       3905.0  ...     0.8207   0.7947   0.8075
Arg2         1101.0000      429.0       1530.0  ...     0.7196   0.6809   0.6997
Arg3           29.0000       40.0         69.0  ...     0.4203   0.2816   0.3372
Arg4           13.0000       12.0         25.0  ...     0.5200   0.3023   0.3824
Arg5            7.0000        1.0          8.0  ...     0.8750   0.4118   0.5600
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      204.0000      113.0        317.0  ...     0.6435   0.4800   0.5499
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       33.0         62.0  ...     0.4677   0.4143   0.4394
ArgM_CMP        6.0000       19.0         25.0  ...     0.2400   0.2727   0.2553
ArgM_CND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CRT        4.0000        1.0          5.0  ...     0.8000   0.5000   0.6154
ArgM_DIR        9.0000        3.0         12.0  ...     0.7500   0.3214   0.4500
ArgM_EXT       23.0000       41.0         64.0  ...     0.3594   0.4894   0.4144
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       92.0000       56.0        148.0  ...     0.6216   0.5644   0.5916
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       56.0000       55.0        111.0  ...     0.5045   0.4828   0.4934
ArgM_MNS        8.0000       18.0         26.0  ...     0.3077   0.2667   0.2857
ArgM_NEG       67.0000       12.0         79.0  ...     0.8481   0.8481   0.8481
ArgM_PRP       11.0000       11.0         22.0  ...     0.5000   0.3793   0.4314
ArgM_PRX       32.0000       43.0         75.0  ...     0.4267   0.3636   0.3926
ArgM_REC       46.0000        3.0         49.0  ...     0.9388   0.7188   0.8142
ArgM_SCP       13.0000       58.0         71.0  ...     0.1831   0.2826   0.2222
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      134.0000       69.0        203.0  ...     0.6601   0.6147   0.6366
sum          6486.0000     1984.0       8470.0  ...    12.4469  10.2917  11.0578
precision       0.7658        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7232        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7438        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7134        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 15 	 loss 42.0969589611517
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1420.0000      275.0       1695.0  ...     0.8378   0.8353   0.8365
Arg1         3231.0000      696.0       3927.0  ...     0.8228   0.8011   0.8118
Arg2         1109.0000      388.0       1497.0  ...     0.7408   0.6858   0.7123
Arg3           28.0000       25.0         53.0  ...     0.5283   0.2718   0.3590
Arg4           14.0000       11.0         25.0  ...     0.5600   0.3256   0.4118
Arg5            5.0000        1.0          6.0  ...     0.8333   0.2941   0.4348
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      211.0000      147.0        358.0  ...     0.5894   0.4965   0.5390
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       29.0         57.0  ...     0.4912   0.4000   0.4409
ArgM_CMP        5.0000       18.0         23.0  ...     0.2174   0.2273   0.2222
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        4.0          6.0  ...     0.3333   0.2500   0.2857
ArgM_DIR       12.0000        7.0         19.0  ...     0.6316   0.4286   0.5106
ArgM_EXT       15.0000       23.0         38.0  ...     0.3947   0.3191   0.3529
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       92.0000       50.0        142.0  ...     0.6479   0.5644   0.6033
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       65.0000       59.0        124.0  ...     0.5242   0.5603   0.5417
ArgM_MNS        7.0000       15.0         22.0  ...     0.3182   0.2333   0.2692
ArgM_NEG       68.0000       11.0         79.0  ...     0.8608   0.8608   0.8608
ArgM_PRP       14.0000       14.0         28.0  ...     0.5000   0.4828   0.4912
ArgM_PRX       34.0000       41.0         75.0  ...     0.4533   0.3864   0.4172
ArgM_REC       47.0000        5.0         52.0  ...     0.9038   0.7344   0.8103
ArgM_SCP       13.0000       59.0         72.0  ...     0.1806   0.2826   0.2203
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      144.0000       78.0        222.0  ...     0.6486   0.6606   0.6545
sum          6565.0000     1960.0       8525.0  ...    12.5180  10.3008  11.0718
precision       0.7701        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7320        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7505        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7143        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7505 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 16 	 loss 37.349715961196466
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1437.0000      284.0       1721.0  ...     0.8350   0.8453   0.8401
Arg1         3161.0000      626.0       3787.0  ...     0.8347   0.7838   0.8084
Arg2         1115.0000      427.0       1542.0  ...     0.7231   0.6895   0.7059
Arg3           28.0000       36.0         64.0  ...     0.4375   0.2718   0.3353
Arg4           11.0000       14.0         25.0  ...     0.4400   0.2558   0.3235
Arg5            6.0000        0.0          6.0  ...     1.0000   0.3529   0.5217
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      205.0000      131.0        336.0  ...     0.6101   0.4824   0.5388
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       31.0         58.0  ...     0.4655   0.3857   0.4219
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        1.0000        7.0          8.0  ...     0.1250   0.1250   0.1250
ArgM_DIR       11.0000       12.0         23.0  ...     0.4783   0.3929   0.4314
ArgM_EXT       19.0000       32.0         51.0  ...     0.3725   0.4043   0.3878
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       97.0000       71.0        168.0  ...     0.5774   0.5951   0.5861
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       65.0000       61.0        126.0  ...     0.5159   0.5603   0.5372
ArgM_MNS        9.0000       20.0         29.0  ...     0.3103   0.3000   0.3051
ArgM_NEG       67.0000       14.0         81.0  ...     0.8272   0.8481   0.8375
ArgM_PRP       13.0000        8.0         21.0  ...     0.6190   0.4483   0.5200
ArgM_PRX       32.0000       33.0         65.0  ...     0.4923   0.3636   0.4183
ArgM_REC       43.0000        4.0         47.0  ...     0.9149   0.6719   0.7748
ArgM_SCP       13.0000       49.0         62.0  ...     0.2097   0.2826   0.2407
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      132.0000       61.0        193.0  ...     0.6839   0.6055   0.6423
sum          6498.0000     1940.0       8438.0  ...    12.2223  10.0921  10.8257
precision       0.7701        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7245        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7466        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.6984        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 17 	 loss 36.533670570040044
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1428.0000      283.0       1711.0  ...     0.8346  0.8400   0.8373
Arg1         3158.0000      610.0       3768.0  ...     0.8381  0.7830   0.8096
Arg2         1105.0000      379.0       1484.0  ...     0.7446  0.6834   0.7127
Arg3           26.0000       18.0         44.0  ...     0.5909  0.2524   0.3537
Arg4            9.0000        9.0         18.0  ...     0.5000  0.2093   0.2951
Arg5            5.0000        0.0          5.0  ...     1.0000  0.2941   0.4545
ArgA            0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      218.0000      145.0        363.0  ...     0.6006  0.5129   0.5533
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       29.0000       32.0         61.0  ...     0.4754  0.4143   0.4427
ArgM_CMP        5.0000       14.0         19.0  ...     0.2632  0.2273   0.2439
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000  0.2500   0.3333
ArgM_DIR       12.0000        8.0         20.0  ...     0.6000  0.4286   0.5000
ArgM_EXT       10.0000       16.0         26.0  ...     0.3846  0.2128   0.2740
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM_LOC       85.0000       57.0        142.0  ...     0.5986  0.5215   0.5574
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       51.0000       38.0         89.0  ...     0.5730  0.4397   0.4976
ArgM_MNS        7.0000       21.0         28.0  ...     0.2500  0.2333   0.2414
ArgM_NEG       65.0000       11.0         76.0  ...     0.8553  0.8228   0.8387
ArgM_PRP       15.0000        7.0         22.0  ...     0.6818  0.5172   0.5882
ArgM_PRX       38.0000       45.0         83.0  ...     0.4578  0.4318   0.4444
ArgM_REC       46.0000        5.0         51.0  ...     0.9020  0.7188   0.8000
ArgM_SCP       13.0000       60.0         73.0  ...     0.1781  0.2826   0.2185
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      136.0000       67.0        203.0  ...     0.6700  0.6239   0.6461
sum          6464.0000     1829.0       8293.0  ...    13.4985  9.8996  10.9758
precision       0.7795        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7207        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7489        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.7081        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 18 	 loss 37.32570233408467
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1410.0000      277.0       1687.0  ...     0.8358   0.8294   0.8326
Arg1         3223.0000      631.0       3854.0  ...     0.8363   0.7992   0.8173
Arg2         1123.0000      410.0       1533.0  ...     0.7326   0.6945   0.7130
Arg3           26.0000       30.0         56.0  ...     0.4643   0.2524   0.3270
Arg4            8.0000       13.0         21.0  ...     0.3810   0.1860   0.2500
Arg5            5.0000        2.0          7.0  ...     0.7143   0.2941   0.4167
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      195.0000      115.0        310.0  ...     0.6290   0.4588   0.5306
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       30.0         57.0  ...     0.4737   0.3857   0.4252
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        5.0          7.0  ...     0.2857   0.2500   0.2667
ArgM_DIR       12.0000        5.0         17.0  ...     0.7059   0.4286   0.5333
ArgM_EXT       16.0000       30.0         46.0  ...     0.3478   0.3404   0.3441
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       88.0000       51.0        139.0  ...     0.6331   0.5399   0.5828
ArgM_MDF        0.0000        4.0          4.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       77.0        138.0  ...     0.4420   0.5259   0.4803
ArgM_MNS        8.0000       16.0         24.0  ...     0.3333   0.2667   0.2963
ArgM_NEG       66.0000       12.0         78.0  ...     0.8462   0.8354   0.8408
ArgM_PRP       17.0000       12.0         29.0  ...     0.5862   0.5862   0.5862
ArgM_PRX       36.0000       33.0         69.0  ...     0.5217   0.4091   0.4586
ArgM_REC       46.0000        8.0         54.0  ...     0.8519   0.7188   0.7797
ArgM_SCP       12.0000       51.0         63.0  ...     0.1905   0.2609   0.2202
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      139.0000       65.0        204.0  ...     0.6814   0.6376   0.6588
sum          6526.0000     1894.0       8420.0  ...    12.2425  10.1268  10.8839
precision       0.7751        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7276        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7506        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7022        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7506 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 19 	 loss 31.22844360372875
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1422.0000      270.0       1692.0  ...     0.8404   0.8365   0.8384
Arg1         3236.0000      620.0       3856.0  ...     0.8392   0.8024   0.8204
Arg2         1125.0000      403.0       1528.0  ...     0.7363   0.6957   0.7154
Arg3           28.0000       37.0         65.0  ...     0.4308   0.2718   0.3333
Arg4           13.0000       19.0         32.0  ...     0.4062   0.3023   0.3467
Arg5            5.0000        0.0          5.0  ...     1.0000   0.2941   0.4545
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      204.0000      121.0        325.0  ...     0.6277   0.4800   0.5440
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       32.0         61.0  ...     0.4754   0.4143   0.4427
ArgM_CMP        6.0000       20.0         26.0  ...     0.2308   0.2727   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        4.0000        5.0          9.0  ...     0.4444   0.5000   0.4706
ArgM_DIR       13.0000       12.0         25.0  ...     0.5200   0.4643   0.4906
ArgM_EXT       19.0000       25.0         44.0  ...     0.4318   0.4043   0.4176
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       98.0000       60.0        158.0  ...     0.6203   0.6012   0.6106
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       63.0000       60.0        123.0  ...     0.5122   0.5431   0.5272
ArgM_MNS       10.0000       21.0         31.0  ...     0.3226   0.3333   0.3279
ArgM_NEG       68.0000       14.0         82.0  ...     0.8293   0.8608   0.8447
ArgM_PRP       16.0000       10.0         26.0  ...     0.6154   0.5517   0.5818
ArgM_PRX       37.0000       42.0         79.0  ...     0.4684   0.4205   0.4431
ArgM_REC       45.0000        9.0         54.0  ...     0.8333   0.7031   0.7627
ArgM_SCP       13.0000       48.0         61.0  ...     0.2131   0.2826   0.2430
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      142.0000       72.0        214.0  ...     0.6636   0.6514   0.6574
sum          6597.0000     1901.0       8498.0  ...    13.0611  10.8861  11.4560
precision       0.7763        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7355        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7554        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7391        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7554 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 20 	 loss 33.136564429489
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1436.0000      315.0       1751.0  ...     0.8201   0.8447   0.8322
Arg1         3194.0000      627.0       3821.0  ...     0.8359   0.7920   0.8133
Arg2         1122.0000      423.0       1545.0  ...     0.7262   0.6939   0.7097
Arg3           28.0000       24.0         52.0  ...     0.5385   0.2718   0.3613
Arg4           12.0000       13.0         25.0  ...     0.4800   0.2791   0.3529
Arg5            6.0000        1.0          7.0  ...     0.8571   0.3529   0.5000
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      232.0000      137.0        369.0  ...     0.6287   0.5459   0.5844
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       27.0         56.0  ...     0.5179   0.4143   0.4603
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        4.0          7.0  ...     0.4286   0.3750   0.4000
ArgM_DIR       12.0000        7.0         19.0  ...     0.6316   0.4286   0.5106
ArgM_EXT       15.0000       18.0         33.0  ...     0.4545   0.3191   0.3750
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC       90.0000       50.0        140.0  ...     0.6429   0.5521   0.5941
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       58.0000       47.0        105.0  ...     0.5524   0.5000   0.5249
ArgM_MNS        6.0000       23.0         29.0  ...     0.2069   0.2000   0.2034
ArgM_NEG       64.0000       11.0         75.0  ...     0.8533   0.8101   0.8312
ArgM_PRP       15.0000       15.0         30.0  ...     0.5000   0.5172   0.5085
ArgM_PRX       41.0000       36.0         77.0  ...     0.5325   0.4659   0.4970
ArgM_REC       47.0000       10.0         57.0  ...     0.8246   0.7344   0.7769
ArgM_SCP       13.0000       58.0         71.0  ...     0.1831   0.2826   0.2222
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      154.0000       82.0        236.0  ...     0.6525   0.7064   0.6784
sum          6583.0000     1949.0       8532.0  ...    13.1172  10.5134  11.3077
precision       0.7716        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7340        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7523        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7295        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 21 	 loss 35.51113609356719
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1441.0000      282.0       1723.0  ...     0.8363   0.8476   0.8420
Arg1         3252.0000      625.0       3877.0  ...     0.8388   0.8063   0.8223
Arg2         1105.0000      416.0       1521.0  ...     0.7265   0.6834   0.7043
Arg3           29.0000       23.0         52.0  ...     0.5577   0.2816   0.3742
Arg4           10.0000        9.0         19.0  ...     0.5263   0.2326   0.3226
Arg5            6.0000        2.0          8.0  ...     0.7500   0.3529   0.4800
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      226.0000      143.0        369.0  ...     0.6125   0.5318   0.5693
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       32.0         60.0  ...     0.4667   0.4000   0.4308
ArgM_CMP        5.0000       17.0         22.0  ...     0.2273   0.2273   0.2273
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        4.0000        6.0         10.0  ...     0.4000   0.5000   0.4444
ArgM_DIR       14.0000       15.0         29.0  ...     0.4828   0.5000   0.4912
ArgM_EXT       20.0000       22.0         42.0  ...     0.4762   0.4255   0.4494
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       92.0000       52.0        144.0  ...     0.6389   0.5644   0.5993
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       59.0000       65.0        124.0  ...     0.4758   0.5086   0.4917
ArgM_MNS        9.0000       26.0         35.0  ...     0.2571   0.3000   0.2769
ArgM_NEG       68.0000       17.0         85.0  ...     0.8000   0.8608   0.8293
ArgM_PRP       14.0000       16.0         30.0  ...     0.4667   0.4828   0.4746
ArgM_PRX       40.0000       33.0         73.0  ...     0.5479   0.4545   0.4969
ArgM_REC       45.0000        4.0         49.0  ...     0.9184   0.7031   0.7965
ArgM_SCP       14.0000       74.0         88.0  ...     0.1591   0.3043   0.2090
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      143.0000       86.0        229.0  ...     0.6245   0.6560   0.6398
sum          6625.0000     1968.0       8593.0  ...    12.7893  10.8235  11.3049
precision       0.7710        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7387        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7545        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7293        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 22 	 loss 34.23499412534807
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1431.0000      243.0       1674.0  ...     0.8548   0.8418   0.8483
Arg1         3272.0000      662.0       3934.0  ...     0.8317   0.8113   0.8214
Arg2         1103.0000      391.0       1494.0  ...     0.7383   0.6821   0.7091
Arg3           31.0000       28.0         59.0  ...     0.5254   0.3010   0.3827
Arg4           15.0000       20.0         35.0  ...     0.4286   0.3488   0.3846
Arg5            7.0000        2.0          9.0  ...     0.7778   0.4118   0.5385
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      217.0000      106.0        323.0  ...     0.6718   0.5106   0.5802
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       30.0         58.0  ...     0.4828   0.4000   0.4375
ArgM_CMP        4.0000       15.0         19.0  ...     0.2105   0.1818   0.1951
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        3.0000        2.0          5.0  ...     0.6000   0.3750   0.4615
ArgM_DIR       14.0000        7.0         21.0  ...     0.6667   0.5000   0.5714
ArgM_EXT       24.0000       33.0         57.0  ...     0.4211   0.5106   0.4615
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC      104.0000       82.0        186.0  ...     0.5591   0.6380   0.5960
ArgM_MDF        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_MNR       67.0000       54.0        121.0  ...     0.5537   0.5776   0.5654
ArgM_MNS        8.0000       20.0         28.0  ...     0.2857   0.2667   0.2759
ArgM_NEG       68.0000       11.0         79.0  ...     0.8608   0.8608   0.8608
ArgM_PRP       13.0000        7.0         20.0  ...     0.6500   0.4483   0.5306
ArgM_PRX       35.0000       39.0         74.0  ...     0.4730   0.3977   0.4321
ArgM_REC       49.0000        5.0         54.0  ...     0.9074   0.7656   0.8305
ArgM_SCP       17.0000       58.0         75.0  ...     0.2267   0.3696   0.2810
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       86.0        226.0  ...     0.6195   0.6422   0.6306
sum          6651.0000     1907.0       8558.0  ...    12.8453  11.0413  11.6804
precision       0.7772        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7416        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7589        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7536        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7589 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 23 	 loss 32.85184386499668
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1430.0000      263.0       1693.0  ...     0.8447   0.8412   0.8429
Arg1         3225.0000      626.0       3851.0  ...     0.8374   0.7997   0.8181
Arg2         1099.0000      399.0       1498.0  ...     0.7336   0.6797   0.7056
Arg3           33.0000       38.0         71.0  ...     0.4648   0.3204   0.3793
Arg4           14.0000       12.0         26.0  ...     0.5385   0.3256   0.4058
Arg5            6.0000        1.0          7.0  ...     0.8571   0.3529   0.5000
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      216.0000      140.0        356.0  ...     0.6067   0.5082   0.5531
ArgM_AND        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       32.0000       31.0         63.0  ...     0.5079   0.4571   0.4812
ArgM_CMP        6.0000       16.0         22.0  ...     0.2727   0.2727   0.2727
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        3.0000        3.0          6.0  ...     0.5000   0.3750   0.4286
ArgM_DIR       11.0000        6.0         17.0  ...     0.6471   0.3929   0.4889
ArgM_EXT       16.0000       28.0         44.0  ...     0.3636   0.3404   0.3516
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       96.0000       59.0        155.0  ...     0.6194   0.5890   0.6038
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       65.0000       64.0        129.0  ...     0.5039   0.5603   0.5306
ArgM_MNS        9.0000       20.0         29.0  ...     0.3103   0.3000   0.3051
ArgM_NEG       65.0000       12.0         77.0  ...     0.8442   0.8228   0.8333
ArgM_PRP       16.0000       15.0         31.0  ...     0.5161   0.5517   0.5333
ArgM_PRX       38.0000       46.0         84.0  ...     0.4524   0.4318   0.4419
ArgM_REC       45.0000        7.0         52.0  ...     0.8654   0.7031   0.7759
ArgM_SCP       13.0000       41.0         54.0  ...     0.2407   0.2826   0.2600
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       66.0        206.0  ...     0.6796   0.6422   0.6604
sum          6579.0000     1900.0       8479.0  ...    12.5395  10.7493  11.4222
precision       0.7759        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7335        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7541        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7369        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 24 	 loss 31.742785970751356
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1459.0000      289.0       1748.0  ...     0.8347   0.8582   0.8463
Arg1         3270.0000      642.0       3912.0  ...     0.8359   0.8108   0.8232
Arg2         1106.0000      414.0       1520.0  ...     0.7276   0.6840   0.7051
Arg3           33.0000       38.0         71.0  ...     0.4648   0.3204   0.3793
Arg4           15.0000       11.0         26.0  ...     0.5769   0.3488   0.4348
Arg5            7.0000        5.0         12.0  ...     0.5833   0.4118   0.4828
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      228.0000      175.0        403.0  ...     0.5658   0.5365   0.5507
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       33.0000       40.0         73.0  ...     0.4521   0.4714   0.4615
ArgM_CMP        5.0000       10.0         15.0  ...     0.3333   0.2273   0.2703
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000   0.2500   0.3333
ArgM_DIR       11.0000       14.0         25.0  ...     0.4400   0.3929   0.4151
ArgM_EXT       18.0000       31.0         49.0  ...     0.3673   0.3830   0.3750
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       90.0000       68.0        158.0  ...     0.5696   0.5521   0.5607
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       69.0000       66.0        135.0  ...     0.5111   0.5948   0.5498
ArgM_MNS        8.0000       27.0         35.0  ...     0.2286   0.2667   0.2462
ArgM_NEG       72.0000       17.0         89.0  ...     0.8090   0.9114   0.8571
ArgM_PRP       16.0000       14.0         30.0  ...     0.5333   0.5517   0.5424
ArgM_PRX       38.0000       35.0         73.0  ...     0.5205   0.4318   0.4720
ArgM_REC       47.0000        5.0         52.0  ...     0.9038   0.7344   0.8103
ArgM_SCP       17.0000       55.0         72.0  ...     0.2361   0.3696   0.2881
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      144.0000       83.0        227.0  ...     0.6344   0.6606   0.6472
sum          6689.0000     2046.0       8735.0  ...    11.9616  10.9681  11.3013
precision       0.7658        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7458        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7556        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7291        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 25 	 loss 30.10038202470855
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1418.0000      246.0       1664.0  ...     0.8522   0.8341   0.8430
Arg1         3234.0000      613.0       3847.0  ...     0.8407   0.8019   0.8208
Arg2         1119.0000      393.0       1512.0  ...     0.7401   0.6920   0.7152
Arg3           35.0000       55.0         90.0  ...     0.3889   0.3398   0.3627
Arg4           10.0000       13.0         23.0  ...     0.4348   0.2326   0.3030
Arg5            8.0000        1.0          9.0  ...     0.8889   0.4706   0.6154
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      201.0000      108.0        309.0  ...     0.6505   0.4729   0.5477
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       33.0000       29.0         62.0  ...     0.5323   0.4714   0.5000
ArgM_CMP        3.0000       13.0         16.0  ...     0.1875   0.1364   0.1579
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        1.0000        4.0          5.0  ...     0.2000   0.1250   0.1538
ArgM_DIR        9.0000       10.0         19.0  ...     0.4737   0.3214   0.3830
ArgM_EXT       20.0000       27.0         47.0  ...     0.4255   0.4255   0.4255
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC      101.0000       57.0        158.0  ...     0.6392   0.6196   0.6293
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       71.0000       63.0        134.0  ...     0.5299   0.6121   0.5680
ArgM_MNS        7.0000       21.0         28.0  ...     0.2500   0.2333   0.2414
ArgM_NEG       68.0000       14.0         82.0  ...     0.8293   0.8608   0.8447
ArgM_PRP       16.0000        8.0         24.0  ...     0.6667   0.5517   0.6038
ArgM_PRX       36.0000       44.0         80.0  ...     0.4500   0.4091   0.4286
ArgM_REC       47.0000        4.0         51.0  ...     0.9216   0.7344   0.8174
ArgM_SCP       16.0000       44.0         60.0  ...     0.2667   0.3478   0.3019
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      139.0000       76.0        215.0  ...     0.6465   0.6376   0.6420
sum          6593.0000     1849.0       8442.0  ...    12.8147  10.5301  11.2385
precision       0.7810        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7351        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7573        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7251        NaN          NaN  ...        NaN      NaN      NaN



Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 265, MAX_LENGTH = 252, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
42022
Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 0 	 loss 37.14770202442537
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1431.0000      273.0       1704.0  ...     0.8398   0.8418   0.8408
Arg1         3269.0000      646.0       3915.0  ...     0.8350   0.8106   0.8226
Arg2         1102.0000      397.0       1499.0  ...     0.7352   0.6815   0.7073
Arg3           33.0000       31.0         64.0  ...     0.5156   0.3204   0.3952
Arg4           16.0000       13.0         29.0  ...     0.5517   0.3721   0.4444
Arg5            6.0000        4.0         10.0  ...     0.6000   0.3529   0.4444
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      218.0000      136.0        354.0  ...     0.6158   0.5129   0.5597
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       23.0000       28.0         51.0  ...     0.4510   0.3286   0.3802
ArgM_CMP        5.0000       17.0         22.0  ...     0.2273   0.2273   0.2273
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        1.0          4.0  ...     0.7500   0.3750   0.5000
ArgM_DIR        9.0000        4.0         13.0  ...     0.6923   0.3214   0.4390
ArgM_EXT       17.0000       29.0         46.0  ...     0.3696   0.3617   0.3656
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       98.0000       62.0        160.0  ...     0.6125   0.6012   0.6068
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       62.0000       57.0        119.0  ...     0.5210   0.5345   0.5277
ArgM_MNS        9.0000       20.0         29.0  ...     0.3103   0.3000   0.3051
ArgM_NEG       69.0000       13.0         82.0  ...     0.8415   0.8734   0.8571
ArgM_PRP       17.0000       13.0         30.0  ...     0.5667   0.5862   0.5763
ArgM_PRX       35.0000       30.0         65.0  ...     0.5385   0.3977   0.4575
ArgM_REC       45.0000        7.0         52.0  ...     0.8654   0.7031   0.7759
ArgM_SCP       11.0000       48.0         59.0  ...     0.1864   0.2391   0.2095
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      147.0000       77.0        224.0  ...     0.6562   0.6743   0.6652
sum          6626.0000     1912.0       8538.0  ...    13.2818  10.6158  11.4409
precision       0.7761        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7388        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7570        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7381        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.757 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 1 	 loss 34.81322495548369
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1409.0000      255.0       1664.0  ...     0.8468   0.8288   0.8377
Arg1         3254.0000      653.0       3907.0  ...     0.8329   0.8068   0.8196
Arg2         1123.0000      394.0       1517.0  ...     0.7403   0.6945   0.7167
Arg3           37.0000       27.0         64.0  ...     0.5781   0.3592   0.4431
Arg4           14.0000       14.0         28.0  ...     0.5000   0.3256   0.3944
Arg5            5.0000        3.0          8.0  ...     0.6250   0.2941   0.4000
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      216.0000      133.0        349.0  ...     0.6189   0.5082   0.5581
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       33.0000       27.0         60.0  ...     0.5500   0.4714   0.5077
ArgM_CMP        6.0000       18.0         24.0  ...     0.2500   0.2727   0.2609
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        3.0000        4.0          7.0  ...     0.4286   0.3750   0.4000
ArgM_DIR       12.0000       17.0         29.0  ...     0.4138   0.4286   0.4211
ArgM_EXT       20.0000       27.0         47.0  ...     0.4255   0.4255   0.4255
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC       87.0000       52.0        139.0  ...     0.6259   0.5337   0.5762
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       70.0000       75.0        145.0  ...     0.4828   0.6034   0.5364
ArgM_MNS        7.0000       18.0         25.0  ...     0.2800   0.2333   0.2545
ArgM_NEG       68.0000       13.0         81.0  ...     0.8395   0.8608   0.8500
ArgM_PRP       15.0000       11.0         26.0  ...     0.5769   0.5172   0.5455
ArgM_PRX       41.0000       42.0         83.0  ...     0.4940   0.4659   0.4795
ArgM_REC       50.0000        8.0         58.0  ...     0.8621   0.7812   0.8197
ArgM_SCP       19.0000       72.0         91.0  ...     0.2088   0.4130   0.2774
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      148.0000       74.0        222.0  ...     0.6667   0.6789   0.6727
sum          6638.0000     1942.0       8580.0  ...    12.3464  11.0782  11.4823
precision       0.7737        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7401        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7565        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7408        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 2 	 loss 31.244977404932285
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1435.0000      254.0       1689.0  ...     0.8496   0.8441   0.8469
Arg1         3255.0000      666.0       3921.0  ...     0.8301   0.8071   0.8185
Arg2         1114.0000      383.0       1497.0  ...     0.7442   0.6889   0.7155
Arg3           33.0000       39.0         72.0  ...     0.4583   0.3204   0.3771
Arg4           14.0000       16.0         30.0  ...     0.4667   0.3256   0.3836
Arg5            5.0000        5.0         10.0  ...     0.5000   0.2941   0.3704
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      227.0000      158.0        385.0  ...     0.5896   0.5341   0.5605
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       30.0000       37.0         67.0  ...     0.4478   0.4286   0.4380
ArgM_CMP        6.0000       18.0         24.0  ...     0.2500   0.2727   0.2609
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        3.0          6.0  ...     0.5000   0.3750   0.4286
ArgM_DIR       11.0000       12.0         23.0  ...     0.4783   0.3929   0.4314
ArgM_EXT       19.0000       29.0         48.0  ...     0.3958   0.4043   0.4000
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       96.0000       61.0        157.0  ...     0.6115   0.5890   0.6000
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       65.0000       50.0        115.0  ...     0.5652   0.5603   0.5628
ArgM_MNS       10.0000       22.0         32.0  ...     0.3125   0.3333   0.3226
ArgM_NEG       67.0000       14.0         81.0  ...     0.8272   0.8481   0.8375
ArgM_PRP       12.0000        9.0         21.0  ...     0.5714   0.4138   0.4800
ArgM_PRX       36.0000       37.0         73.0  ...     0.4932   0.4091   0.4472
ArgM_REC       51.0000        5.0         56.0  ...     0.9107   0.7969   0.8500
ArgM_SCP       13.0000       45.0         58.0  ...     0.2241   0.2826   0.2500
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      139.0000       73.0        212.0  ...     0.6557   0.6376   0.6465
sum          6642.0000     1943.0       8585.0  ...    12.6818  10.7585  11.3610
precision       0.7737        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7406        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7568        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7330        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 3 	 loss 28.444553805203356
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1424.0000      264.0       1688.0  ...     0.8436   0.8376   0.8406
Arg1         3248.0000      651.0       3899.0  ...     0.8330   0.8054   0.8190
Arg2         1131.0000      421.0       1552.0  ...     0.7287   0.6994   0.7138
Arg3           25.0000       30.0         55.0  ...     0.4545   0.2427   0.3165
Arg4           16.0000       17.0         33.0  ...     0.4848   0.3721   0.4211
Arg5            5.0000        5.0         10.0  ...     0.5000   0.2941   0.3704
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      216.0000      106.0        322.0  ...     0.6708   0.5082   0.5783
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       32.0000       35.0         67.0  ...     0.4776   0.4571   0.4672
ArgM_CMP        7.0000       21.0         28.0  ...     0.2500   0.3182   0.2800
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        3.0          6.0  ...     0.5000   0.3750   0.4286
ArgM_DIR       11.0000       14.0         25.0  ...     0.4400   0.3929   0.4151
ArgM_EXT       24.0000       27.0         51.0  ...     0.4706   0.5106   0.4898
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       94.0000       45.0        139.0  ...     0.6763   0.5767   0.6225
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       68.0000       67.0        135.0  ...     0.5037   0.5862   0.5418
ArgM_MNS        9.0000       24.0         33.0  ...     0.2727   0.3000   0.2857
ArgM_NEG       69.0000        9.0         78.0  ...     0.8846   0.8734   0.8790
ArgM_PRP       12.0000       10.0         22.0  ...     0.5455   0.4138   0.4706
ArgM_PRX       34.0000       36.0         70.0  ...     0.4857   0.3864   0.4304
ArgM_REC       48.0000       10.0         58.0  ...     0.8276   0.7500   0.7869
ArgM_SCP       17.0000       48.0         65.0  ...     0.2615   0.3696   0.3063
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      144.0000       63.0        207.0  ...     0.6957   0.6606   0.6776
sum          6638.0000     1911.0       8549.0  ...    12.8070  10.9300  11.4744
precision       0.7765        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7401        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7578        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7403        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7578 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 4 	 loss 29.500842493577665
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1402.0000      222.0       1624.0  ...     0.8633   0.8247   0.8436
Arg1         3278.0000      704.0       3982.0  ...     0.8232   0.8128   0.8180
Arg2         1120.0000      390.0       1510.0  ...     0.7417   0.6926   0.7163
Arg3           31.0000       43.0         74.0  ...     0.4189   0.3010   0.3503
Arg4           14.0000       13.0         27.0  ...     0.5185   0.3256   0.4000
Arg5            4.0000        5.0          9.0  ...     0.4444   0.2353   0.3077
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      214.0000      126.0        340.0  ...     0.6294   0.5035   0.5595
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       30.0         57.0  ...     0.4737   0.3857   0.4252
ArgM_CMP        7.0000       15.0         22.0  ...     0.3182   0.3182   0.3182
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        1.0          4.0  ...     0.7500   0.3750   0.5000
ArgM_DIR       10.0000        6.0         16.0  ...     0.6250   0.3571   0.4545
ArgM_EXT       21.0000       25.0         46.0  ...     0.4565   0.4468   0.4516
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC       99.0000       61.0        160.0  ...     0.6188   0.6074   0.6130
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       67.0000       60.0        127.0  ...     0.5276   0.5776   0.5514
ArgM_MNS       10.0000       22.0         32.0  ...     0.3125   0.3333   0.3226
ArgM_NEG       70.0000       12.0         82.0  ...     0.8537   0.8861   0.8696
ArgM_PRP       14.0000        9.0         23.0  ...     0.6087   0.4828   0.5385
ArgM_PRX       38.0000       41.0         79.0  ...     0.4810   0.4318   0.4551
ArgM_REC       51.0000        6.0         57.0  ...     0.8947   0.7969   0.8430
ArgM_SCP       17.0000       61.0         78.0  ...     0.2179   0.3696   0.2742
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      147.0000       73.0        220.0  ...     0.6682   0.6743   0.6712
sum          6645.0000     1929.0       8574.0  ...    13.2460  10.9381  11.6167
precision       0.7750        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7409        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7576        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7495        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 5 	 loss 28.81857896787244
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1429.0000      258.0       1687.0  ...     0.8471   0.8406   0.8438
Arg1         3239.0000      656.0       3895.0  ...     0.8316   0.8031   0.8171
Arg2         1112.0000      388.0       1500.0  ...     0.7413   0.6877   0.7135
Arg3           37.0000       46.0         83.0  ...     0.4458   0.3592   0.3978
Arg4           13.0000       18.0         31.0  ...     0.4194   0.3023   0.3514
Arg5            4.0000        3.0          7.0  ...     0.5714   0.2353   0.3333
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      221.0000      118.0        339.0  ...     0.6519   0.5200   0.5785
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       38.0         66.0  ...     0.4242   0.4000   0.4118
ArgM_CMP        5.0000       14.0         19.0  ...     0.2632   0.2273   0.2439
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        4.0000        2.0          6.0  ...     0.6667   0.5000   0.5714
ArgM_DIR       12.0000       12.0         24.0  ...     0.5000   0.4286   0.4615
ArgM_EXT       19.0000       21.0         40.0  ...     0.4750   0.4043   0.4368
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       93.0000       43.0        136.0  ...     0.6838   0.5706   0.6221
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       43.0        104.0  ...     0.5865   0.5259   0.5545
ArgM_MNS       10.0000       22.0         32.0  ...     0.3125   0.3333   0.3226
ArgM_NEG       71.0000       20.0         91.0  ...     0.7802   0.8987   0.8353
ArgM_PRP       18.0000        8.0         26.0  ...     0.6923   0.6207   0.6545
ArgM_PRX       35.0000       32.0         67.0  ...     0.5224   0.3977   0.4516
ArgM_REC       48.0000        4.0         52.0  ...     0.9231   0.7500   0.8276
ArgM_SCP       16.0000       62.0         78.0  ...     0.2051   0.3478   0.2581
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      138.0000       75.0        213.0  ...     0.6479   0.6330   0.6404
sum          6614.0000     1886.0       8500.0  ...    12.6914  10.9861  11.6133
precision       0.7781        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7374        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7572        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7492        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 6 	 loss 29.521444017602988
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1428.0000      254.0       1682.0  ...     0.8490   0.8400   0.8445
Arg1         3277.0000      649.0       3926.0  ...     0.8347   0.8125   0.8235
Arg2         1125.0000      421.0       1546.0  ...     0.7277   0.6957   0.7113
Arg3           33.0000       37.0         70.0  ...     0.4714   0.3204   0.3815
Arg4           15.0000       17.0         32.0  ...     0.4688   0.3488   0.4000
Arg5            7.0000        2.0          9.0  ...     0.7778   0.4118   0.5385
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      227.0000      138.0        365.0  ...     0.6219   0.5341   0.5747
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       30.0000       34.0         64.0  ...     0.4688   0.4286   0.4478
ArgM_CMP        6.0000       14.0         20.0  ...     0.3000   0.2727   0.2857
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        3.0000        5.0          8.0  ...     0.3750   0.3750   0.3750
ArgM_DIR       11.0000       10.0         21.0  ...     0.5238   0.3929   0.4490
ArgM_EXT       20.0000       29.0         49.0  ...     0.4082   0.4255   0.4167
ArgM_GOL        0.0000        4.0          4.0  ...     0.0000   0.0000      NaN
ArgM_LOC      100.0000       69.0        169.0  ...     0.5917   0.6135   0.6024
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       76.0000       77.0        153.0  ...     0.4967   0.6552   0.5651
ArgM_MNS        8.0000       15.0         23.0  ...     0.3478   0.2667   0.3019
ArgM_NEG       66.0000       11.0         77.0  ...     0.8571   0.8354   0.8462
ArgM_PRP       15.0000       10.0         25.0  ...     0.6000   0.5172   0.5556
ArgM_PRX       36.0000       34.0         70.0  ...     0.5143   0.4091   0.4557
ArgM_REC       48.0000        2.0         50.0  ...     0.9600   0.7500   0.8421
ArgM_SCP       15.0000       53.0         68.0  ...     0.2206   0.3261   0.2632
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      150.0000       88.0        238.0  ...     0.6303   0.6881   0.6579
sum          6697.0000     1979.0       8676.0  ...    12.3788  11.1193  11.5880
precision       0.7719        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7467        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7591        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7476        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7591 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 7 	 loss 31.80492091237693
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1461.0000      304.0       1765.0  ...     0.8278   0.8594   0.8433
Arg1         3256.0000      620.0       3876.0  ...     0.8400   0.8073   0.8234
Arg2         1099.0000      357.0       1456.0  ...     0.7548   0.6797   0.7153
Arg3           35.0000       54.0         89.0  ...     0.3933   0.3398   0.3646
Arg4           12.0000       12.0         24.0  ...     0.5000   0.2791   0.3582
Arg5            7.0000        3.0         10.0  ...     0.7000   0.4118   0.5185
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      231.0000      157.0        388.0  ...     0.5954   0.5435   0.5683
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       22.0         51.0  ...     0.5686   0.4143   0.4793
ArgM_CMP        6.0000       20.0         26.0  ...     0.2308   0.2727   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        2.0000        5.0          7.0  ...     0.2857   0.2500   0.2667
ArgM_DIR       12.0000       11.0         23.0  ...     0.5217   0.4286   0.4706
ArgM_EXT       19.0000       24.0         43.0  ...     0.4419   0.4043   0.4222
ArgM_GOL        0.0000        6.0          6.0  ...     0.0000   0.0000      NaN
ArgM_LOC       95.0000       79.0        174.0  ...     0.5460   0.5828   0.5638
ArgM_MDF        0.0000        4.0          4.0  ...     0.0000   0.0000      NaN
ArgM_MNR       66.0000       67.0        133.0  ...     0.4962   0.5690   0.5301
ArgM_MNS       13.0000       21.0         34.0  ...     0.3824   0.4333   0.4062
ArgM_NEG       68.0000        9.0         77.0  ...     0.8831   0.8608   0.8718
ArgM_PRP       12.0000       10.0         22.0  ...     0.5455   0.4138   0.4706
ArgM_PRX       36.0000       33.0         69.0  ...     0.5217   0.4091   0.4586
ArgM_REC       52.0000        4.0         56.0  ...     0.9286   0.8125   0.8667
ArgM_SCP       19.0000       73.0         92.0  ...     0.2065   0.4130   0.2754
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      142.0000       87.0        229.0  ...     0.6201   0.6514   0.6353
sum          6673.0000     1982.0       8655.0  ...    12.7900  11.0361  11.4922
precision       0.7710        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7440        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7573        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7414        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 8 	 loss 29.79226146189751
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1443.0000      263.0       1706.0  ...     0.8458   0.8488   0.8473
Arg1         3242.0000      648.0       3890.0  ...     0.8334   0.8039   0.8184
Arg2         1135.0000      405.0       1540.0  ...     0.7370   0.7019   0.7190
Arg3           36.0000       52.0         88.0  ...     0.4091   0.3495   0.3770
Arg4           15.0000       18.0         33.0  ...     0.4545   0.3488   0.3947
Arg5            7.0000        4.0         11.0  ...     0.6364   0.4118   0.5000
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      238.0000      135.0        373.0  ...     0.6381   0.5600   0.5965
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       22.0         50.0  ...     0.5600   0.4000   0.4667
ArgM_CMP        5.0000       17.0         22.0  ...     0.2273   0.2273   0.2273
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        4.0          6.0  ...     0.3333   0.2500   0.2857
ArgM_DIR       14.0000        5.0         19.0  ...     0.7368   0.5000   0.5957
ArgM_EXT       20.0000       28.0         48.0  ...     0.4167   0.4255   0.4211
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       92.0000       60.0        152.0  ...     0.6053   0.5644   0.5841
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       56.0        117.0  ...     0.5214   0.5259   0.5236
ArgM_MNS        9.0000       21.0         30.0  ...     0.3000   0.3000   0.3000
ArgM_NEG       70.0000       11.0         81.0  ...     0.8642   0.8861   0.8750
ArgM_PRP       13.0000       11.0         24.0  ...     0.5417   0.4483   0.4906
ArgM_PRX       36.0000       24.0         60.0  ...     0.6000   0.4091   0.4865
ArgM_REC       49.0000        4.0         53.0  ...     0.9245   0.7656   0.8376
ArgM_SCP       12.0000       42.0         54.0  ...     0.2222   0.2609   0.2400
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      146.0000       68.0        214.0  ...     0.6822   0.6697   0.6759
sum          6674.0000     1904.0       8578.0  ...    12.5899  10.8575  11.5484
precision       0.7780        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7441        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7607        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7451        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7607 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 9 	 loss 29.391072773795088
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1445.0000      286.0       1731.0  ...     0.8348   0.8500   0.8423
Arg1         3252.0000      632.0       3884.0  ...     0.8373   0.8063   0.8215
Arg2         1133.0000      424.0       1557.0  ...     0.7277   0.7007   0.7139
Arg3           32.0000       41.0         73.0  ...     0.4384   0.3107   0.3636
Arg4           15.0000       18.0         33.0  ...     0.4545   0.3488   0.3947
Arg5            8.0000        4.0         12.0  ...     0.6667   0.4706   0.5517
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      222.0000      123.0        345.0  ...     0.6435   0.5224   0.5766
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       30.0000       28.0         58.0  ...     0.5172   0.4286   0.4688
ArgM_CMP        7.0000       20.0         27.0  ...     0.2593   0.3182   0.2857
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        2.0000        3.0          5.0  ...     0.4000   0.2500   0.3077
ArgM_DIR       14.0000       18.0         32.0  ...     0.4375   0.5000   0.4667
ArgM_EXT       21.0000       33.0         54.0  ...     0.3889   0.4468   0.4158
ArgM_GOL        0.0000        4.0          4.0  ...     0.0000   0.0000      NaN
ArgM_LOC       96.0000       58.0        154.0  ...     0.6234   0.5890   0.6057
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       69.0000       69.0        138.0  ...     0.5000   0.5948   0.5433
ArgM_MNS        9.0000       19.0         28.0  ...     0.3214   0.3000   0.3103
ArgM_NEG       69.0000       14.0         83.0  ...     0.8313   0.8734   0.8519
ArgM_PRP       15.0000       15.0         30.0  ...     0.5000   0.5172   0.5085
ArgM_PRX       38.0000       33.0         71.0  ...     0.5352   0.4318   0.4780
ArgM_REC       53.0000        4.0         57.0  ...     0.9298   0.8281   0.8760
ArgM_SCP       18.0000       64.0         82.0  ...     0.2195   0.3913   0.2812
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      150.0000       82.0        232.0  ...     0.6466   0.6881   0.6667
sum          6699.0000     1995.0       8694.0  ...    12.7129  11.3668  11.6641
precision       0.7705        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7469        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7585        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7525        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 10 	 loss 24.315468357795282
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1436.0000      278.0       1714.0  ...     0.8378   0.8447   0.8412
Arg1         3295.0000      608.0       3903.0  ...     0.8442   0.8170   0.8304
Arg2         1113.0000      407.0       1520.0  ...     0.7322   0.6883   0.7096
Arg3           32.0000       49.0         81.0  ...     0.3951   0.3107   0.3478
Arg4           16.0000       18.0         34.0  ...     0.4706   0.3721   0.4156
Arg5            5.0000        0.0          5.0  ...     1.0000   0.2941   0.4545
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      214.0000      136.0        350.0  ...     0.6114   0.5035   0.5523
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       26.0000       18.0         44.0  ...     0.5909   0.3714   0.4561
ArgM_CMP        6.0000       16.0         22.0  ...     0.2727   0.2727   0.2727
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        6.0          8.0  ...     0.2500   0.2500   0.2500
ArgM_DIR       15.0000        8.0         23.0  ...     0.6522   0.5357   0.5882
ArgM_EXT       19.0000       34.0         53.0  ...     0.3585   0.4043   0.3800
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       89.0000       57.0        146.0  ...     0.6096   0.5460   0.5761
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       68.0000       61.0        129.0  ...     0.5271   0.5862   0.5551
ArgM_MNS       10.0000       25.0         35.0  ...     0.2857   0.3333   0.3077
ArgM_NEG       71.0000       15.0         86.0  ...     0.8256   0.8987   0.8606
ArgM_PRP       16.0000       17.0         33.0  ...     0.4848   0.5517   0.5161
ArgM_PRX       38.0000       38.0         76.0  ...     0.5000   0.4318   0.4634
ArgM_REC       50.0000        2.0         52.0  ...     0.9615   0.7812   0.8621
ArgM_SCP       18.0000       72.0         90.0  ...     0.2000   0.3913   0.2647
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       69.0        209.0  ...     0.6699   0.6422   0.6557
sum          6680.0000     1939.0       8619.0  ...    12.5799  11.0272  11.4458
precision       0.7750        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7448        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7596        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7384        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 11 	 loss 25.662293759260592
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1467.0000      307.0       1774.0  ...     0.8269   0.8629   0.8446
Arg1         3241.0000      617.0       3858.0  ...     0.8401   0.8036   0.8214
Arg2         1121.0000      386.0       1507.0  ...     0.7439   0.6933   0.7177
Arg3           35.0000       42.0         77.0  ...     0.4545   0.3398   0.3889
Arg4           15.0000       15.0         30.0  ...     0.5000   0.3488   0.4110
Arg5            6.0000        2.0          8.0  ...     0.7500   0.3529   0.4800
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      232.0000      147.0        379.0  ...     0.6121   0.5459   0.5771
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       23.0         52.0  ...     0.5577   0.4143   0.4754
ArgM_CMP        7.0000       20.0         27.0  ...     0.2593   0.3182   0.2857
ArgM_CND        1.0000        4.0          5.0  ...     0.2000   0.2000   0.2000
ArgM_CRT        3.0000        5.0          8.0  ...     0.3750   0.3750   0.3750
ArgM_DIR       15.0000       10.0         25.0  ...     0.6000   0.5357   0.5660
ArgM_EXT       18.0000       29.0         47.0  ...     0.3830   0.3830   0.3830
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC       90.0000       53.0        143.0  ...     0.6294   0.5521   0.5882
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       69.0000       67.0        136.0  ...     0.5074   0.5948   0.5476
ArgM_MNS       13.0000       34.0         47.0  ...     0.2766   0.4333   0.3377
ArgM_NEG       67.0000       13.0         80.0  ...     0.8375   0.8481   0.8428
ArgM_PRP       15.0000       15.0         30.0  ...     0.5000   0.5172   0.5085
ArgM_PRX       38.0000       41.0         79.0  ...     0.4810   0.4318   0.4551
ArgM_REC       47.0000        6.0         53.0  ...     0.8868   0.7344   0.8034
ArgM_SCP       17.0000       61.0         78.0  ...     0.2179   0.3696   0.2742
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      157.0000       70.0        227.0  ...     0.6916   0.7202   0.7056
sum          6703.0000     1972.0       8675.0  ...    12.1307  11.3750  11.5889
precision       0.7727        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7474        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7598        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7477        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 12 	 loss 25.757361370818273
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1425.0000      250.0       1675.0  ...     0.8507   0.8382   0.8444
Arg1         3243.0000      598.0       3841.0  ...     0.8443   0.8041   0.8237
Arg2         1126.0000      419.0       1545.0  ...     0.7288   0.6964   0.7122
Arg3           34.0000       61.0         95.0  ...     0.3579   0.3301   0.3434
Arg4           15.0000       11.0         26.0  ...     0.5769   0.3488   0.4348
Arg5            7.0000        4.0         11.0  ...     0.6364   0.4118   0.5000
ArgA            0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        5.0          5.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      242.0000      154.0        396.0  ...     0.6111   0.5694   0.5895
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       31.0000       25.0         56.0  ...     0.5536   0.4429   0.4921
ArgM_CMP        6.0000       16.0         22.0  ...     0.2727   0.2727   0.2727
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        4.0000        7.0         11.0  ...     0.3636   0.5000   0.4211
ArgM_DIR       12.0000       11.0         23.0  ...     0.5217   0.4286   0.4706
ArgM_EXT       16.0000       20.0         36.0  ...     0.4444   0.3404   0.3855
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       85.0000       55.0        140.0  ...     0.6071   0.5215   0.5611
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       68.0000       73.0        141.0  ...     0.4823   0.5862   0.5292
ArgM_MNS       13.0000       18.0         31.0  ...     0.4194   0.4333   0.4262
ArgM_NEG       66.0000       12.0         78.0  ...     0.8462   0.8354   0.8408
ArgM_PRP       13.0000       16.0         29.0  ...     0.4483   0.4483   0.4483
ArgM_PRX       39.0000       35.0         74.0  ...     0.5270   0.4432   0.4815
ArgM_REC       49.0000        2.0         51.0  ...     0.9608   0.7656   0.8522
ArgM_SCP       16.0000       75.0         91.0  ...     0.1758   0.3478   0.2336
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       67.0        207.0  ...     0.6763   0.6422   0.6588
sum          6651.0000     1941.0       8592.0  ...    12.2388  11.2070  11.5717
precision       0.7741        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7416        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7575        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7466        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 13 	 loss 26.98089391584689
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1453.0000      286.0       1739.0  ...     0.8355   0.8547   0.8450
Arg1         3248.0000      647.0       3895.0  ...     0.8339   0.8054   0.8194
Arg2         1123.0000      403.0       1526.0  ...     0.7359   0.6945   0.7146
Arg3           36.0000       47.0         83.0  ...     0.4337   0.3495   0.3871
Arg4           14.0000       13.0         27.0  ...     0.5185   0.3256   0.4000
Arg5            6.0000        3.0          9.0  ...     0.6667   0.3529   0.4615
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            1.0000        0.0          1.0  ...     1.0000   1.0000   1.0000
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      216.0000      133.0        349.0  ...     0.6189   0.5082   0.5581
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       24.0         52.0  ...     0.5385   0.4000   0.4590
ArgM_CMP        6.0000       12.0         18.0  ...     0.3333   0.2727   0.3000
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        3.0000        5.0          8.0  ...     0.3750   0.3750   0.3750
ArgM_DIR       14.0000       13.0         27.0  ...     0.5185   0.5000   0.5091
ArgM_EXT       18.0000       30.0         48.0  ...     0.3750   0.3830   0.3789
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       99.0000       48.0        147.0  ...     0.6735   0.6074   0.6387
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       66.0000       71.0        137.0  ...     0.4818   0.5690   0.5217
ArgM_MNS       14.0000       30.0         44.0  ...     0.3182   0.4667   0.3784
ArgM_NEG       68.0000       11.0         79.0  ...     0.8608   0.8608   0.8608
ArgM_PRP       15.0000       14.0         29.0  ...     0.5172   0.5172   0.5172
ArgM_PRX       42.0000       32.0         74.0  ...     0.5676   0.4773   0.5185
ArgM_REC       50.0000        7.0         57.0  ...     0.8772   0.7812   0.8264
ArgM_SCP       19.0000       69.0         88.0  ...     0.2159   0.4130   0.2836
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      152.0000       77.0        229.0  ...     0.6638   0.6972   0.6801
sum          6692.0000     1980.0       8672.0  ...    13.2926  12.4113  12.6833
precision       0.7717        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7461        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7587        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.8183        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/media/takeuchi/HDPH-UT/callum/SpanSRL/Span1/Main/Train/../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
Stop. No change in valid f1


 1 day, 20:59:03.647634 

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1449.0000      265.0       1714.0  ...     0.8454   0.8584   0.8519
Arg1         3316.0000      608.0       3924.0  ...     0.8451   0.8224   0.8336
Arg2         1140.0000      360.0       1500.0  ...     0.7600   0.7037   0.7308
Arg3           19.0000       48.0         67.0  ...     0.2836   0.2043   0.2375
Arg4           16.0000       16.0         32.0  ...     0.5000   0.3556   0.4156
Arg5            2.0000        5.0          7.0  ...     0.2857   0.2857   0.2857
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      221.0000      151.0        372.0  ...     0.5941   0.5140   0.5511
ArgM_AND        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       37.0000       16.0         53.0  ...     0.6981   0.5362   0.6066
ArgM_CMP        7.0000        6.0         13.0  ...     0.5385   0.4118   0.4667
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.1667   0.2500
ArgM_CRT        7.0000        5.0         12.0  ...     0.5833   0.3500   0.4375
ArgM_DIR       13.0000        1.0         14.0  ...     0.9286   0.5000   0.6500
ArgM_EXT       10.0000       25.0         35.0  ...     0.2857   0.3226   0.3030
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       86.0000       64.0        150.0  ...     0.5733   0.6014   0.5870
ArgM_MDF        1.0000        2.0          3.0  ...     0.3333   0.1250   0.1818
ArgM_MNR       65.0000       48.0        113.0  ...     0.5752   0.5118   0.5417
ArgM_MNS       18.0000       25.0         43.0  ...     0.4186   0.4615   0.4390
ArgM_NEG       86.0000       11.0         97.0  ...     0.8866   0.8866   0.8866
ArgM_PRP       12.0000        8.0         20.0  ...     0.6000   0.4615   0.5217
ArgM_PRX       34.0000       26.0         60.0  ...     0.5667   0.3656   0.4444
ArgM_REC       34.0000       12.0         46.0  ...     0.7391   0.7556   0.7473
ArgM_SCP       12.0000       39.0         51.0  ...     0.2353   0.2143   0.2243
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      149.0000       74.0        223.0  ...     0.6682   0.6535   0.6608
sum          6735.0000     1819.0       8554.0  ...    13.2444  11.0681  11.8545
precision       0.7874        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7521        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7693        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7648        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]


fix_labels
87.1464131401318
move_arg
88.37623192848861
merge_span
88.79349915900542
split_span
89.18619992863366
fix_border
90.64477688692514
remove_arg
91.6518204810616
add_arg
96.58409858010094