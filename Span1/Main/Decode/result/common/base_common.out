Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 265, MAX_LENGTH = 252, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
42022
Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 0 	 loss 537.730842590332
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1051.0000      116.0       1167.0  ...     0.9006  0.6182  0.7332
Arg1         2164.0000      531.0       2695.0  ...     0.8030  0.5366  0.6433
Arg2          839.0000      542.0       1381.0  ...     0.6075  0.5189  0.5597
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV       75.0000       95.0        170.0  ...     0.4412  0.1765  0.2521
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        3.0000        3.0          6.0  ...     0.5000  0.0184  0.0355
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        8.0000       10.0         18.0  ...     0.4444  0.0690  0.1194
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG       39.0000        6.0         45.0  ...     0.8667  0.4937  0.6290
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       16.0000       26.0         42.0  ...     0.3810  0.0734  0.1231
sum          4195.0000     1329.0       5524.0  ...     4.9443  2.5046  3.0953
precision       0.7594        NaN          NaN  ...        NaN     NaN     NaN
recall          0.4677        NaN          NaN  ...        NaN     NaN     NaN
f1              0.5789        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.1997        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.5789 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 1 	 loss 304.1792175475857
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1312.0000      260.0       1572.0  ...     0.8346  0.7718  0.8020
Arg1         2725.0000      582.0       3307.0  ...     0.8240  0.6757  0.7425
Arg2          957.0000      440.0       1397.0  ...     0.6850  0.5918  0.6350
Arg3           14.0000        0.0         14.0  ...     1.0000  0.1359  0.2393
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      143.0000      123.0        266.0  ...     0.5376  0.3365  0.4139
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       10.0000        9.0         19.0  ...     0.5263  0.1429  0.2247
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       30.0000       26.0         56.0  ...     0.5357  0.1840  0.2740
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       21.0000       17.0         38.0  ...     0.5526  0.1810  0.2727
ArgM_MNS        2.0000        0.0          2.0  ...     1.0000  0.0667  0.1250
ArgM_NEG       58.0000       20.0         78.0  ...     0.7436  0.7342  0.7389
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX       15.0000        5.0         20.0  ...     0.7500  0.1705  0.2778
ArgM_REC       32.0000        1.0         33.0  ...     0.9697  0.5000  0.6598
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       73.0000       62.0        135.0  ...     0.5407  0.3349  0.4136
sum          5392.0000     1545.0       6937.0  ...     9.4999  4.8258  5.8192
precision       0.7773        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6012        NaN          NaN  ...        NaN     NaN     NaN
f1              0.6780        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.3754        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.678 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 2 	 loss 205.514308473852
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1358.0000      325.0       1683.0  ...     0.8069  0.7988  0.8028
Arg1         2931.0000      650.0       3581.0  ...     0.8185  0.7268  0.7699
Arg2         1000.0000      441.0       1441.0  ...     0.6940  0.6184  0.6540
Arg3           16.0000        4.0         20.0  ...     0.8000  0.1553  0.2602
Arg4            3.0000        0.0          3.0  ...     1.0000  0.0698  0.1304
Arg5            1.0000        1.0          2.0  ...     0.5000  0.0588  0.1053
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      167.0000      122.0        289.0  ...     0.5779  0.3929  0.4678
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       17.0000       23.0         40.0  ...     0.4250  0.2429  0.3091
ArgM_CMP        1.0000        0.0          1.0  ...     1.0000  0.0455  0.0870
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       44.0000       28.0         72.0  ...     0.6111  0.2699  0.3745
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       34.0000       28.0         62.0  ...     0.5484  0.2931  0.3820
ArgM_MNS        5.0000        9.0         14.0  ...     0.3571  0.1667  0.2273
ArgM_NEG       62.0000       17.0         79.0  ...     0.7848  0.7848  0.7848
ArgM_PRP        6.0000        0.0          6.0  ...     1.0000  0.2069  0.3429
ArgM_PRX       17.0000        7.0         24.0  ...     0.7083  0.1932  0.3036
ArgM_REC       41.0000        3.0         44.0  ...     0.9318  0.6406  0.7593
ArgM_SCP        4.0000        7.0         11.0  ...     0.3636  0.0870  0.1404
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP       95.0000       76.0        171.0  ...     0.5556  0.4358  0.4884
sum          5802.0000     1741.0       7543.0  ...    12.4830  6.1871  7.3895
precision       0.7692        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6469        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7028        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.4767        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.7028 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 3 	 loss 153.51527459608042
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1406.0000      369.0       1775.0  ...     0.7921  0.8271  0.8092
Arg1         2912.0000      646.0       3558.0  ...     0.8184  0.7220  0.7672
Arg2          972.0000      364.0       1336.0  ...     0.7275  0.6011  0.6583
Arg3           18.0000        5.0         23.0  ...     0.7826  0.1748  0.2857
Arg4            4.0000        6.0         10.0  ...     0.4000  0.0930  0.1509
Arg5            2.0000        3.0          5.0  ...     0.4000  0.1176  0.1818
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      185.0000      112.0        297.0  ...     0.6229  0.4353  0.5125
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       24.0000       19.0         43.0  ...     0.5581  0.3429  0.4248
ArgM_CMP        3.0000        4.0          7.0  ...     0.4286  0.1364  0.2069
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        6.0000        0.0          6.0  ...     1.0000  0.2143  0.3529
ArgM_EXT        2.0000        2.0          4.0  ...     0.5000  0.0426  0.0784
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       51.0000       25.0         76.0  ...     0.6711  0.3129  0.4268
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       37.0000       50.0         87.0  ...     0.4253  0.3190  0.3645
ArgM_MNS        8.0000       17.0         25.0  ...     0.3200  0.2667  0.2909
ArgM_NEG       68.0000       20.0         88.0  ...     0.7727  0.8608  0.8144
ArgM_PRP        6.0000        2.0          8.0  ...     0.7500  0.2069  0.3243
ArgM_PRX       28.0000       12.0         40.0  ...     0.7000  0.3182  0.4375
ArgM_REC       42.0000        4.0         46.0  ...     0.9130  0.6562  0.7636
ArgM_SCP        5.0000       23.0         28.0  ...     0.1786  0.1087  0.1351
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      100.0000       53.0        153.0  ...     0.6536  0.4587  0.5391
sum          5879.0000     1736.0       7615.0  ...    12.4146  7.2150  8.5250
precision       0.7720        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6555        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7090        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.5500        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.709 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 4 	 loss 117.11897747701732
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1383.0000      305.0       1688.0  ...     0.8193  0.8135  0.8164
Arg1         2941.0000      662.0       3603.0  ...     0.8163  0.7292  0.7703
Arg2          990.0000      361.0       1351.0  ...     0.7328  0.6122  0.6671
Arg3           18.0000       12.0         30.0  ...     0.6000  0.1748  0.2707
Arg4            7.0000       10.0         17.0  ...     0.4118  0.1628  0.2333
Arg5            5.0000        1.0          6.0  ...     0.8333  0.2941  0.4348
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      221.0000      146.0        367.0  ...     0.6022  0.5200  0.5581
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       26.0000       23.0         49.0  ...     0.5306  0.3714  0.4370
ArgM_CMP        4.0000        6.0         10.0  ...     0.4000  0.1818  0.2500
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        7.0000        1.0          8.0  ...     0.8750  0.2500  0.3889
ArgM_EXT        4.0000        7.0         11.0  ...     0.3636  0.0851  0.1379
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       32.0000       15.0         47.0  ...     0.6809  0.1963  0.3048
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       41.0000       36.0         77.0  ...     0.5325  0.3534  0.4249
ArgM_MNS        7.0000       13.0         20.0  ...     0.3500  0.2333  0.2800
ArgM_NEG       67.0000       12.0         79.0  ...     0.8481  0.8481  0.8481
ArgM_PRP        7.0000        3.0         10.0  ...     0.7000  0.2414  0.3590
ArgM_PRX       26.0000       17.0         43.0  ...     0.6047  0.2955  0.3969
ArgM_REC       40.0000        3.0         43.0  ...     0.9302  0.6250  0.7477
ArgM_SCP        6.0000       21.0         27.0  ...     0.2222  0.1304  0.1644
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      126.0000       81.0        207.0  ...     0.6087  0.5780  0.5929
sum          5958.0000     1735.0       7693.0  ...    12.4621  7.6965  9.0831
precision       0.7745        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6643        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7152        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.5860        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.7152 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 5 	 loss 96.04506228857645
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1378.0000      298.0       1676.0  ...     0.8222  0.8106   0.8164
Arg1         3040.0000      631.0       3671.0  ...     0.8281  0.7538   0.7892
Arg2         1028.0000      377.0       1405.0  ...     0.7317  0.6357   0.6803
Arg3           23.0000       12.0         35.0  ...     0.6571  0.2233   0.3333
Arg4           11.0000        8.0         19.0  ...     0.5789  0.2558   0.3548
Arg5            5.0000        1.0          6.0  ...     0.8333  0.2941   0.4348
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      191.0000      107.0        298.0  ...     0.6409  0.4494   0.5284
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       24.0000       19.0         43.0  ...     0.5581  0.3429   0.4248
ArgM_CMP        5.0000       11.0         16.0  ...     0.3125  0.2273   0.2632
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        0.0          1.0  ...     1.0000  0.1250   0.2222
ArgM_DIR        9.0000        3.0         12.0  ...     0.7500  0.3214   0.4500
ArgM_EXT        8.0000       11.0         19.0  ...     0.4211  0.1702   0.2424
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       81.0000       39.0        120.0  ...     0.6750  0.4969   0.5724
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       47.0000       43.0         90.0  ...     0.5222  0.4052   0.4563
ArgM_MNS        8.0000       15.0         23.0  ...     0.3478  0.2667   0.3019
ArgM_NEG       68.0000       15.0         83.0  ...     0.8193  0.8608   0.8395
ArgM_PRP        9.0000        5.0         14.0  ...     0.6429  0.3103   0.4186
ArgM_PRX       30.0000       23.0         53.0  ...     0.5660  0.3409   0.4255
ArgM_REC       38.0000        2.0         40.0  ...     0.9500  0.5938   0.7308
ArgM_SCP        8.0000       27.0         35.0  ...     0.2286  0.1739   0.1975
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      127.0000       72.0        199.0  ...     0.6382  0.5826   0.6091
sum          6139.0000     1719.0       7858.0  ...    13.5240  8.6405  10.0915
precision       0.7812        NaN          NaN  ...        NaN     NaN      NaN
recall          0.6845        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7297        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6511        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7297 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 6 	 loss 78.0950909037274
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1412.0000      339.0       1751.0  ...     0.8064  0.8306   0.8183
Arg1         3106.0000      668.0       3774.0  ...     0.8230  0.7701   0.7957
Arg2         1062.0000      423.0       1485.0  ...     0.7152  0.6568   0.6847
Arg3           24.0000       16.0         40.0  ...     0.6000  0.2330   0.3357
Arg4           10.0000       10.0         20.0  ...     0.5000  0.2326   0.3175
Arg5            6.0000        2.0          8.0  ...     0.7500  0.3529   0.4800
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      201.0000      104.0        305.0  ...     0.6590  0.4729   0.5507
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       28.0000       30.0         58.0  ...     0.4828  0.4000   0.4375
ArgM_CMP        5.0000       11.0         16.0  ...     0.3125  0.2273   0.2632
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        2.0          3.0  ...     0.3333  0.1250   0.1818
ArgM_DIR        9.0000       10.0         19.0  ...     0.4737  0.3214   0.3830
ArgM_EXT       10.0000       16.0         26.0  ...     0.3846  0.2128   0.2740
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       63.0000       30.0         93.0  ...     0.6774  0.3865   0.4922
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       54.0000       64.0        118.0  ...     0.4576  0.4655   0.4615
ArgM_MNS        7.0000        8.0         15.0  ...     0.4667  0.2333   0.3111
ArgM_NEG       69.0000       20.0         89.0  ...     0.7753  0.8734   0.8214
ArgM_PRP        8.0000        6.0         14.0  ...     0.5714  0.2759   0.3721
ArgM_PRX       31.0000       27.0         58.0  ...     0.5345  0.3523   0.4247
ArgM_REC       44.0000        0.0         44.0  ...     1.0000  0.6875   0.8148
ArgM_SCP       10.0000       30.0         40.0  ...     0.2500  0.2174   0.2326
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      132.0000       83.0        215.0  ...     0.6140  0.6055   0.6097
sum          6292.0000     1899.0       8191.0  ...    12.1873  8.9327  10.0621
precision       0.7682        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7015        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7333        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6492        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7333 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 7 	 loss 72.59088859778512
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1434.0000      377.0       1811.0  ...     0.7918  0.8435   0.8169
Arg1         3091.0000      666.0       3757.0  ...     0.8227  0.7664   0.7936
Arg2         1071.0000      414.0       1485.0  ...     0.7212  0.6623   0.6905
Arg3           25.0000       14.0         39.0  ...     0.6410  0.2427   0.3521
Arg4           10.0000        8.0         18.0  ...     0.5556  0.2326   0.3279
Arg5            6.0000        0.0          6.0  ...     1.0000  0.3529   0.5217
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      220.0000      145.0        365.0  ...     0.6027  0.5176   0.5570
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       30.0000       32.0         62.0  ...     0.4839  0.4286   0.4545
ArgM_CMP        5.0000       12.0         17.0  ...     0.2941  0.2273   0.2564
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CRT        1.0000        1.0          2.0  ...     0.5000  0.1250   0.2000
ArgM_DIR       13.0000        6.0         19.0  ...     0.6842  0.4643   0.5532
ArgM_EXT        8.0000       13.0         21.0  ...     0.3810  0.1702   0.2353
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       82.0000       43.0        125.0  ...     0.6560  0.5031   0.5694
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       56.0000       51.0        107.0  ...     0.5234  0.4828   0.5022
ArgM_MNS        6.0000       12.0         18.0  ...     0.3333  0.2000   0.2500
ArgM_NEG       67.0000       18.0         85.0  ...     0.7882  0.8481   0.8171
ArgM_PRP       10.0000        6.0         16.0  ...     0.6250  0.3448   0.4444
ArgM_PRX       33.0000       29.0         62.0  ...     0.5323  0.3750   0.4400
ArgM_REC       44.0000        8.0         52.0  ...     0.8462  0.6875   0.7586
ArgM_SCP        9.0000       40.0         49.0  ...     0.1837  0.1957   0.1895
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      147.0000       82.0        229.0  ...     0.6419  0.6743   0.6577
sum          6368.0000     1977.0       8345.0  ...    12.6082  9.3447  10.3881
precision       0.7631        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7100        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7356        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6702        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7356 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 8 	 loss 61.23969194867868
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1447.0000      365.0       1812.0  ...     0.7986  0.8512   0.8240
Arg1         3120.0000      670.0       3790.0  ...     0.8232  0.7736   0.7976
Arg2         1074.0000      415.0       1489.0  ...     0.7213  0.6642   0.6916
Arg3           19.0000        7.0         26.0  ...     0.7308  0.1845   0.2946
Arg4            9.0000        7.0         16.0  ...     0.5625  0.2093   0.3051
Arg5            4.0000        1.0          5.0  ...     0.8000  0.2353   0.3636
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      226.0000      149.0        375.0  ...     0.6027  0.5318   0.5650
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       32.0000       34.0         66.0  ...     0.4848  0.4571   0.4706
ArgM_CMP        5.0000       12.0         17.0  ...     0.2941  0.2273   0.2564
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        0.0          2.0  ...     1.0000  0.2500   0.4000
ArgM_DIR       14.0000       11.0         25.0  ...     0.5600  0.5000   0.5283
ArgM_EXT       14.0000       25.0         39.0  ...     0.3590  0.2979   0.3256
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       77.0000       57.0        134.0  ...     0.5746  0.4724   0.5185
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       57.0000       63.0        120.0  ...     0.4750  0.4914   0.4831
ArgM_MNS        8.0000       14.0         22.0  ...     0.3636  0.2667   0.3077
ArgM_NEG       67.0000       18.0         85.0  ...     0.7882  0.8481   0.8171
ArgM_PRP       12.0000        7.0         19.0  ...     0.6316  0.4138   0.5000
ArgM_PRX       37.0000       29.0         66.0  ...     0.5606  0.4205   0.4805
ArgM_REC       42.0000        3.0         45.0  ...     0.9333  0.6562   0.7706
ArgM_SCP        7.0000       34.0         41.0  ...     0.1707  0.1522   0.1609
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      134.0000       88.0        222.0  ...     0.6036  0.6147   0.6091
sum          6408.0000     2010.0       8418.0  ...    13.8383  9.7180  10.8033
precision       0.7612        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7145        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7371        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.6970        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7371 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 9 	 loss 56.59169330140503
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0         1360.0000      275.0       1635.0  ...     0.8318  0.8000  0.8156
Arg1         2911.0000      669.0       3580.0  ...     0.8131  0.7218  0.7647
Arg2         1050.0000      475.0       1525.0  ...     0.6885  0.6494  0.6684
Arg3           15.0000        7.0         22.0  ...     0.6818  0.1456  0.2400
Arg4           13.0000       11.0         24.0  ...     0.5417  0.3023  0.3881
Arg5            7.0000        0.0          7.0  ...     1.0000  0.4118  0.5833
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV      166.0000       88.0        254.0  ...     0.6535  0.3906  0.4890
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU       26.0000       21.0         47.0  ...     0.5532  0.3714  0.4444
ArgM_CMP        4.0000        8.0         12.0  ...     0.3333  0.1818  0.2353
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        1.0000        0.0          1.0  ...     1.0000  0.1250  0.2222
ArgM_DIR       12.0000        9.0         21.0  ...     0.5714  0.4286  0.4898
ArgM_EXT       14.0000       25.0         39.0  ...     0.3590  0.2979  0.3256
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC       70.0000       34.0        104.0  ...     0.6731  0.4294  0.5243
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR       48.0000       46.0         94.0  ...     0.5106  0.4138  0.4571
ArgM_MNS        5.0000       10.0         15.0  ...     0.3333  0.1667  0.2222
ArgM_NEG       64.0000       14.0         78.0  ...     0.8205  0.8101  0.8153
ArgM_PRP       11.0000        7.0         18.0  ...     0.6111  0.3793  0.4681
ArgM_PRX       30.0000       11.0         41.0  ...     0.7317  0.3409  0.4651
ArgM_REC       32.0000        0.0         32.0  ...     1.0000  0.5000  0.6667
ArgM_SCP        3.0000        9.0         12.0  ...     0.2500  0.0652  0.1034
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP      117.0000       69.0        186.0  ...     0.6290  0.5367  0.5792
sum          5959.0000     1789.0       7748.0  ...    13.5868  8.4683  9.9679
precision       0.7691        NaN          NaN  ...        NaN     NaN     NaN
recall          0.6644        NaN          NaN  ...        NaN     NaN     NaN
f1              0.7129        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.6431        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 10 	 loss 56.215595457831114
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1412.0000      307.0       1719.0  ...     0.8214  0.8306   0.8260
Arg1         3097.0000      640.0       3737.0  ...     0.8287  0.7679   0.7972
Arg2         1051.0000      385.0       1436.0  ...     0.7319  0.6500   0.6885
Arg3           21.0000       19.0         40.0  ...     0.5250  0.2039   0.2937
Arg4           11.0000        9.0         20.0  ...     0.5500  0.2558   0.3492
Arg5            8.0000        0.0          8.0  ...     1.0000  0.4706   0.6400
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      210.0000      105.0        315.0  ...     0.6667  0.4941   0.5676
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       24.0000       22.0         46.0  ...     0.5217  0.3429   0.4138
ArgM_CMP        5.0000       13.0         18.0  ...     0.2778  0.2273   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000  0.2500   0.3333
ArgM_DIR       13.0000        8.0         21.0  ...     0.6190  0.4643   0.5306
ArgM_EXT       10.0000       21.0         31.0  ...     0.3226  0.2128   0.2564
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_LOC       87.0000       52.0        139.0  ...     0.6259  0.5337   0.5762
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       57.0000       50.0        107.0  ...     0.5327  0.4914   0.5112
ArgM_MNS        7.0000       17.0         24.0  ...     0.2917  0.2333   0.2593
ArgM_NEG       63.0000       14.0         77.0  ...     0.8182  0.7975   0.8077
ArgM_PRP       10.0000        6.0         16.0  ...     0.6250  0.3448   0.4444
ArgM_PRX       28.0000       29.0         57.0  ...     0.4912  0.3182   0.3862
ArgM_REC       49.0000        4.0         53.0  ...     0.9245  0.7656   0.8376
ArgM_SCP       13.0000       39.0         52.0  ...     0.2500  0.2826   0.2653
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      131.0000       74.0        205.0  ...     0.6390  0.6009   0.6194
sum          6310.0000     1816.0       8126.0  ...    13.5631  9.7381  10.9869
precision       0.7765        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7035        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7382        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.7088        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7382 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 11 	 loss 51.301266960780595
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1436.0000      309.0       1745.0  ...     0.8229   0.8447   0.8337
Arg1         3210.0000      649.0       3859.0  ...     0.8318   0.7959   0.8135
Arg2         1082.0000      407.0       1489.0  ...     0.7267   0.6691   0.6967
Arg3           27.0000       30.0         57.0  ...     0.4737   0.2621   0.3375
Arg4           12.0000       11.0         23.0  ...     0.5217   0.2791   0.3636
Arg5            7.0000        1.0          8.0  ...     0.8750   0.4118   0.5600
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      212.0000      120.0        332.0  ...     0.6386   0.4988   0.5601
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       24.0000       25.0         49.0  ...     0.4898   0.3429   0.4034
ArgM_CMP        5.0000       14.0         19.0  ...     0.2632   0.2273   0.2439
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000   0.2500   0.3333
ArgM_DIR       12.0000       12.0         24.0  ...     0.5000   0.4286   0.4615
ArgM_EXT       12.0000       32.0         44.0  ...     0.2727   0.2553   0.2637
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       88.0000       52.0        140.0  ...     0.6286   0.5399   0.5809
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       64.0000       75.0        139.0  ...     0.4604   0.5517   0.5020
ArgM_MNS        9.0000       23.0         32.0  ...     0.2812   0.3000   0.2903
ArgM_NEG       67.0000       15.0         82.0  ...     0.8171   0.8481   0.8323
ArgM_PRP       11.0000       10.0         21.0  ...     0.5238   0.3793   0.4400
ArgM_PRX       35.0000       35.0         70.0  ...     0.5000   0.3977   0.4430
ArgM_REC       43.0000        3.0         46.0  ...     0.9348   0.6719   0.7818
ArgM_SCP       20.0000       50.0         70.0  ...     0.2857   0.4348   0.3448
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      141.0000       76.0        217.0  ...     0.6498   0.6468   0.6483
sum          6520.0000     1954.0       8474.0  ...    12.4975  10.2358  11.0201
precision       0.7694        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7269        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7476        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7110        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7476 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 12 	 loss 45.524705889021334
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1451.0000      354.0       1805.0  ...     0.8039   0.8535   0.8280
Arg1         3124.0000      640.0       3764.0  ...     0.8300   0.7746   0.8013
Arg2         1107.0000      482.0       1589.0  ...     0.6967   0.6846   0.6906
Arg3           26.0000       12.0         38.0  ...     0.6842   0.2524   0.3688
Arg4           11.0000       10.0         21.0  ...     0.5238   0.2558   0.3438
Arg5            5.0000        2.0          7.0  ...     0.7143   0.2941   0.4167
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      218.0000      131.0        349.0  ...     0.6246   0.5129   0.5633
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       33.0         60.0  ...     0.4500   0.3857   0.4154
ArgM_CMP        5.0000       13.0         18.0  ...     0.2778   0.2273   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        2.0000        0.0          2.0  ...     1.0000   0.2500   0.4000
ArgM_DIR       12.0000        6.0         18.0  ...     0.6667   0.4286   0.5217
ArgM_EXT       14.0000       23.0         37.0  ...     0.3784   0.2979   0.3333
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       81.0000       49.0        130.0  ...     0.6231   0.4969   0.5529
ArgM_MDF        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       57.0        118.0  ...     0.5169   0.5259   0.5214
ArgM_MNS       10.0000       18.0         28.0  ...     0.3571   0.3333   0.3448
ArgM_NEG       70.0000       18.0         88.0  ...     0.7955   0.8861   0.8383
ArgM_PRP       13.0000       10.0         23.0  ...     0.5652   0.4483   0.5000
ArgM_PRX       28.0000       18.0         46.0  ...     0.6087   0.3182   0.4179
ArgM_REC       46.0000        3.0         49.0  ...     0.9388   0.7188   0.8142
ArgM_SCP       13.0000       46.0         59.0  ...     0.2203   0.2826   0.2476
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      145.0000       79.0        224.0  ...     0.6473   0.6651   0.6561
sum          6470.0000     2008.0       8478.0  ...    13.9233  10.0926  11.1594
precision       0.7632        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7214        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7417        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7200        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 13 	 loss 40.963574874829646
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1461.0000      340.0       1801.0  ...     0.8112   0.8594   0.8346
Arg1         3227.0000      621.0       3848.0  ...     0.8386   0.8001   0.8189
Arg2         1071.0000      372.0       1443.0  ...     0.7422   0.6623   0.7000
Arg3           25.0000       26.0         51.0  ...     0.4902   0.2427   0.3247
Arg4            8.0000        9.0         17.0  ...     0.4706   0.1860   0.2667
Arg5            4.0000        2.0          6.0  ...     0.6667   0.2353   0.3478
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      205.0000      137.0        342.0  ...     0.5994   0.4824   0.5346
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       26.0000       28.0         54.0  ...     0.4815   0.3714   0.4194
ArgM_CMP        5.0000       16.0         21.0  ...     0.2381   0.2273   0.2326
ArgM_CND        1.0000        3.0          4.0  ...     0.2500   0.2000   0.2222
ArgM_CRT        2.0000        7.0          9.0  ...     0.2222   0.2500   0.2353
ArgM_DIR       12.0000       12.0         24.0  ...     0.5000   0.4286   0.4615
ArgM_EXT       15.0000       32.0         47.0  ...     0.3191   0.3191   0.3191
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC      100.0000       77.0        177.0  ...     0.5650   0.6135   0.5882
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       61.0000       72.0        133.0  ...     0.4586   0.5259   0.4900
ArgM_MNS        8.0000       17.0         25.0  ...     0.3200   0.2667   0.2909
ArgM_NEG       70.0000       10.0         80.0  ...     0.8750   0.8861   0.8805
ArgM_PRP       15.0000        9.0         24.0  ...     0.6250   0.5172   0.5660
ArgM_PRX       34.0000       41.0         75.0  ...     0.4533   0.3864   0.4172
ArgM_REC       51.0000       11.0         62.0  ...     0.8226   0.7969   0.8095
ArgM_SCP       15.0000       48.0         63.0  ...     0.2381   0.3261   0.2752
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      146.0000       86.0        232.0  ...     0.6293   0.6697   0.6489
sum          6562.0000     1977.0       8539.0  ...    11.6168  10.2531  10.6839
precision       0.7685        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7316        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7496        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.6893        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7496 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 14 	 loss 43.35606135417197
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1397.0000      266.0       1663.0  ...     0.8400   0.8218   0.8308
Arg1         3205.0000      700.0       3905.0  ...     0.8207   0.7947   0.8075
Arg2         1101.0000      429.0       1530.0  ...     0.7196   0.6809   0.6997
Arg3           29.0000       40.0         69.0  ...     0.4203   0.2816   0.3372
Arg4           13.0000       12.0         25.0  ...     0.5200   0.3023   0.3824
Arg5            7.0000        1.0          8.0  ...     0.8750   0.4118   0.5600
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      204.0000      113.0        317.0  ...     0.6435   0.4800   0.5499
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       33.0         62.0  ...     0.4677   0.4143   0.4394
ArgM_CMP        6.0000       19.0         25.0  ...     0.2400   0.2727   0.2553
ArgM_CND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CRT        4.0000        1.0          5.0  ...     0.8000   0.5000   0.6154
ArgM_DIR        9.0000        3.0         12.0  ...     0.7500   0.3214   0.4500
ArgM_EXT       23.0000       41.0         64.0  ...     0.3594   0.4894   0.4144
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       92.0000       56.0        148.0  ...     0.6216   0.5644   0.5916
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       56.0000       55.0        111.0  ...     0.5045   0.4828   0.4934
ArgM_MNS        8.0000       18.0         26.0  ...     0.3077   0.2667   0.2857
ArgM_NEG       67.0000       12.0         79.0  ...     0.8481   0.8481   0.8481
ArgM_PRP       11.0000       11.0         22.0  ...     0.5000   0.3793   0.4314
ArgM_PRX       32.0000       43.0         75.0  ...     0.4267   0.3636   0.3926
ArgM_REC       46.0000        3.0         49.0  ...     0.9388   0.7188   0.8142
ArgM_SCP       13.0000       58.0         71.0  ...     0.1831   0.2826   0.2222
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      134.0000       69.0        203.0  ...     0.6601   0.6147   0.6366
sum          6486.0000     1984.0       8470.0  ...    12.4469  10.2917  11.0578
precision       0.7658        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7232        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7438        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7134        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 15 	 loss 42.0969589611517
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1420.0000      275.0       1695.0  ...     0.8378   0.8353   0.8365
Arg1         3231.0000      696.0       3927.0  ...     0.8228   0.8011   0.8118
Arg2         1109.0000      388.0       1497.0  ...     0.7408   0.6858   0.7123
Arg3           28.0000       25.0         53.0  ...     0.5283   0.2718   0.3590
Arg4           14.0000       11.0         25.0  ...     0.5600   0.3256   0.4118
Arg5            5.0000        1.0          6.0  ...     0.8333   0.2941   0.4348
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      211.0000      147.0        358.0  ...     0.5894   0.4965   0.5390
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       29.0         57.0  ...     0.4912   0.4000   0.4409
ArgM_CMP        5.0000       18.0         23.0  ...     0.2174   0.2273   0.2222
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        4.0          6.0  ...     0.3333   0.2500   0.2857
ArgM_DIR       12.0000        7.0         19.0  ...     0.6316   0.4286   0.5106
ArgM_EXT       15.0000       23.0         38.0  ...     0.3947   0.3191   0.3529
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       92.0000       50.0        142.0  ...     0.6479   0.5644   0.6033
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       65.0000       59.0        124.0  ...     0.5242   0.5603   0.5417
ArgM_MNS        7.0000       15.0         22.0  ...     0.3182   0.2333   0.2692
ArgM_NEG       68.0000       11.0         79.0  ...     0.8608   0.8608   0.8608
ArgM_PRP       14.0000       14.0         28.0  ...     0.5000   0.4828   0.4912
ArgM_PRX       34.0000       41.0         75.0  ...     0.4533   0.3864   0.4172
ArgM_REC       47.0000        5.0         52.0  ...     0.9038   0.7344   0.8103
ArgM_SCP       13.0000       59.0         72.0  ...     0.1806   0.2826   0.2203
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      144.0000       78.0        222.0  ...     0.6486   0.6606   0.6545
sum          6565.0000     1960.0       8525.0  ...    12.5180  10.3008  11.0718
precision       0.7701        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7320        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7505        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7143        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7505 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 16 	 loss 37.349715961196466
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1437.0000      284.0       1721.0  ...     0.8350   0.8453   0.8401
Arg1         3161.0000      626.0       3787.0  ...     0.8347   0.7838   0.8084
Arg2         1115.0000      427.0       1542.0  ...     0.7231   0.6895   0.7059
Arg3           28.0000       36.0         64.0  ...     0.4375   0.2718   0.3353
Arg4           11.0000       14.0         25.0  ...     0.4400   0.2558   0.3235
Arg5            6.0000        0.0          6.0  ...     1.0000   0.3529   0.5217
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      205.0000      131.0        336.0  ...     0.6101   0.4824   0.5388
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       31.0         58.0  ...     0.4655   0.3857   0.4219
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        1.0000        7.0          8.0  ...     0.1250   0.1250   0.1250
ArgM_DIR       11.0000       12.0         23.0  ...     0.4783   0.3929   0.4314
ArgM_EXT       19.0000       32.0         51.0  ...     0.3725   0.4043   0.3878
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       97.0000       71.0        168.0  ...     0.5774   0.5951   0.5861
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       65.0000       61.0        126.0  ...     0.5159   0.5603   0.5372
ArgM_MNS        9.0000       20.0         29.0  ...     0.3103   0.3000   0.3051
ArgM_NEG       67.0000       14.0         81.0  ...     0.8272   0.8481   0.8375
ArgM_PRP       13.0000        8.0         21.0  ...     0.6190   0.4483   0.5200
ArgM_PRX       32.0000       33.0         65.0  ...     0.4923   0.3636   0.4183
ArgM_REC       43.0000        4.0         47.0  ...     0.9149   0.6719   0.7748
ArgM_SCP       13.0000       49.0         62.0  ...     0.2097   0.2826   0.2407
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      132.0000       61.0        193.0  ...     0.6839   0.6055   0.6423
sum          6498.0000     1940.0       8438.0  ...    12.2223  10.0921  10.8257
precision       0.7701        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7245        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7466        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.6984        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 17 	 loss 36.533670570040044
Validation Start

           correct_num  wrong_num  predict_num  ...  precision  recall       f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN      NaN
Arg0         1428.0000      283.0       1711.0  ...     0.8346  0.8400   0.8373
Arg1         3158.0000      610.0       3768.0  ...     0.8381  0.7830   0.8096
Arg2         1105.0000      379.0       1484.0  ...     0.7446  0.6834   0.7127
Arg3           26.0000       18.0         44.0  ...     0.5909  0.2524   0.3537
Arg4            9.0000        9.0         18.0  ...     0.5000  0.2093   0.2951
Arg5            5.0000        0.0          5.0  ...     1.0000  0.2941   0.4545
ArgA            0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_ADV      218.0000      145.0        363.0  ...     0.6006  0.5129   0.5533
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_CAU       29.0000       32.0         61.0  ...     0.4754  0.4143   0.4427
ArgM_CMP        5.0000       14.0         19.0  ...     0.2632  0.2273   0.2439
ArgM_CND        1.0000        0.0          1.0  ...     1.0000  0.2000   0.3333
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000  0.2500   0.3333
ArgM_DIR       12.0000        8.0         20.0  ...     0.6000  0.4286   0.5000
ArgM_EXT       10.0000       16.0         26.0  ...     0.3846  0.2128   0.2740
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000  0.0000      NaN
ArgM_LOC       85.0000       57.0        142.0  ...     0.5986  0.5215   0.5574
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000      NaN
ArgM_MNR       51.0000       38.0         89.0  ...     0.5730  0.4397   0.4976
ArgM_MNS        7.0000       21.0         28.0  ...     0.2500  0.2333   0.2414
ArgM_NEG       65.0000       11.0         76.0  ...     0.8553  0.8228   0.8387
ArgM_PRP       15.0000        7.0         22.0  ...     0.6818  0.5172   0.5882
ArgM_PRX       38.0000       45.0         83.0  ...     0.4578  0.4318   0.4444
ArgM_REC       46.0000        5.0         51.0  ...     0.9020  0.7188   0.8000
ArgM_SCP       13.0000       60.0         73.0  ...     0.1781  0.2826   0.2185
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN      NaN
ArgM_TMP      136.0000       67.0        203.0  ...     0.6700  0.6239   0.6461
sum          6464.0000     1829.0       8293.0  ...    13.4985  9.8996  10.9758
precision       0.7795        NaN          NaN  ...        NaN     NaN      NaN
recall          0.7207        NaN          NaN  ...        NaN     NaN      NaN
f1              0.7489        NaN          NaN  ...        NaN     NaN      NaN
f1_macro        0.7081        NaN          NaN  ...        NaN     NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 18 	 loss 37.32570233408467
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1410.0000      277.0       1687.0  ...     0.8358   0.8294   0.8326
Arg1         3223.0000      631.0       3854.0  ...     0.8363   0.7992   0.8173
Arg2         1123.0000      410.0       1533.0  ...     0.7326   0.6945   0.7130
Arg3           26.0000       30.0         56.0  ...     0.4643   0.2524   0.3270
Arg4            8.0000       13.0         21.0  ...     0.3810   0.1860   0.2500
Arg5            5.0000        2.0          7.0  ...     0.7143   0.2941   0.4167
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      195.0000      115.0        310.0  ...     0.6290   0.4588   0.5306
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       27.0000       30.0         57.0  ...     0.4737   0.3857   0.4252
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        2.0000        5.0          7.0  ...     0.2857   0.2500   0.2667
ArgM_DIR       12.0000        5.0         17.0  ...     0.7059   0.4286   0.5333
ArgM_EXT       16.0000       30.0         46.0  ...     0.3478   0.3404   0.3441
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       88.0000       51.0        139.0  ...     0.6331   0.5399   0.5828
ArgM_MDF        0.0000        4.0          4.0  ...     0.0000   0.0000      NaN
ArgM_MNR       61.0000       77.0        138.0  ...     0.4420   0.5259   0.4803
ArgM_MNS        8.0000       16.0         24.0  ...     0.3333   0.2667   0.2963
ArgM_NEG       66.0000       12.0         78.0  ...     0.8462   0.8354   0.8408
ArgM_PRP       17.0000       12.0         29.0  ...     0.5862   0.5862   0.5862
ArgM_PRX       36.0000       33.0         69.0  ...     0.5217   0.4091   0.4586
ArgM_REC       46.0000        8.0         54.0  ...     0.8519   0.7188   0.7797
ArgM_SCP       12.0000       51.0         63.0  ...     0.1905   0.2609   0.2202
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      139.0000       65.0        204.0  ...     0.6814   0.6376   0.6588
sum          6526.0000     1894.0       8420.0  ...    12.2425  10.1268  10.8839
precision       0.7751        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7276        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7506        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7022        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7506 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 19 	 loss 31.22844360372875
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1422.0000      270.0       1692.0  ...     0.8404   0.8365   0.8384
Arg1         3236.0000      620.0       3856.0  ...     0.8392   0.8024   0.8204
Arg2         1125.0000      403.0       1528.0  ...     0.7363   0.6957   0.7154
Arg3           28.0000       37.0         65.0  ...     0.4308   0.2718   0.3333
Arg4           13.0000       19.0         32.0  ...     0.4062   0.3023   0.3467
Arg5            5.0000        0.0          5.0  ...     1.0000   0.2941   0.4545
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      204.0000      121.0        325.0  ...     0.6277   0.4800   0.5440
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       32.0         61.0  ...     0.4754   0.4143   0.4427
ArgM_CMP        6.0000       20.0         26.0  ...     0.2308   0.2727   0.2500
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        4.0000        5.0          9.0  ...     0.4444   0.5000   0.4706
ArgM_DIR       13.0000       12.0         25.0  ...     0.5200   0.4643   0.4906
ArgM_EXT       19.0000       25.0         44.0  ...     0.4318   0.4043   0.4176
ArgM_GOL        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_LOC       98.0000       60.0        158.0  ...     0.6203   0.6012   0.6106
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       63.0000       60.0        123.0  ...     0.5122   0.5431   0.5272
ArgM_MNS       10.0000       21.0         31.0  ...     0.3226   0.3333   0.3279
ArgM_NEG       68.0000       14.0         82.0  ...     0.8293   0.8608   0.8447
ArgM_PRP       16.0000       10.0         26.0  ...     0.6154   0.5517   0.5818
ArgM_PRX       37.0000       42.0         79.0  ...     0.4684   0.4205   0.4431
ArgM_REC       45.0000        9.0         54.0  ...     0.8333   0.7031   0.7627
ArgM_SCP       13.0000       48.0         61.0  ...     0.2131   0.2826   0.2430
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      142.0000       72.0        214.0  ...     0.6636   0.6514   0.6574
sum          6597.0000     1901.0       8498.0  ...    13.0611  10.8861  11.4560
precision       0.7763        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7355        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7554        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7391        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7554 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 20 	 loss 33.136564429489
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1436.0000      315.0       1751.0  ...     0.8201   0.8447   0.8322
Arg1         3194.0000      627.0       3821.0  ...     0.8359   0.7920   0.8133
Arg2         1122.0000      423.0       1545.0  ...     0.7262   0.6939   0.7097
Arg3           28.0000       24.0         52.0  ...     0.5385   0.2718   0.3613
Arg4           12.0000       13.0         25.0  ...     0.4800   0.2791   0.3529
Arg5            6.0000        1.0          7.0  ...     0.8571   0.3529   0.5000
ArgA            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      232.0000      137.0        369.0  ...     0.6287   0.5459   0.5844
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       29.0000       27.0         56.0  ...     0.5179   0.4143   0.4603
ArgM_CMP        5.0000       15.0         20.0  ...     0.2500   0.2273   0.2381
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        3.0000        4.0          7.0  ...     0.4286   0.3750   0.4000
ArgM_DIR       12.0000        7.0         19.0  ...     0.6316   0.4286   0.5106
ArgM_EXT       15.0000       18.0         33.0  ...     0.4545   0.3191   0.3750
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC       90.0000       50.0        140.0  ...     0.6429   0.5521   0.5941
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       58.0000       47.0        105.0  ...     0.5524   0.5000   0.5249
ArgM_MNS        6.0000       23.0         29.0  ...     0.2069   0.2000   0.2034
ArgM_NEG       64.0000       11.0         75.0  ...     0.8533   0.8101   0.8312
ArgM_PRP       15.0000       15.0         30.0  ...     0.5000   0.5172   0.5085
ArgM_PRX       41.0000       36.0         77.0  ...     0.5325   0.4659   0.4970
ArgM_REC       47.0000       10.0         57.0  ...     0.8246   0.7344   0.7769
ArgM_SCP       13.0000       58.0         71.0  ...     0.1831   0.2826   0.2222
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      154.0000       82.0        236.0  ...     0.6525   0.7064   0.6784
sum          6583.0000     1949.0       8532.0  ...    13.1172  10.5134  11.3077
precision       0.7716        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7340        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7523        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7295        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 21 	 loss 35.51113609356719
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1441.0000      282.0       1723.0  ...     0.8363   0.8476   0.8420
Arg1         3252.0000      625.0       3877.0  ...     0.8388   0.8063   0.8223
Arg2         1105.0000      416.0       1521.0  ...     0.7265   0.6834   0.7043
Arg3           29.0000       23.0         52.0  ...     0.5577   0.2816   0.3742
Arg4           10.0000        9.0         19.0  ...     0.5263   0.2326   0.3226
Arg5            6.0000        2.0          8.0  ...     0.7500   0.3529   0.4800
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      226.0000      143.0        369.0  ...     0.6125   0.5318   0.5693
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       32.0         60.0  ...     0.4667   0.4000   0.4308
ArgM_CMP        5.0000       17.0         22.0  ...     0.2273   0.2273   0.2273
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        4.0000        6.0         10.0  ...     0.4000   0.5000   0.4444
ArgM_DIR       14.0000       15.0         29.0  ...     0.4828   0.5000   0.4912
ArgM_EXT       20.0000       22.0         42.0  ...     0.4762   0.4255   0.4494
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       92.0000       52.0        144.0  ...     0.6389   0.5644   0.5993
ArgM_MDF        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_MNR       59.0000       65.0        124.0  ...     0.4758   0.5086   0.4917
ArgM_MNS        9.0000       26.0         35.0  ...     0.2571   0.3000   0.2769
ArgM_NEG       68.0000       17.0         85.0  ...     0.8000   0.8608   0.8293
ArgM_PRP       14.0000       16.0         30.0  ...     0.4667   0.4828   0.4746
ArgM_PRX       40.0000       33.0         73.0  ...     0.5479   0.4545   0.4969
ArgM_REC       45.0000        4.0         49.0  ...     0.9184   0.7031   0.7965
ArgM_SCP       14.0000       74.0         88.0  ...     0.1591   0.3043   0.2090
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      143.0000       86.0        229.0  ...     0.6245   0.6560   0.6398
sum          6625.0000     1968.0       8593.0  ...    12.7893  10.8235  11.3049
precision       0.7710        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7387        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7545        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7293        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 22 	 loss 34.23499412534807
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1431.0000      243.0       1674.0  ...     0.8548   0.8418   0.8483
Arg1         3272.0000      662.0       3934.0  ...     0.8317   0.8113   0.8214
Arg2         1103.0000      391.0       1494.0  ...     0.7383   0.6821   0.7091
Arg3           31.0000       28.0         59.0  ...     0.5254   0.3010   0.3827
Arg4           15.0000       20.0         35.0  ...     0.4286   0.3488   0.3846
Arg5            7.0000        2.0          9.0  ...     0.7778   0.4118   0.5385
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      217.0000      106.0        323.0  ...     0.6718   0.5106   0.5802
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       28.0000       30.0         58.0  ...     0.4828   0.4000   0.4375
ArgM_CMP        4.0000       15.0         19.0  ...     0.2105   0.1818   0.1951
ArgM_CND        1.0000        1.0          2.0  ...     0.5000   0.2000   0.2857
ArgM_CRT        3.0000        2.0          5.0  ...     0.6000   0.3750   0.4615
ArgM_DIR       14.0000        7.0         21.0  ...     0.6667   0.5000   0.5714
ArgM_EXT       24.0000       33.0         57.0  ...     0.4211   0.5106   0.4615
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC      104.0000       82.0        186.0  ...     0.5591   0.6380   0.5960
ArgM_MDF        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_MNR       67.0000       54.0        121.0  ...     0.5537   0.5776   0.5654
ArgM_MNS        8.0000       20.0         28.0  ...     0.2857   0.2667   0.2759
ArgM_NEG       68.0000       11.0         79.0  ...     0.8608   0.8608   0.8608
ArgM_PRP       13.0000        7.0         20.0  ...     0.6500   0.4483   0.5306
ArgM_PRX       35.0000       39.0         74.0  ...     0.4730   0.3977   0.4321
ArgM_REC       49.0000        5.0         54.0  ...     0.9074   0.7656   0.8305
ArgM_SCP       17.0000       58.0         75.0  ...     0.2267   0.3696   0.2810
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       86.0        226.0  ...     0.6195   0.6422   0.6306
sum          6651.0000     1907.0       8558.0  ...    12.8453  11.0413  11.6804
precision       0.7772        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7416        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7589        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7536        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
Valid f1 =  0.7589 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 23 	 loss 32.85184386499668
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1430.0000      263.0       1693.0  ...     0.8447   0.8412   0.8429
Arg1         3225.0000      626.0       3851.0  ...     0.8374   0.7997   0.8181
Arg2         1099.0000      399.0       1498.0  ...     0.7336   0.6797   0.7056
Arg3           33.0000       38.0         71.0  ...     0.4648   0.3204   0.3793
Arg4           14.0000       12.0         26.0  ...     0.5385   0.3256   0.4058
Arg5            6.0000        1.0          7.0  ...     0.8571   0.3529   0.5000
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      216.0000      140.0        356.0  ...     0.6067   0.5082   0.5531
ArgM_AND        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       32.0000       31.0         63.0  ...     0.5079   0.4571   0.4812
ArgM_CMP        6.0000       16.0         22.0  ...     0.2727   0.2727   0.2727
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        3.0000        3.0          6.0  ...     0.5000   0.3750   0.4286
ArgM_DIR       11.0000        6.0         17.0  ...     0.6471   0.3929   0.4889
ArgM_EXT       16.0000       28.0         44.0  ...     0.3636   0.3404   0.3516
ArgM_GOL        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_LOC       96.0000       59.0        155.0  ...     0.6194   0.5890   0.6038
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       65.0000       64.0        129.0  ...     0.5039   0.5603   0.5306
ArgM_MNS        9.0000       20.0         29.0  ...     0.3103   0.3000   0.3051
ArgM_NEG       65.0000       12.0         77.0  ...     0.8442   0.8228   0.8333
ArgM_PRP       16.0000       15.0         31.0  ...     0.5161   0.5517   0.5333
ArgM_PRX       38.0000       46.0         84.0  ...     0.4524   0.4318   0.4419
ArgM_REC       45.0000        7.0         52.0  ...     0.8654   0.7031   0.7759
ArgM_SCP       13.0000       41.0         54.0  ...     0.2407   0.2826   0.2600
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      140.0000       66.0        206.0  ...     0.6796   0.6422   0.6604
sum          6579.0000     1900.0       8479.0  ...    12.5395  10.7493  11.4222
precision       0.7759        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7335        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7541        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7369        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 24 	 loss 31.742785970751356
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1459.0000      289.0       1748.0  ...     0.8347   0.8582   0.8463
Arg1         3270.0000      642.0       3912.0  ...     0.8359   0.8108   0.8232
Arg2         1106.0000      414.0       1520.0  ...     0.7276   0.6840   0.7051
Arg3           33.0000       38.0         71.0  ...     0.4648   0.3204   0.3793
Arg4           15.0000       11.0         26.0  ...     0.5769   0.3488   0.4348
Arg5            7.0000        5.0         12.0  ...     0.5833   0.4118   0.4828
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      228.0000      175.0        403.0  ...     0.5658   0.5365   0.5507
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       33.0000       40.0         73.0  ...     0.4521   0.4714   0.4615
ArgM_CMP        5.0000       10.0         15.0  ...     0.3333   0.2273   0.2703
ArgM_CND        1.0000        2.0          3.0  ...     0.3333   0.2000   0.2500
ArgM_CRT        2.0000        2.0          4.0  ...     0.5000   0.2500   0.3333
ArgM_DIR       11.0000       14.0         25.0  ...     0.4400   0.3929   0.4151
ArgM_EXT       18.0000       31.0         49.0  ...     0.3673   0.3830   0.3750
ArgM_GOL        0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM_LOC       90.0000       68.0        158.0  ...     0.5696   0.5521   0.5607
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       69.0000       66.0        135.0  ...     0.5111   0.5948   0.5498
ArgM_MNS        8.0000       27.0         35.0  ...     0.2286   0.2667   0.2462
ArgM_NEG       72.0000       17.0         89.0  ...     0.8090   0.9114   0.8571
ArgM_PRP       16.0000       14.0         30.0  ...     0.5333   0.5517   0.5424
ArgM_PRX       38.0000       35.0         73.0  ...     0.5205   0.4318   0.4720
ArgM_REC       47.0000        5.0         52.0  ...     0.9038   0.7344   0.8103
ArgM_SCP       17.0000       55.0         72.0  ...     0.2361   0.3696   0.2881
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      144.0000       83.0        227.0  ...     0.6344   0.6606   0.6472
sum          6689.0000     2046.0       8735.0  ...    11.9616  10.9681  11.3013
precision       0.7658        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7458        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7556        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7291        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
../../preprocess/base/mk_dataset.py:55: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
epoch 25 	 loss 30.10038202470855
Validation Start

           correct_num  wrong_num  predict_num  ...  precision   recall       f1
Arg             0.0000        0.0          0.0  ...        NaN      NaN      NaN
Arg0         1418.0000      246.0       1664.0  ...     0.8522   0.8341   0.8430
Arg1         3234.0000      613.0       3847.0  ...     0.8407   0.8019   0.8208
Arg2         1119.0000      393.0       1512.0  ...     0.7401   0.6920   0.7152
Arg3           35.0000       55.0         90.0  ...     0.3889   0.3398   0.3627
Arg4           10.0000       13.0         23.0  ...     0.4348   0.2326   0.3030
Arg5            8.0000        1.0          9.0  ...     0.8889   0.4706   0.6154
ArgA            0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM            0.0000        2.0          2.0  ...     0.0000   0.0000      NaN
ArgM2           0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_ADV      201.0000      108.0        309.0  ...     0.6505   0.4729   0.5477
ArgM_AND        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN   0.0000      NaN
ArgM_CAU       33.0000       29.0         62.0  ...     0.5323   0.4714   0.5000
ArgM_CMP        3.0000       13.0         16.0  ...     0.1875   0.1364   0.1579
ArgM_CND        1.0000        0.0          1.0  ...     1.0000   0.2000   0.3333
ArgM_CRT        1.0000        4.0          5.0  ...     0.2000   0.1250   0.1538
ArgM_DIR        9.0000       10.0         19.0  ...     0.4737   0.3214   0.3830
ArgM_EXT       20.0000       27.0         47.0  ...     0.4255   0.4255   0.4255
ArgM_GOL        0.0000        3.0          3.0  ...     0.0000   0.0000      NaN
ArgM_LOC      101.0000       57.0        158.0  ...     0.6392   0.6196   0.6293
ArgM_MDF        0.0000        1.0          1.0  ...     0.0000   0.0000      NaN
ArgM_MNR       71.0000       63.0        134.0  ...     0.5299   0.6121   0.5680
ArgM_MNS        7.0000       21.0         28.0  ...     0.2500   0.2333   0.2414
ArgM_NEG       68.0000       14.0         82.0  ...     0.8293   0.8608   0.8447
ArgM_PRP       16.0000        8.0         24.0  ...     0.6667   0.5517   0.6038
ArgM_PRX       36.0000       44.0         80.0  ...     0.4500   0.4091   0.4286
ArgM_REC       47.0000        4.0         51.0  ...     0.9216   0.7344   0.8174
ArgM_SCP       16.0000       44.0         60.0  ...     0.2667   0.3478   0.3019
ArgM_SPK        0.0000        0.0          0.0  ...        NaN      NaN      NaN
ArgM_TMP      139.0000       76.0        215.0  ...     0.6465   0.6376   0.6420
sum          6593.0000     1849.0       8442.0  ...    12.8147  10.5301  11.2385
precision       0.7810        NaN          NaN  ...        NaN      NaN      NaN
recall          0.7351        NaN          NaN  ...        NaN      NaN      NaN
f1              0.7573        NaN          NaN  ...        NaN      NaN      NaN
f1_macro        0.7251        NaN          NaN  ...        NaN      NaN      NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
