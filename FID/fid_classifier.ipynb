{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} \n",
      "\n",
      "MAX_TOKEN = 265, MAX_LENGTH = 252, MAX_ARGUMENT_SEQUENCE_LENGTH = 30\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/callum/.local/share/virtualenvs/Dev_srl-mFS1mltI/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "pd.set_option('display.max_rows', 990)\n",
    "pd.set_option('display.max_columns', 990)\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "sys.path.append('../../')\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import itertools\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from mk_train_test_for_fid import get_train_test\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from pcgrad import PCGrad\n",
    "\n",
    "#DATAPATH = r'C:\\Users\\callu\\Desktop\\Univ\\SRL_v2\\Data\\data_v5_under53_fid.json'\n",
    "DATAPATH = '/home/callum/Desktop/SRL_ALL/Data/data_v2_myCandidate.json'\n",
    "FORM = 'bert'\n",
    "# read data\n",
    "#with open(DATAPATH, 'r', encoding=\"utf-8_sig\") as json_file:\n",
    "#    data = pd.read_json(json_file)\n",
    "json_file = open(f'{DATAPATH}', 'r',encoding='utf-8')\n",
    "data = pd.read_json(json_file, orient='records', lines=True)\n",
    "\n",
    "# 正解ラベル（カテゴリー）をデータセットから取得\n",
    "labels = []\n",
    "for args in data['args']:\n",
    "    labels += [ arg['argrole'] for arg in args]\n",
    "labels = set(labels)\n",
    "labels = sorted(list(labels)) + ['F-A', 'F-P', 'V', 'O', 'N']\n",
    "\n",
    "# frameID の数は決まっている．v5_fid(0~2029)  v2_fid(1~1097)\n",
    "FID_LAYER = max(data['predicate'].map(lambda x:x['frameID']))\n",
    "#FID_LAYER = 1097\n",
    "fid2id=dict(zip(np.arange(1,FID_LAYER+1), np.arange(FID_LAYER)))\n",
    "fid2id[-1] = -1\n",
    "\n",
    "# カテゴリーのID辞書を作成，出力層の数定義\n",
    "id2lab = dict(zip(list(range(len(labels))), labels))\n",
    "lab2id = dict(zip(labels, list(range(len(labels)))))\n",
    "print(lab2id, '\\n')\n",
    "\n",
    "# 各種定義\n",
    "OUTPUT_LAYER = len(labels)                              # 全ラベルの数\n",
    "PRED_SEP_CRITERION = 10 + 2                              # 述語情報のためのトークン数．sep:2, pred:8(最長)\n",
    "MAX_LENGTH = 252\n",
    "MAX_ARGUMENT_SEQUENCE_LENGTH = 30                       # 項の最高トークン数．（これより大きいものは予測不可能）\n",
    "MAX_TOKEN = MAX_LENGTH + PRED_SEP_CRITERION + 1         # BERT に入力するトークン数．+1 は cls 分のトークン． \n",
    "print(f'MAX_TOKEN = {MAX_TOKEN}, MAX_LENGTH = {MAX_LENGTH}, MAX_ARGUMENT_SEQUENCE_LENGTH = {MAX_ARGUMENT_SEQUENCE_LENGTH}\\n\\n')\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "def bert_tokenizer_unidic(wakati, pred): \n",
    "    #wakati = normalize(wakati)\n",
    "    token_num = len(wakati.split(' '))\n",
    "    pred_token_num = len(pred['surface'].split(' '))\n",
    "    pred_sep_num =  pred_token_num + 2 if pred_token_num <= PRED_SEP_CRITERION-2 else PRED_SEP_CRITERION\n",
    "\n",
    "    if token_num <= MAX_LENGTH:\n",
    "        front_padding_num = MAX_LENGTH - token_num\n",
    "        back_padding_num = MAX_TOKEN - MAX_LENGTH - pred_sep_num - 1 # -1 は cls\n",
    "        tokens = ['[CLS]'] + wakati.split(' ') + ['[PAD]']*front_padding_num + ['[SEP]'] + pred['surface'].split(' ')[:pred_token_num] + ['[SEP]'] + ['[PAD]']*back_padding_num\n",
    "    else:\n",
    "        padding_num = MAX_TOKEN - MAX_LENGTH - pred_sep_num - 1 # padding >= 0 は保証\n",
    "        tokens = ['[CLS]'] + wakati.split(' ')[:MAX_LENGTH] + ['[SEP]'] + pred['surface'].split(' ')[:pred_token_num] + ['[SEP]'] + ['[PAD]']*padding_num\n",
    "    ids = torch.tensor( [tokenizer.encode(token)[1] for token in tokens], dtype=torch.long)\n",
    "    #print(tokenizer.convert_ids_to_tokens(ids))\n",
    "    #print(len(ids))\n",
    "    return ids\n",
    "\n",
    "def bert_tokenizer_bert(wakati, pred): \n",
    "    #wakati = normalize(wakati)\n",
    "    token_num = len(wakati.split(' '))\n",
    "    pred_token_num = len(pred['surface'].split(' '))\n",
    "    pred_sep_num =  pred_token_num + 2 if pred_token_num <= PRED_SEP_CRITERION-2 else PRED_SEP_CRITERION\n",
    "\n",
    "    if token_num <= MAX_LENGTH:\n",
    "        front_padding_num = MAX_LENGTH - token_num\n",
    "        back_padding_num = MAX_TOKEN - MAX_LENGTH - pred_sep_num - 1 # -1 は cls\n",
    "        tokens = ['[CLS]'] + wakati.split(' ') + ['[PAD]']*front_padding_num + ['[SEP]'] + pred['surface'].split(' ')[:pred_token_num] + ['[SEP]'] + ['[PAD]']*back_padding_num\n",
    "    else:\n",
    "        padding_num = MAX_TOKEN - MAX_LENGTH - pred_sep_num - 1 # padding >= 0 は保証\n",
    "        tokens = ['[CLS]'] + wakati.split(' ')[:MAX_LENGTH] + ['[SEP]'] + pred['surface'].split(' ')[:pred_token_num] + ['[SEP]'] + ['[PAD]']*padding_num\n",
    "    ids = np.array(tokenizer.convert_tokens_to_ids(tokens))\n",
    "    #print(tokenizer.convert_ids_to_tokens(ids))\n",
    "    #print(len(ids))\n",
    "    return ids\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # BERT\n",
    "        self.bert = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-v2', output_attentions=True, output_hidden_states=True) # attention 受け取り，隠れ層取得\n",
    "        self.linear_fid = nn.Linear(768*2+FID_LAYER, 768+FID_LAYER)    # input, output\n",
    "        self.linear_fid2 = nn.Linear(768+FID_LAYER, FID_LAYER)    # input, output\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.linear_fid.weight, std=0.02)   # std 正規分布の標準偏差\n",
    "        nn.init.normal_(self.linear_fid.bias, 0)\n",
    "\n",
    "    # ベクトルを取得する用の関数\n",
    "    def _get_cls_vec(self, vec):\n",
    "        vecs = vec[:,0,:].view(-1, 768)     # cls ベクトル\n",
    "        return vecs # vecs[batch][768]\n",
    "    \n",
    "    # ベクトルを取得する用の関数\n",
    "    def _get_last_vecs(self, vec, pred_spans):\n",
    "        vecs=[]\n",
    "        for i, span in enumerate(pred_spans):\n",
    "            span_len = span[1]-span[0]+1\n",
    "            pred_start = MAX_LENGTH+2\n",
    "            pred_end = pred_start+span_len\n",
    "            vecs.append(vec[i, pred_start:pred_end, :].mean(0).reshape(1,768) )   # ex: vec[token_num][768]  mean-> vec[1][768] (0) は列に対して\n",
    "        vecs = torch.cat([vec for vec in vecs], axis=0)        \n",
    "        return vecs # vecs[batch][768]\n",
    "    \n",
    "    # ベクトルを取得する用の関数\n",
    "    def _get_pred_vec(self, vec, pred_spans):\n",
    "        vecs=[]\n",
    "        for i, span in enumerate(pred_spans):\n",
    "            vecs.append(vec[i, span[0]+1:span[1]+2, :].mean(0).reshape(1,768) )   # ex: vec[token_num][768]  mean-> vec[1][768] (0) は列に対して\n",
    "        vecs = torch.cat([vec for vec in vecs], axis=0)\n",
    "        return vecs # vecs[batch][768]\n",
    "\n",
    "    #@profile\n",
    "    def forward(self, input_ids, pred_spans, fid_vecs):\n",
    "        # 順伝播の出力結果は辞書形式なので、必要な値のkeyを指定して取得する\n",
    "        output = self.bert(input_ids)\n",
    "        hidden_states = output['hidden_states']\n",
    "\n",
    "        # 隠れ層からそれぞれ トークンのベクトルを取得する\n",
    "        cls_vecs = self._get_cls_vec(hidden_states[-1])  # vecs[batch][768]\n",
    "        #pred_vec = self._get_last_vecs(hidden_states[-1], pred_spans)  # vecs[batch][768]\n",
    "        pred_vec = self._get_pred_vec(hidden_states[-1], pred_spans)  # vecs[batch][768]\n",
    "\n",
    "        # fid\n",
    "        #input_vecs = torch.cat([cls_vecs, pred_vec], axis= 1)\n",
    "        input_vecs = torch.cat([cls_vecs, pred_vec, fid_vecs], axis= 1)\n",
    "        outs_fid = self.linear_fid(input_vecs) #[batch][cls+last+fid]\n",
    "        outs_fid = self.relu(outs_fid) #[batch][cls+last+fid]\n",
    "        outs_fid = self.linear_fid2(outs_fid) #[batch][cls+last+fid]\n",
    "        results_fid = F.log_softmax(outs_fid, dim=1) #[batch][fid]\n",
    "        \n",
    "        return results_fid\n",
    "\n",
    "\n",
    "classifier = BertClassifier()\n",
    "\n",
    "\n",
    "# まずは全部OFF\n",
    "for param in classifier.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# BERTの最終4層分をON\n",
    "for param in classifier.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in classifier.bert.encoder.layer[-2].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in classifier.bert.encoder.layer[-3].parameters():\n",
    "    param.requires_grad = True\n",
    "for param in classifier.bert.encoder.layer[-4].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 追加した層のところもON\n",
    "for param in classifier.linear_fid.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 事前学習済の箇所は学習率小さめ、最後の全結合層は大きめにする。\n",
    "ENC_NUM = 1\n",
    "optimizer = optim.Adam([\n",
    "    {'params': classifier.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': classifier.bert.encoder.layer[-2].parameters(), 'lr': 5e-5},\n",
    "    {'params': classifier.bert.encoder.layer[-3].parameters(), 'lr': 5e-5},\n",
    "    {'params': classifier.bert.encoder.layer[-4].parameters(), 'lr': 5e-5},\n",
    "    {'params': classifier.linear_fid.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "# GPUの設定\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ネットワークをGPUへ送る\n",
    "classifier.to(device)\n",
    "prev_acc = -1\n",
    "patience_counter = 0\n",
    "\n",
    "def whether_to_stop(epoch, valid_dataset):\n",
    "    print('Validation Start\\n')\n",
    "    torch.save(classifier.state_dict(), f\"models/fid_{MAX_LENGTH}_enc{ENC_NUM}_eachEP.pth\")\n",
    "    fid_preds, fid_ans = [],[]\n",
    "    all_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (batch_features, batch_preds, batch_token, batch_cand_vecs, batch_fids) in enumerate(valid_dataset):   # labelsはバッチ16こに対し128トークンのラベル\n",
    "            # 各特徴，ラベルをデバイスへ送る\n",
    "            input_ids = batch_features.to(device)         # input token ids\n",
    "            cand_vecs = batch_cand_vecs.to(device)        # candidate fid vecs\n",
    "            pred_span = batch_preds.to(device)            # predicate span\n",
    "            fids = batch_fids.to(device)                   # correct fids\n",
    "            \n",
    "            outs = classifier(input_ids, pred_span, cand_vecs)\n",
    "            # fid loss and span loss\n",
    "            loss_fid = loss_function(outs, fids)\n",
    "            all_loss += loss_fid.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            classifier.zero_grad()  # 累積されるので，ここで初期化しなくてはならない．\n",
    "\n",
    "            _, pred = torch.max(outs, 1)\n",
    "            fid_preds += pred.detach().clone().cpu()\n",
    "            fid_ans += fids.detach().clone().cpu()\n",
    "        acc = accuracy_score(fid_ans, fid_preds)\n",
    "        print(acc)\n",
    "\n",
    "    global prev_acc\n",
    "    global patience_counter\n",
    "\n",
    "    if prev_acc < acc:\n",
    "        patience_counter = 0\n",
    "        prev_acc = acc\n",
    "        print('Valid acc = ',acc,'\\n')\n",
    "        torch.save(classifier.state_dict(), f\"models/fid_{MAX_LENGTH}_enc{ENC_NUM}_best.pth\")\n",
    "        return False\n",
    "\n",
    "    elif (prev_acc >= acc) and (patience_counter < 3):   # 10回連続でaccが下がらなければ終了\n",
    "        print('No change in valid acc\\n') \n",
    "        patience_counter += 1     \n",
    "        return False\n",
    "    else: \n",
    "        print('Stop. No change in valid acc\\n') \n",
    "        return True\n",
    "\n",
    "def mk_fid_vecs(fid_candidates_lists):\n",
    "    fid_vecs=[]\n",
    "    for fid_candidates in fid_candidates_lists:\n",
    "        fid_vec = np.zeros(FID_LAYER)\n",
    "        for fid in fid_candidates:\n",
    "            if fid == -1:\n",
    "                break\n",
    "            else:\n",
    "                fid_vec[fid] = 1\n",
    "        fid_vecs.append(fid_vec)\n",
    "    return fid_vecs\n",
    "\n",
    "\n",
    "def preprocess(sentence_s, predicates_s, token_num, fid_candidates_lists, frameID, form):\n",
    "    sentences = []\n",
    "    pred_span = []\n",
    "    for sent, pred in zip(sentence_s, predicates_s):\n",
    "        if form == 'unidic':\n",
    "            sentences.append(bert_tokenizer_unidic(sent, pred))\n",
    "        if form == 'bert':\n",
    "            sentences.append(bert_tokenizer_bert(sent, pred))\n",
    "        pred_span.append((pred['word_start'],pred['word_end']))\n",
    "    # 各要素の\n",
    "    sentences = torch.tensor(sentences, dtype=torch.long)\n",
    "    pred_span = torch.tensor(pred_span, dtype=torch.long)\n",
    "    token_num = torch.tensor(list(token_num), dtype=torch.long)\n",
    "    fid_candidate_vecs = torch.tensor(mk_fid_vecs(fid_candidates_lists), dtype=torch.long)\n",
    "    frameID = torch.tensor(list(frameID), dtype=torch.long)\n",
    "    return [sentences, pred_span, token_num, fid_candidate_vecs, frameID]\n",
    "\n",
    "def mk_dataset(df, BATCH_SIZE, form):\n",
    "    df.sort_values(by='num_of_tokens',inplace = True, ascending=True)\n",
    "    batch_set = [df.iloc[i*BATCH_SIZE : (i+1)*BATCH_SIZE] for i in range(int(len(df)/BATCH_SIZE))]\n",
    "    batch_set = [preprocess(set['sentence'], set['predicate'], set['num_of_tokens'], set['fid_candidates'], set['frameID'], form) for set in batch_set]\n",
    "    return batch_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_253012/2445027653.py:267: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  sentences = torch.tensor(sentences, dtype=torch.long)\n",
      "/tmp/ipykernel_253012/2445027653.py:270: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  fid_candidate_vecs = torch.tensor(mk_fid_vecs(fid_candidates_lists), dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 0 \t loss 8570.107573509216\n",
      "0.46834595493165865\n",
      "Validation Start\n",
      "\n",
      "0.5589866863905325\n",
      "Valid acc =  0.5589866863905325 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 1 \t loss 4082.0084929373115\n",
      "0.5852650535648319\n",
      "Validation Start\n",
      "\n",
      "0.6407174556213018\n",
      "Valid acc =  0.6407174556213018 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 2 \t loss 1835.4730935115367\n",
      "0.6767639453269302\n",
      "Validation Start\n",
      "\n",
      "0.6849112426035503\n",
      "Valid acc =  0.6849112426035503 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 3 \t loss 708.4489133954048\n",
      "0.7447935906908016\n",
      "Validation Start\n",
      "\n",
      "0.696560650887574\n",
      "Valid acc =  0.696560650887574 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 4 \t loss 352.6602293709293\n",
      "0.7906169190986332\n",
      "Validation Start\n",
      "\n",
      "0.6950813609467456\n",
      "No change in valid acc\n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 5 \t loss 267.16355886508245\n",
      "0.8221393609161434\n",
      "Validation Start\n",
      "\n",
      "0.7050665680473372\n",
      "Valid acc =  0.7050665680473372 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 6 \t loss 227.13571933167987\n",
      "0.844856588738192\n",
      "Validation Start\n",
      "\n",
      "0.7078402366863905\n",
      "Valid acc =  0.7078402366863905 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 7 \t loss 208.2888223071932\n",
      "0.8620994181751016\n",
      "Validation Start\n",
      "\n",
      "0.7041420118343196\n",
      "No change in valid acc\n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 8 \t loss 198.0225762105474\n",
      "0.8756541682058859\n",
      "Validation Start\n",
      "\n",
      "0.7120931952662722\n",
      "Valid acc =  0.7120931952662722 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 9 \t loss 166.65079871173657\n",
      "0.8867357776135943\n",
      "Validation Start\n",
      "\n",
      "0.7126479289940828\n",
      "Valid acc =  0.7126479289940828 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 10 \t loss 162.72369708219048\n",
      "0.895894902105652\n",
      "Validation Start\n",
      "\n",
      "0.709689349112426\n",
      "No change in valid acc\n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n",
      "Progress 16000 / 43312\n",
      "Progress 17600 / 43312\n",
      "Progress 19200 / 43312\n",
      "Progress 20800 / 43312\n",
      "Progress 22400 / 43312\n",
      "Progress 24000 / 43312\n",
      "Progress 25600 / 43312\n",
      "Progress 27200 / 43312\n",
      "Progress 28800 / 43312\n",
      "Progress 30400 / 43312\n",
      "Progress 32000 / 43312\n",
      "Progress 33600 / 43312\n",
      "Progress 35200 / 43312\n",
      "Progress 36800 / 43312\n",
      "Progress 38400 / 43312\n",
      "Progress 40000 / 43312\n",
      "Progress 41600 / 43312\n",
      "Progress 43200 / 43312\n",
      "epoch 11 \t loss 162.1812617809992\n",
      "0.9034370767146903\n",
      "Validation Start\n",
      "\n",
      "0.7156065088757396\n",
      "Valid acc =  0.7156065088757396 \n",
      "\n",
      "Progress 0 / 43312\n",
      "Progress 1600 / 43312\n",
      "Progress 3200 / 43312\n",
      "Progress 4800 / 43312\n",
      "Progress 6400 / 43312\n",
      "Progress 8000 / 43312\n",
      "Progress 9600 / 43312\n",
      "Progress 11200 / 43312\n",
      "Progress 12800 / 43312\n",
      "Progress 14400 / 43312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #classifier.load_state_dict(torch.load('../../models/span_all_support_lesspadding.pth'))\n",
    "\n",
    "    \"\"\"\n",
    "    Train\n",
    "    \"\"\"\n",
    "    data = data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "    train_df, test_df, valid_df = get_train_test(MAX_LENGTH, MAX_ARGUMENT_SEQUENCE_LENGTH, data, lab2id, fid2id)\n",
    "    # dataset, dateloader\n",
    "    train_dataset = mk_dataset(train_df, 16, FORM)\n",
    "    test_dataset = mk_dataset(test_df, 16, FORM)\n",
    "    valid_dataset = mk_dataset(valid_df, 16, FORM)\n",
    "    #whether_to_stop(0, valid_dataset)\n",
    "\n",
    "    time_start = datetime.datetime.now()\n",
    "    # エポック数は5で\n",
    "    fid_preds, fid_ans = [],[]\n",
    "    srl_preds, srl_ans = [],[]\n",
    "    for epoch in range(100):\n",
    "        all_loss = 0\n",
    "        for i, (batch_features, batch_preds, batch_token, batch_cand_vecs, batch_fids) in enumerate(train_dataset):   # labelsはバッチ16こに対し128トークンのラベル\n",
    "            # 各特徴，ラベルをデバイスへ送る\n",
    "            if i%100==0: \n",
    "                print(f'Progress {i*BATCH_SIZE} / {len(train_dataset)*BATCH_SIZE}')\n",
    "            input_ids = batch_features.to(device)         # input token ids\n",
    "            cand_vecs = batch_cand_vecs.to(device)        # candidate fid vecs\n",
    "            pred_span = batch_preds.to(device)            # predicate span\n",
    "            fids = batch_fids.to(device)                  # correct fids\n",
    "            \n",
    "            outs = classifier(input_ids, pred_span, cand_vecs)\n",
    "            # fid loss and span loss\n",
    "            loss_fid = loss_function(outs, fids)\n",
    "            loss_fid.backward() # calculate the gradient can apply gradient modification                \n",
    "            all_loss += loss_fid.item()\n",
    "\n",
    "            optimizer.step()\n",
    "            classifier.zero_grad()  # 累積されるので，ここで初期化しなくてはならない．\n",
    "\n",
    "            _, pred = torch.max(outs, 1)\n",
    "            fid_preds += pred.detach().clone().cpu()\n",
    "            fid_ans += fids.detach().clone().cpu()\n",
    "        print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss)\n",
    "        print(accuracy_score(fid_ans, fid_preds))\n",
    "        #print(classification_report(fid_ans, fid_preds, labels=list(fid2id.kyes())) )\n",
    "        \"\"\"\n",
    "        Validation\n",
    "        \"\"\"\n",
    "        if whether_to_stop(epoch, valid_dataset):\n",
    "            break\n",
    "    # 時間計測\n",
    "    time_end = datetime.datetime.now()\n",
    "    print('\\n', time_end - time_start, '\\n')\n",
    "\n",
    "    \"\"\"\n",
    "    Test\n",
    "    \"\"\"\n",
    "    classifier.load_state_dict(torch.load(f'models/fid_{MAX_LENGTH}_enc{ENC_NUM}_best.pth'))\n",
    "    fid_preds, fid_ans = [],[]\n",
    "    all_loss = 0\n",
    "    for i, (batch_features, batch_labels, batch_preds, batch_token, batch_cand_vecs, batch_fids) in enumerate(test_dataset):   # labelsはバッチ16こに対し128トークンのラベル\n",
    "        # 使用するスパンの範囲を教える変数（使用可：１， 使用不可：－１）\n",
    "        print(f'Progress {i*BATCH_SIZE} / {len(valid_dataset)*BATCH_SIZE}')\n",
    "        # 各特徴，ラベルをデバイスへ送る\n",
    "        input_ids = batch_features.to(device)         # input token ids\n",
    "        pred_span = batch_preds.to(device)            # predicate span\n",
    "        cand_vecs = batch_cand_vecs.to(device)        # candidate fid vecs\n",
    "        fids = batch_fids.to(device)                   # correct fids\n",
    "        \n",
    "        outs = classifier(input_ids, pred_span, cand_vecs)\n",
    "        # fid loss and span loss \n",
    "        loss_fid = loss_function(outs, fids)\n",
    "        all_loss += loss_fid.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        classifier.zero_grad()  # 累積されるので，ここで初期化しなくてはならない．\n",
    "\n",
    "        _, pred = torch.max(outs, 1)\n",
    "        fid_preds += pred.detach().clone().cpu()\n",
    "        fid_ans += fids.detach().clone().cpu()\n",
    "    acc = accuracy_score(fid_ans, fid_preds)\n",
    "    print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-v2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0 / 5296\n",
      "Progress 16 / 5296\n",
      "Progress 32 / 5296\n",
      "Progress 48 / 5296\n",
      "Progress 64 / 5296\n",
      "Progress 80 / 5296\n",
      "Progress 96 / 5296\n",
      "Progress 112 / 5296\n",
      "Progress 128 / 5296\n",
      "Progress 144 / 5296\n",
      "Progress 160 / 5296\n",
      "Progress 176 / 5296\n",
      "Progress 192 / 5296\n",
      "Progress 208 / 5296\n",
      "Progress 224 / 5296\n",
      "Progress 240 / 5296\n",
      "Progress 256 / 5296\n",
      "Progress 272 / 5296\n",
      "Progress 288 / 5296\n",
      "Progress 304 / 5296\n",
      "Progress 320 / 5296\n",
      "Progress 336 / 5296\n",
      "Progress 352 / 5296\n",
      "Progress 368 / 5296\n",
      "Progress 384 / 5296\n",
      "Progress 400 / 5296\n",
      "Progress 416 / 5296\n",
      "Progress 432 / 5296\n",
      "Progress 448 / 5296\n",
      "Progress 464 / 5296\n",
      "Progress 480 / 5296\n",
      "Progress 496 / 5296\n",
      "Progress 512 / 5296\n",
      "Progress 528 / 5296\n",
      "Progress 544 / 5296\n",
      "Progress 560 / 5296\n",
      "Progress 576 / 5296\n",
      "Progress 592 / 5296\n",
      "Progress 608 / 5296\n",
      "Progress 624 / 5296\n",
      "Progress 640 / 5296\n",
      "Progress 656 / 5296\n",
      "Progress 672 / 5296\n",
      "Progress 688 / 5296\n",
      "Progress 704 / 5296\n",
      "Progress 720 / 5296\n",
      "Progress 736 / 5296\n",
      "Progress 752 / 5296\n",
      "Progress 768 / 5296\n",
      "Progress 784 / 5296\n",
      "Progress 800 / 5296\n",
      "Progress 816 / 5296\n",
      "Progress 832 / 5296\n",
      "Progress 848 / 5296\n",
      "Progress 864 / 5296\n",
      "Progress 880 / 5296\n",
      "Progress 896 / 5296\n",
      "Progress 912 / 5296\n",
      "Progress 928 / 5296\n",
      "Progress 944 / 5296\n",
      "Progress 960 / 5296\n",
      "Progress 976 / 5296\n",
      "Progress 992 / 5296\n",
      "Progress 1008 / 5296\n",
      "Progress 1024 / 5296\n",
      "Progress 1040 / 5296\n",
      "Progress 1056 / 5296\n",
      "Progress 1072 / 5296\n",
      "Progress 1088 / 5296\n",
      "Progress 1104 / 5296\n",
      "Progress 1120 / 5296\n",
      "Progress 1136 / 5296\n",
      "Progress 1152 / 5296\n",
      "Progress 1168 / 5296\n",
      "Progress 1184 / 5296\n",
      "Progress 1200 / 5296\n",
      "Progress 1216 / 5296\n",
      "Progress 1232 / 5296\n",
      "Progress 1248 / 5296\n",
      "Progress 1264 / 5296\n",
      "Progress 1280 / 5296\n",
      "Progress 1296 / 5296\n",
      "Progress 1312 / 5296\n",
      "Progress 1328 / 5296\n",
      "Progress 1344 / 5296\n",
      "Progress 1360 / 5296\n",
      "Progress 1376 / 5296\n",
      "Progress 1392 / 5296\n",
      "Progress 1408 / 5296\n",
      "Progress 1424 / 5296\n",
      "Progress 1440 / 5296\n",
      "Progress 1456 / 5296\n",
      "Progress 1472 / 5296\n",
      "Progress 1488 / 5296\n",
      "Progress 1504 / 5296\n",
      "Progress 1520 / 5296\n",
      "Progress 1536 / 5296\n",
      "Progress 1552 / 5296\n",
      "Progress 1568 / 5296\n",
      "Progress 1584 / 5296\n",
      "Progress 1600 / 5296\n",
      "Progress 1616 / 5296\n",
      "Progress 1632 / 5296\n",
      "Progress 1648 / 5296\n",
      "Progress 1664 / 5296\n",
      "Progress 1680 / 5296\n",
      "Progress 1696 / 5296\n",
      "Progress 1712 / 5296\n",
      "Progress 1728 / 5296\n",
      "Progress 1744 / 5296\n",
      "Progress 1760 / 5296\n",
      "Progress 1776 / 5296\n",
      "Progress 1792 / 5296\n",
      "Progress 1808 / 5296\n",
      "Progress 1824 / 5296\n",
      "Progress 1840 / 5296\n",
      "Progress 1856 / 5296\n",
      "Progress 1872 / 5296\n",
      "Progress 1888 / 5296\n",
      "Progress 1904 / 5296\n",
      "Progress 1920 / 5296\n",
      "Progress 1936 / 5296\n",
      "Progress 1952 / 5296\n",
      "Progress 1968 / 5296\n",
      "Progress 1984 / 5296\n",
      "Progress 2000 / 5296\n",
      "Progress 2016 / 5296\n",
      "Progress 2032 / 5296\n",
      "Progress 2048 / 5296\n",
      "Progress 2064 / 5296\n",
      "Progress 2080 / 5296\n",
      "Progress 2096 / 5296\n",
      "Progress 2112 / 5296\n",
      "Progress 2128 / 5296\n",
      "Progress 2144 / 5296\n",
      "Progress 2160 / 5296\n",
      "Progress 2176 / 5296\n",
      "Progress 2192 / 5296\n",
      "Progress 2208 / 5296\n",
      "Progress 2224 / 5296\n",
      "Progress 2240 / 5296\n",
      "Progress 2256 / 5296\n",
      "Progress 2272 / 5296\n",
      "Progress 2288 / 5296\n",
      "Progress 2304 / 5296\n",
      "Progress 2320 / 5296\n",
      "Progress 2336 / 5296\n",
      "Progress 2352 / 5296\n",
      "Progress 2368 / 5296\n",
      "Progress 2384 / 5296\n",
      "Progress 2400 / 5296\n",
      "Progress 2416 / 5296\n",
      "Progress 2432 / 5296\n",
      "Progress 2448 / 5296\n",
      "Progress 2464 / 5296\n",
      "Progress 2480 / 5296\n",
      "Progress 2496 / 5296\n",
      "Progress 2512 / 5296\n",
      "Progress 2528 / 5296\n",
      "Progress 2544 / 5296\n",
      "Progress 2560 / 5296\n",
      "Progress 2576 / 5296\n",
      "Progress 2592 / 5296\n",
      "Progress 2608 / 5296\n",
      "Progress 2624 / 5296\n",
      "Progress 2640 / 5296\n",
      "Progress 2656 / 5296\n",
      "Progress 2672 / 5296\n",
      "Progress 2688 / 5296\n",
      "Progress 2704 / 5296\n",
      "Progress 2720 / 5296\n",
      "Progress 2736 / 5296\n",
      "Progress 2752 / 5296\n",
      "Progress 2768 / 5296\n",
      "Progress 2784 / 5296\n",
      "Progress 2800 / 5296\n",
      "Progress 2816 / 5296\n",
      "Progress 2832 / 5296\n",
      "Progress 2848 / 5296\n",
      "Progress 2864 / 5296\n",
      "Progress 2880 / 5296\n",
      "Progress 2896 / 5296\n",
      "Progress 2912 / 5296\n",
      "Progress 2928 / 5296\n",
      "Progress 2944 / 5296\n",
      "Progress 2960 / 5296\n",
      "Progress 2976 / 5296\n",
      "Progress 2992 / 5296\n",
      "Progress 3008 / 5296\n",
      "Progress 3024 / 5296\n",
      "Progress 3040 / 5296\n",
      "Progress 3056 / 5296\n",
      "Progress 3072 / 5296\n",
      "Progress 3088 / 5296\n",
      "Progress 3104 / 5296\n",
      "Progress 3120 / 5296\n",
      "Progress 3136 / 5296\n",
      "Progress 3152 / 5296\n",
      "Progress 3168 / 5296\n",
      "Progress 3184 / 5296\n",
      "Progress 3200 / 5296\n",
      "Progress 3216 / 5296\n",
      "Progress 3232 / 5296\n",
      "Progress 3248 / 5296\n",
      "Progress 3264 / 5296\n",
      "Progress 3280 / 5296\n",
      "Progress 3296 / 5296\n",
      "Progress 3312 / 5296\n",
      "Progress 3328 / 5296\n",
      "Progress 3344 / 5296\n",
      "Progress 3360 / 5296\n",
      "Progress 3376 / 5296\n",
      "Progress 3392 / 5296\n",
      "Progress 3408 / 5296\n",
      "Progress 3424 / 5296\n",
      "Progress 3440 / 5296\n",
      "Progress 3456 / 5296\n",
      "Progress 3472 / 5296\n",
      "Progress 3488 / 5296\n",
      "Progress 3504 / 5296\n",
      "Progress 3520 / 5296\n",
      "Progress 3536 / 5296\n",
      "Progress 3552 / 5296\n",
      "Progress 3568 / 5296\n",
      "Progress 3584 / 5296\n",
      "Progress 3600 / 5296\n",
      "Progress 3616 / 5296\n",
      "Progress 3632 / 5296\n",
      "Progress 3648 / 5296\n",
      "Progress 3664 / 5296\n",
      "Progress 3680 / 5296\n",
      "Progress 3696 / 5296\n",
      "Progress 3712 / 5296\n",
      "Progress 3728 / 5296\n",
      "Progress 3744 / 5296\n",
      "Progress 3760 / 5296\n",
      "Progress 3776 / 5296\n",
      "Progress 3792 / 5296\n",
      "Progress 3808 / 5296\n",
      "Progress 3824 / 5296\n",
      "Progress 3840 / 5296\n",
      "Progress 3856 / 5296\n",
      "Progress 3872 / 5296\n",
      "Progress 3888 / 5296\n",
      "Progress 3904 / 5296\n",
      "Progress 3920 / 5296\n",
      "Progress 3936 / 5296\n",
      "Progress 3952 / 5296\n",
      "Progress 3968 / 5296\n",
      "Progress 3984 / 5296\n",
      "Progress 4000 / 5296\n",
      "Progress 4016 / 5296\n",
      "Progress 4032 / 5296\n",
      "Progress 4048 / 5296\n",
      "Progress 4064 / 5296\n",
      "Progress 4080 / 5296\n",
      "Progress 4096 / 5296\n",
      "Progress 4112 / 5296\n",
      "Progress 4128 / 5296\n",
      "Progress 4144 / 5296\n",
      "Progress 4160 / 5296\n",
      "Progress 4176 / 5296\n",
      "Progress 4192 / 5296\n",
      "Progress 4208 / 5296\n",
      "Progress 4224 / 5296\n",
      "Progress 4240 / 5296\n",
      "Progress 4256 / 5296\n",
      "Progress 4272 / 5296\n",
      "Progress 4288 / 5296\n",
      "Progress 4304 / 5296\n",
      "Progress 4320 / 5296\n",
      "Progress 4336 / 5296\n",
      "Progress 4352 / 5296\n",
      "Progress 4368 / 5296\n",
      "Progress 4384 / 5296\n",
      "Progress 4400 / 5296\n",
      "Progress 4416 / 5296\n",
      "Progress 4432 / 5296\n",
      "Progress 4448 / 5296\n",
      "Progress 4464 / 5296\n",
      "Progress 4480 / 5296\n",
      "Progress 4496 / 5296\n",
      "Progress 4512 / 5296\n",
      "Progress 4528 / 5296\n",
      "Progress 4544 / 5296\n",
      "Progress 4560 / 5296\n",
      "Progress 4576 / 5296\n",
      "Progress 4592 / 5296\n",
      "Progress 4608 / 5296\n",
      "Progress 4624 / 5296\n",
      "Progress 4640 / 5296\n",
      "Progress 4656 / 5296\n",
      "Progress 4672 / 5296\n",
      "Progress 4688 / 5296\n",
      "Progress 4704 / 5296\n",
      "Progress 4720 / 5296\n",
      "Progress 4736 / 5296\n",
      "Progress 4752 / 5296\n",
      "Progress 4768 / 5296\n",
      "Progress 4784 / 5296\n",
      "Progress 4800 / 5296\n",
      "Progress 4816 / 5296\n",
      "Progress 4832 / 5296\n",
      "Progress 4848 / 5296\n",
      "Progress 4864 / 5296\n",
      "Progress 4880 / 5296\n",
      "Progress 4896 / 5296\n",
      "Progress 4912 / 5296\n",
      "Progress 4928 / 5296\n",
      "Progress 4944 / 5296\n",
      "Progress 4960 / 5296\n",
      "Progress 4976 / 5296\n",
      "Progress 4992 / 5296\n",
      "Progress 5008 / 5296\n",
      "Progress 5024 / 5296\n",
      "Progress 5040 / 5296\n",
      "Progress 5056 / 5296\n",
      "Progress 5072 / 5296\n",
      "Progress 5088 / 5296\n",
      "Progress 5104 / 5296\n",
      "Progress 5120 / 5296\n",
      "Progress 5136 / 5296\n",
      "Progress 5152 / 5296\n",
      "Progress 5168 / 5296\n",
      "Progress 5184 / 5296\n",
      "Progress 5200 / 5296\n",
      "Progress 5216 / 5296\n",
      "Progress 5232 / 5296\n",
      "Progress 5248 / 5296\n",
      "Progress 5264 / 5296\n",
      "Progress 5280 / 5296\n",
      "0.9378776435045317\n"
     ]
    }
   ],
   "source": [
    "# 各種データ作成（学習，テスト，検証）\n",
    "data = data.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "train_df, test_df, valid_df = get_train_test(MAX_LENGTH, MAX_ARGUMENT_SEQUENCE_LENGTH, data, lab2id, fid2id)\n",
    "test_dataset = mk_dataset(test_df, 16, FORM)\n",
    "valid_dataset = mk_dataset(valid_df, 16, FORM)\n",
    "\n",
    "del classifier\n",
    "classifier = BertClassifier()\n",
    "classifier.to(device)\n",
    "classifier.load_state_dict(torch.load(f'../../models/fid_{MAX_LENGTH}_enc{ENC_NUM}_best.pth'))\n",
    "fid_preds, fid_ans = [],[]\n",
    "all_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (batch_features, batch_labels, batch_preds, batch_token, batch_cand_vecs, batch_fids) in enumerate(valid_dataset):   # labelsはバッチ16こに対し128トークンのラベル\n",
    "        # 使用するスパンの範囲を教える変数（使用可：１， 使用不可：－１）\n",
    "        print(f'Progress {i*BATCH_SIZE} / {len(valid_dataset)*BATCH_SIZE}')\n",
    "        # 各特徴，ラベルをデバイスへ送る\n",
    "        input_ids = batch_features.to(device)         # input token ids\n",
    "        pred_span = batch_preds.to(device)            # predicate span\n",
    "        cand_vecs = batch_cand_vecs.to(device)        # candidate fid vecs\n",
    "        fids = batch_fids.to(device)                   # correct fids\n",
    "        \n",
    "        outs = classifier(input_ids, pred_span, cand_vecs)\n",
    "        # fid loss and span loss \n",
    "        loss_fid = loss_function(outs, fids)\n",
    "        all_loss += loss_fid.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        classifier.zero_grad()  # 累積されるので，ここで初期化しなくてはならない．\n",
    "\n",
    "        _, pred = torch.max(outs, 1)\n",
    "        fid_preds += pred.detach().clone().cpu()\n",
    "        fid_ans += fids.detach().clone().cpu()\n",
    "    acc = accuracy_score(fid_ans, fid_preds)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress 0 / 5296\n",
      "0.875\n",
      "Progress 16 / 5296\n",
      "0.9375\n",
      "Progress 32 / 5296\n",
      "0.9166666666666666\n",
      "Progress 48 / 5296\n",
      "0.921875\n",
      "Progress 64 / 5296\n",
      "0.9\n",
      "Progress 80 / 5296\n",
      "0.8958333333333334\n",
      "Progress 96 / 5296\n",
      "0.8928571428571429\n",
      "Progress 112 / 5296\n",
      "0.8828125\n",
      "Progress 128 / 5296\n",
      "0.8958333333333334\n",
      "Progress 144 / 5296\n",
      "0.89375\n",
      "Progress 160 / 5296\n",
      "0.8863636363636364\n",
      "Progress 176 / 5296\n",
      "0.890625\n",
      "Progress 192 / 5296\n",
      "0.8942307692307693\n",
      "Progress 208 / 5296\n",
      "0.8973214285714286\n",
      "Progress 224 / 5296\n",
      "0.9\n",
      "Progress 240 / 5296\n",
      "0.90234375\n",
      "Progress 256 / 5296\n",
      "0.9044117647058824\n",
      "Progress 272 / 5296\n",
      "0.90625\n",
      "Progress 288 / 5296\n",
      "0.9013157894736842\n",
      "Progress 304 / 5296\n",
      "0.9\n",
      "Progress 320 / 5296\n",
      "0.9047619047619048\n",
      "Progress 336 / 5296\n",
      "0.90625\n",
      "Progress 352 / 5296\n",
      "0.9103260869565217\n",
      "Progress 368 / 5296\n",
      "0.9036458333333334\n",
      "Progress 384 / 5296\n",
      "0.9025\n",
      "Progress 400 / 5296\n",
      "0.9038461538461539\n",
      "Progress 416 / 5296\n",
      "0.9004629629629629\n",
      "Progress 432 / 5296\n",
      "0.9017857142857143\n",
      "Progress 448 / 5296\n",
      "0.9030172413793104\n",
      "Progress 464 / 5296\n",
      "0.90625\n",
      "Progress 480 / 5296\n",
      "0.9092741935483871\n",
      "Progress 496 / 5296\n",
      "0.91015625\n",
      "Progress 512 / 5296\n",
      "0.9090909090909091\n",
      "Progress 528 / 5296\n",
      "0.9099264705882353\n",
      "Progress 544 / 5296\n",
      "0.9107142857142857\n",
      "Progress 560 / 5296\n",
      "0.9114583333333334\n",
      "Progress 576 / 5296\n",
      "0.9121621621621622\n",
      "Progress 592 / 5296\n",
      "0.9095394736842105\n",
      "Progress 608 / 5296\n",
      "0.9086538461538461\n",
      "Progress 624 / 5296\n",
      "0.90625\n",
      "Progress 640 / 5296\n",
      "0.9070121951219512\n",
      "Progress 656 / 5296\n",
      "0.9077380952380952\n",
      "Progress 672 / 5296\n",
      "0.9055232558139535\n",
      "Progress 688 / 5296\n",
      "0.90625\n",
      "Progress 704 / 5296\n",
      "0.9069444444444444\n",
      "Progress 720 / 5296\n",
      "0.907608695652174\n",
      "Progress 736 / 5296\n",
      "0.9095744680851063\n",
      "Progress 752 / 5296\n",
      "0.9088541666666666\n",
      "Progress 768 / 5296\n",
      "0.9094387755102041\n",
      "Progress 784 / 5296\n",
      "0.91\n",
      "Progress 800 / 5296\n",
      "0.9117647058823529\n",
      "Progress 816 / 5296\n",
      "0.9098557692307693\n",
      "Progress 832 / 5296\n",
      "0.9091981132075472\n",
      "Progress 848 / 5296\n",
      "0.9097222222222222\n",
      "Progress 864 / 5296\n",
      "0.9102272727272728\n",
      "Progress 880 / 5296\n",
      "0.9095982142857143\n",
      "Progress 896 / 5296\n",
      "0.9100877192982456\n",
      "Progress 912 / 5296\n",
      "0.9094827586206896\n",
      "Progress 928 / 5296\n",
      "0.909957627118644\n",
      "Progress 944 / 5296\n",
      "0.9104166666666667\n",
      "Progress 960 / 5296\n",
      "0.9088114754098361\n",
      "Progress 976 / 5296\n",
      "0.9082661290322581\n",
      "Progress 992 / 5296\n",
      "0.9077380952380952\n",
      "Progress 1008 / 5296\n",
      "0.908203125\n",
      "Progress 1024 / 5296\n",
      "0.9096153846153846\n",
      "Progress 1040 / 5296\n",
      "0.9100378787878788\n",
      "Progress 1056 / 5296\n",
      "0.9113805970149254\n",
      "Progress 1072 / 5296\n",
      "0.9126838235294118\n",
      "Progress 1088 / 5296\n",
      "0.9139492753623188\n",
      "Progress 1104 / 5296\n",
      "0.9151785714285714\n",
      "Progress 1120 / 5296\n",
      "0.9154929577464789\n",
      "Progress 1136 / 5296\n",
      "0.9131944444444444\n",
      "Progress 1152 / 5296\n",
      "0.9143835616438356\n",
      "Progress 1168 / 5296\n",
      "0.9146959459459459\n",
      "Progress 1184 / 5296\n",
      "0.915\n",
      "Progress 1200 / 5296\n",
      "0.9161184210526315\n",
      "Progress 1216 / 5296\n",
      "0.9172077922077922\n",
      "Progress 1232 / 5296\n",
      "0.9182692307692307\n",
      "Progress 1248 / 5296\n",
      "0.9185126582278481\n",
      "Progress 1264 / 5296\n",
      "0.91953125\n",
      "Progress 1280 / 5296\n",
      "0.9197530864197531\n",
      "Progress 1296 / 5296\n",
      "0.9199695121951219\n",
      "Progress 1312 / 5296\n",
      "0.9201807228915663\n",
      "Progress 1328 / 5296\n",
      "0.9203869047619048\n",
      "Progress 1344 / 5296\n",
      "0.9213235294117647\n",
      "Progress 1360 / 5296\n",
      "0.9215116279069767\n",
      "Progress 1376 / 5296\n",
      "0.9209770114942529\n",
      "Progress 1392 / 5296\n",
      "0.921875\n",
      "Progress 1408 / 5296\n",
      "0.9213483146067416\n",
      "Progress 1424 / 5296\n",
      "0.9222222222222223\n",
      "Progress 1440 / 5296\n",
      "0.9223901098901099\n",
      "Progress 1456 / 5296\n",
      "0.923233695652174\n",
      "Progress 1472 / 5296\n",
      "0.9233870967741935\n",
      "Progress 1488 / 5296\n",
      "0.9235372340425532\n",
      "Progress 1504 / 5296\n",
      "0.9230263157894737\n",
      "Progress 1520 / 5296\n",
      "0.9231770833333334\n",
      "Progress 1536 / 5296\n",
      "0.9233247422680413\n",
      "Progress 1552 / 5296\n",
      "0.9228316326530612\n",
      "Progress 1568 / 5296\n",
      "0.922979797979798\n",
      "Progress 1584 / 5296\n",
      "0.9225\n",
      "Progress 1600 / 5296\n",
      "0.9220297029702971\n",
      "Progress 1616 / 5296\n",
      "0.9221813725490197\n",
      "Progress 1632 / 5296\n",
      "0.9211165048543689\n",
      "Progress 1648 / 5296\n",
      "0.921875\n",
      "Progress 1664 / 5296\n",
      "0.9226190476190477\n",
      "Progress 1680 / 5296\n",
      "0.9227594339622641\n",
      "Progress 1696 / 5296\n",
      "0.9228971962616822\n",
      "Progress 1712 / 5296\n",
      "0.9230324074074074\n",
      "Progress 1728 / 5296\n",
      "0.9220183486238532\n",
      "Progress 1744 / 5296\n",
      "0.9215909090909091\n",
      "Progress 1760 / 5296\n",
      "0.9217342342342343\n",
      "Progress 1776 / 5296\n",
      "0.921875\n",
      "Progress 1792 / 5296\n",
      "0.922566371681416\n",
      "Progress 1808 / 5296\n",
      "0.9226973684210527\n",
      "Progress 1824 / 5296\n",
      "0.9233695652173913\n",
      "Progress 1840 / 5296\n",
      "0.9234913793103449\n",
      "Progress 1856 / 5296\n",
      "0.9241452991452992\n",
      "Progress 1872 / 5296\n",
      "0.9242584745762712\n",
      "Progress 1888 / 5296\n",
      "0.9248949579831933\n",
      "Progress 1904 / 5296\n",
      "0.9255208333333333\n",
      "Progress 1920 / 5296\n",
      "0.9256198347107438\n",
      "Progress 1936 / 5296\n",
      "0.9252049180327869\n",
      "Progress 1952 / 5296\n",
      "0.9253048780487805\n",
      "Progress 1968 / 5296\n",
      "0.9259072580645161\n",
      "Progress 1984 / 5296\n",
      "0.926\n",
      "Progress 2000 / 5296\n",
      "0.9260912698412699\n",
      "Progress 2016 / 5296\n",
      "0.9261811023622047\n",
      "Progress 2032 / 5296\n",
      "0.9267578125\n",
      "Progress 2048 / 5296\n",
      "0.9268410852713178\n",
      "Progress 2064 / 5296\n",
      "0.9259615384615385\n",
      "Progress 2080 / 5296\n",
      "0.9265267175572519\n",
      "Progress 2096 / 5296\n",
      "0.9266098484848485\n",
      "Progress 2112 / 5296\n",
      "0.9271616541353384\n",
      "Progress 2128 / 5296\n",
      "0.9277052238805971\n",
      "Progress 2144 / 5296\n",
      "0.9282407407407407\n",
      "Progress 2160 / 5296\n",
      "0.9287683823529411\n",
      "Progress 2176 / 5296\n",
      "0.9288321167883211\n",
      "Progress 2192 / 5296\n",
      "0.9288949275362319\n",
      "Progress 2208 / 5296\n",
      "0.9294064748201439\n",
      "Progress 2224 / 5296\n",
      "0.9294642857142857\n",
      "Progress 2240 / 5296\n",
      "0.9299645390070922\n",
      "Progress 2256 / 5296\n",
      "0.9304577464788732\n",
      "Progress 2272 / 5296\n",
      "0.9300699300699301\n",
      "Progress 2288 / 5296\n",
      "0.9301215277777778\n",
      "Progress 2304 / 5296\n",
      "0.9301724137931034\n",
      "Progress 2320 / 5296\n",
      "0.9297945205479452\n",
      "Progress 2336 / 5296\n",
      "0.9302721088435374\n",
      "Progress 2352 / 5296\n",
      "0.9307432432432432\n",
      "Progress 2368 / 5296\n",
      "0.9295302013422819\n",
      "Progress 2384 / 5296\n",
      "0.93\n",
      "Progress 2400 / 5296\n",
      "0.9304635761589404\n",
      "Progress 2416 / 5296\n",
      "0.9305098684210527\n",
      "Progress 2432 / 5296\n",
      "0.9305555555555556\n",
      "Progress 2448 / 5296\n",
      "0.9310064935064936\n",
      "Progress 2464 / 5296\n",
      "0.9314516129032258\n",
      "Progress 2480 / 5296\n",
      "0.9310897435897436\n",
      "Progress 2496 / 5296\n",
      "0.9311305732484076\n",
      "Progress 2512 / 5296\n",
      "0.9315664556962026\n",
      "Progress 2528 / 5296\n",
      "0.9319968553459119\n",
      "Progress 2544 / 5296\n",
      "0.931640625\n",
      "Progress 2560 / 5296\n",
      "0.9320652173913043\n",
      "Progress 2576 / 5296\n",
      "0.9324845679012346\n",
      "Progress 2592 / 5296\n",
      "0.932898773006135\n",
      "Progress 2608 / 5296\n",
      "0.9333079268292683\n",
      "Progress 2624 / 5296\n",
      "0.9333333333333333\n",
      "Progress 2640 / 5296\n",
      "0.9337349397590361\n",
      "Progress 2656 / 5296\n",
      "0.9341317365269461\n",
      "Progress 2672 / 5296\n",
      "0.9341517857142857\n",
      "Progress 2688 / 5296\n",
      "0.9341715976331361\n",
      "Progress 2704 / 5296\n",
      "0.9345588235294118\n",
      "Progress 2720 / 5296\n",
      "0.9345760233918129\n",
      "Progress 2736 / 5296\n",
      "0.9349563953488372\n",
      "Progress 2752 / 5296\n",
      "0.934971098265896\n",
      "Progress 2768 / 5296\n",
      "0.9353448275862069\n",
      "Progress 2784 / 5296\n",
      "0.9353571428571429\n",
      "Progress 2800 / 5296\n",
      "0.9353693181818182\n",
      "Progress 2816 / 5296\n",
      "0.9357344632768362\n",
      "Progress 2832 / 5296\n",
      "0.9350421348314607\n",
      "Progress 2848 / 5296\n",
      "0.9350558659217877\n",
      "Progress 2864 / 5296\n",
      "0.9350694444444444\n",
      "Progress 2880 / 5296\n",
      "0.9350828729281768\n",
      "Progress 2896 / 5296\n",
      "0.9354395604395604\n",
      "Progress 2912 / 5296\n",
      "0.9357923497267759\n",
      "Progress 2928 / 5296\n",
      "0.936141304347826\n",
      "Progress 2944 / 5296\n",
      "0.9364864864864865\n",
      "Progress 2960 / 5296\n",
      "0.9358198924731183\n",
      "Progress 2976 / 5296\n",
      "0.9361631016042781\n",
      "Progress 2992 / 5296\n",
      "0.9365026595744681\n",
      "Progress 3008 / 5296\n",
      "0.9368386243386243\n",
      "Progress 3024 / 5296\n",
      "0.937171052631579\n",
      "Progress 3040 / 5296\n",
      "0.9368455497382199\n",
      "Progress 3056 / 5296\n",
      "0.9371744791666666\n",
      "Progress 3072 / 5296\n",
      "0.9368523316062176\n",
      "Progress 3088 / 5296\n",
      "0.9371778350515464\n",
      "Progress 3104 / 5296\n",
      "0.9371794871794872\n",
      "Progress 3120 / 5296\n",
      "0.9368622448979592\n",
      "Progress 3136 / 5296\n",
      "0.9371827411167513\n",
      "Progress 3152 / 5296\n",
      "0.9371843434343434\n",
      "Progress 3168 / 5296\n",
      "0.9375\n",
      "Progress 3184 / 5296\n",
      "0.9378125\n",
      "Progress 3200 / 5296\n",
      "0.9378109452736318\n",
      "Progress 3216 / 5296\n",
      "0.9375\n",
      "Progress 3232 / 5296\n",
      "0.937807881773399\n",
      "Progress 3248 / 5296\n",
      "0.9378063725490197\n",
      "Progress 3264 / 5296\n",
      "0.938109756097561\n",
      "Progress 3280 / 5296\n",
      "0.9381067961165048\n",
      "Progress 3296 / 5296\n",
      "0.9384057971014492\n",
      "Progress 3312 / 5296\n",
      "0.9387019230769231\n",
      "Progress 3328 / 5296\n",
      "0.9389952153110048\n",
      "Progress 3344 / 5296\n",
      "0.9392857142857143\n",
      "Progress 3360 / 5296\n",
      "0.9395734597156398\n",
      "Progress 3376 / 5296\n",
      "0.9395636792452831\n",
      "Progress 3392 / 5296\n",
      "0.9392605633802817\n",
      "Progress 3408 / 5296\n",
      "0.9389602803738317\n",
      "Progress 3424 / 5296\n",
      "0.9392441860465116\n",
      "Progress 3440 / 5296\n",
      "0.9395254629629629\n",
      "Progress 3456 / 5296\n",
      "0.9398041474654378\n",
      "Progress 3472 / 5296\n",
      "0.9392201834862385\n",
      "Progress 3488 / 5296\n",
      "0.9392123287671232\n",
      "Progress 3504 / 5296\n",
      "0.9394886363636363\n",
      "Progress 3520 / 5296\n",
      "0.9391968325791855\n",
      "Progress 3536 / 5296\n",
      "0.9394707207207207\n",
      "Progress 3552 / 5296\n",
      "0.9394618834080718\n",
      "Progress 3568 / 5296\n",
      "0.939453125\n",
      "Progress 3584 / 5296\n",
      "0.9394444444444444\n",
      "Progress 3600 / 5296\n",
      "0.9397123893805309\n",
      "Progress 3616 / 5296\n",
      "0.9397026431718062\n",
      "Progress 3632 / 5296\n",
      "0.9396929824561403\n",
      "Progress 3648 / 5296\n",
      "0.9399563318777293\n",
      "Progress 3664 / 5296\n",
      "0.939945652173913\n",
      "Progress 3680 / 5296\n",
      "0.9396645021645021\n",
      "Progress 3696 / 5296\n",
      "0.9399245689655172\n",
      "Progress 3712 / 5296\n",
      "0.9399141630901288\n",
      "Progress 3728 / 5296\n",
      "0.9401709401709402\n",
      "Progress 3744 / 5296\n",
      "0.9404255319148936\n",
      "Progress 3760 / 5296\n",
      "0.9398834745762712\n",
      "Progress 3776 / 5296\n",
      "0.9393459915611815\n",
      "Progress 3792 / 5296\n",
      "0.9393382352941176\n",
      "Progress 3808 / 5296\n",
      "0.939592050209205\n",
      "Progress 3824 / 5296\n",
      "0.9390625\n",
      "Progress 3840 / 5296\n",
      "0.9393153526970954\n",
      "Progress 3856 / 5296\n",
      "0.9393078512396694\n",
      "Progress 3872 / 5296\n",
      "0.9387860082304527\n",
      "Progress 3888 / 5296\n",
      "0.938780737704918\n",
      "Progress 3904 / 5296\n",
      "0.9390306122448979\n",
      "Progress 3920 / 5296\n",
      "0.9392784552845529\n",
      "Progress 3936 / 5296\n",
      "0.9395242914979757\n",
      "Progress 3952 / 5296\n",
      "0.9397681451612904\n",
      "Progress 3968 / 5296\n",
      "0.9400100401606426\n",
      "Progress 3984 / 5296\n",
      "0.94025\n",
      "Progress 4000 / 5296\n",
      "0.9404880478087649\n",
      "Progress 4016 / 5296\n",
      "0.9407242063492064\n",
      "Progress 4032 / 5296\n",
      "0.9407114624505929\n",
      "Progress 4048 / 5296\n",
      "0.9409448818897638\n",
      "Progress 4064 / 5296\n",
      "0.9406862745098039\n",
      "Progress 4080 / 5296\n",
      "0.940673828125\n",
      "Progress 4096 / 5296\n",
      "0.9404182879377432\n",
      "Progress 4112 / 5296\n",
      "0.9401647286821705\n",
      "Progress 4128 / 5296\n",
      "0.940395752895753\n",
      "Progress 4144 / 5296\n",
      "0.9401442307692308\n",
      "Progress 4160 / 5296\n",
      "0.9401340996168582\n",
      "Progress 4176 / 5296\n",
      "0.9398854961832062\n",
      "Progress 4192 / 5296\n",
      "0.9398764258555133\n",
      "Progress 4208 / 5296\n",
      "0.9401041666666666\n",
      "Progress 4224 / 5296\n",
      "0.9403301886792453\n",
      "Progress 4240 / 5296\n",
      "0.9405545112781954\n",
      "Progress 4256 / 5296\n",
      "0.9407771535580525\n",
      "Progress 4272 / 5296\n",
      "0.9409981343283582\n",
      "Progress 4288 / 5296\n",
      "0.9409851301115242\n",
      "Progress 4304 / 5296\n",
      "0.9409722222222222\n",
      "Progress 4320 / 5296\n",
      "0.941190036900369\n",
      "Progress 4336 / 5296\n",
      "0.94140625\n",
      "Progress 4352 / 5296\n",
      "0.9416208791208791\n",
      "Progress 4368 / 5296\n",
      "0.9416058394160584\n",
      "Progress 4384 / 5296\n",
      "0.9418181818181818\n",
      "Progress 4400 / 5296\n",
      "0.9420289855072463\n",
      "Progress 4416 / 5296\n",
      "0.9422382671480144\n",
      "Progress 4432 / 5296\n",
      "0.9424460431654677\n",
      "Progress 4448 / 5296\n",
      "0.9424283154121864\n",
      "Progress 4464 / 5296\n",
      "0.9424107142857143\n",
      "Progress 4480 / 5296\n",
      "0.9423932384341637\n",
      "Progress 4496 / 5296\n",
      "0.9425975177304965\n",
      "Progress 4512 / 5296\n",
      "0.9423586572438163\n",
      "Progress 4528 / 5296\n",
      "0.9423415492957746\n",
      "Progress 4544 / 5296\n",
      "0.9423245614035087\n",
      "Progress 4560 / 5296\n",
      "0.9423076923076923\n",
      "Progress 4576 / 5296\n",
      "0.9425087108013938\n",
      "Progress 4592 / 5296\n",
      "0.9424913194444444\n",
      "Progress 4608 / 5296\n",
      "0.9424740484429066\n",
      "Progress 4624 / 5296\n",
      "0.9422413793103448\n",
      "Progress 4640 / 5296\n",
      "0.9420103092783505\n",
      "Progress 4656 / 5296\n",
      "0.9419948630136986\n",
      "Progress 4672 / 5296\n",
      "0.9419795221843004\n",
      "Progress 4688 / 5296\n",
      "0.9421768707482994\n",
      "Progress 4704 / 5296\n",
      "0.9419491525423729\n",
      "Progress 4720 / 5296\n",
      "0.9421452702702703\n",
      "Progress 4736 / 5296\n",
      "0.9421296296296297\n",
      "Progress 4752 / 5296\n",
      "0.9421140939597316\n",
      "Progress 4768 / 5296\n",
      "0.9423076923076923\n",
      "Progress 4784 / 5296\n",
      "0.9425\n",
      "Progress 4800 / 5296\n",
      "0.9426910299003323\n",
      "Progress 4816 / 5296\n",
      "0.9426738410596026\n",
      "Progress 4832 / 5296\n",
      "0.942450495049505\n",
      "Progress 4848 / 5296\n",
      "0.942639802631579\n",
      "Progress 4864 / 5296\n",
      "0.9426229508196722\n",
      "Progress 4880 / 5296\n",
      "0.9426062091503268\n",
      "Progress 4896 / 5296\n",
      "0.9427931596091205\n",
      "Progress 4912 / 5296\n",
      "0.9429788961038961\n",
      "Progress 4928 / 5296\n",
      "0.9429611650485437\n",
      "Progress 4944 / 5296\n",
      "0.9431451612903226\n",
      "Progress 4960 / 5296\n",
      "0.9431270096463023\n",
      "Progress 4976 / 5296\n",
      "0.9433092948717948\n",
      "Progress 4992 / 5296\n",
      "0.9432907348242812\n",
      "Progress 5008 / 5296\n",
      "0.9434713375796179\n",
      "Progress 5024 / 5296\n",
      "0.9434523809523809\n",
      "Progress 5040 / 5296\n",
      "0.943631329113924\n",
      "Progress 5056 / 5296\n",
      "0.9436119873817035\n",
      "Progress 5072 / 5296\n",
      "0.9431996855345912\n",
      "Progress 5088 / 5296\n",
      "0.9431818181818182\n",
      "Progress 5104 / 5296\n",
      "0.94296875\n",
      "Progress 5120 / 5296\n",
      "0.9427570093457944\n",
      "Progress 5136 / 5296\n",
      "0.9427406832298136\n",
      "Progress 5152 / 5296\n",
      "0.9425309597523219\n",
      "Progress 5168 / 5296\n",
      "0.9425154320987654\n",
      "Progress 5184 / 5296\n",
      "0.9426923076923077\n",
      "Progress 5200 / 5296\n",
      "0.9426763803680982\n",
      "Progress 5216 / 5296\n",
      "0.9426605504587156\n",
      "Progress 5232 / 5296\n",
      "0.9426448170731707\n",
      "Progress 5248 / 5296\n",
      "0.9428191489361702\n",
      "Progress 5264 / 5296\n",
      "0.9428030303030303\n",
      "Progress 5280 / 5296\n",
      "0.9427870090634441\n"
     ]
    }
   ],
   "source": [
    "fid_preds, fid_ans = [],[]\n",
    "all_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, (batch_features, batch_labels, batch_preds, batch_token, batch_cand_vecs, batch_fids) in enumerate(test_dataset):   # labelsはバッチ16こに対し128トークンのラベル\n",
    "        # 使用するスパンの範囲を教える変数（使用可：１， 使用不可：－１）\n",
    "        print(f'Progress {i*BATCH_SIZE} / {len(valid_dataset)*BATCH_SIZE}')\n",
    "        # 各特徴，ラベルをデバイスへ送る\n",
    "        input_ids = batch_features.to(device)         # input token ids\n",
    "        pred_span = batch_preds.to(device)            # predicate span\n",
    "        cand_vecs = batch_cand_vecs.to(device)        # candidate fid vecs\n",
    "        fids = batch_fids.to(device)                   # correct fids\n",
    "        \n",
    "        outs = classifier(input_ids, pred_span, cand_vecs)\n",
    "        # fid loss and span loss \n",
    "        loss_fid = loss_function(outs, fids)\n",
    "        all_loss += loss_fid.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        classifier.zero_grad()  # 累積されるので，ここで初期化しなくてはならない．\n",
    "\n",
    "        _, pred = torch.max(outs, 1)\n",
    "        fid_preds += pred.detach().clone().cpu()\n",
    "        fid_ans += fids.detach().clone().cpu()\n",
    "        acc = accuracy_score(fid_ans, fid_preds)\n",
    "        print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a01a490ff1ebc15e1fdb6e736ec47e60bab4dd5696e19fea62a1ddff1b6c569f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
