Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 206, MAX_LENGTH = 192, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
840
base_model.model.rinna.embed_in.weight False
base_model.model.rinna.layers.0.input_layernorm.weight False
base_model.model.rinna.layers.0.input_layernorm.bias False
base_model.model.rinna.layers.0.post_attention_layernorm.weight False
base_model.model.rinna.layers.0.post_attention_layernorm.bias False
base_model.model.rinna.layers.0.attention.query_key_value.weight False
base_model.model.rinna.layers.0.attention.query_key_value.bias False
base_model.model.rinna.layers.0.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.0.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.0.attention.dense.weight False
base_model.model.rinna.layers.0.attention.dense.bias False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.1.input_layernorm.weight False
base_model.model.rinna.layers.1.input_layernorm.bias False
base_model.model.rinna.layers.1.post_attention_layernorm.weight False
base_model.model.rinna.layers.1.post_attention_layernorm.bias False
base_model.model.rinna.layers.1.attention.query_key_value.weight False
base_model.model.rinna.layers.1.attention.query_key_value.bias False
base_model.model.rinna.layers.1.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.1.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.1.attention.dense.weight False
base_model.model.rinna.layers.1.attention.dense.bias False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.2.input_layernorm.weight False
base_model.model.rinna.layers.2.input_layernorm.bias False
base_model.model.rinna.layers.2.post_attention_layernorm.weight False
base_model.model.rinna.layers.2.post_attention_layernorm.bias False
base_model.model.rinna.layers.2.attention.query_key_value.weight False
base_model.model.rinna.layers.2.attention.query_key_value.bias False
base_model.model.rinna.layers.2.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.2.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.2.attention.dense.weight False
base_model.model.rinna.layers.2.attention.dense.bias False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.3.input_layernorm.weight False
base_model.model.rinna.layers.3.input_layernorm.bias False
base_model.model.rinna.layers.3.post_attention_layernorm.weight False
base_model.model.rinna.layers.3.post_attention_layernorm.bias False
base_model.model.rinna.layers.3.attention.query_key_value.weight False
base_model.model.rinna.layers.3.attention.query_key_value.bias False
base_model.model.rinna.layers.3.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.3.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.3.attention.dense.weight False
base_model.model.rinna.layers.3.attention.dense.bias False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.4.input_layernorm.weight False
base_model.model.rinna.layers.4.input_layernorm.bias False
base_model.model.rinna.layers.4.post_attention_layernorm.weight False
base_model.model.rinna.layers.4.post_attention_layernorm.bias False
base_model.model.rinna.layers.4.attention.query_key_value.weight False
base_model.model.rinna.layers.4.attention.query_key_value.bias False
base_model.model.rinna.layers.4.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.4.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.4.attention.dense.weight False
base_model.model.rinna.layers.4.attention.dense.bias False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.5.input_layernorm.weight False
base_model.model.rinna.layers.5.input_layernorm.bias False
base_model.model.rinna.layers.5.post_attention_layernorm.weight False
base_model.model.rinna.layers.5.post_attention_layernorm.bias False
base_model.model.rinna.layers.5.attention.query_key_value.weight False
base_model.model.rinna.layers.5.attention.query_key_value.bias False
base_model.model.rinna.layers.5.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.5.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.5.attention.dense.weight False
base_model.model.rinna.layers.5.attention.dense.bias False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.6.input_layernorm.weight False
base_model.model.rinna.layers.6.input_layernorm.bias False
base_model.model.rinna.layers.6.post_attention_layernorm.weight False
base_model.model.rinna.layers.6.post_attention_layernorm.bias False
base_model.model.rinna.layers.6.attention.query_key_value.weight False
base_model.model.rinna.layers.6.attention.query_key_value.bias False
base_model.model.rinna.layers.6.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.6.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.6.attention.dense.weight False
base_model.model.rinna.layers.6.attention.dense.bias False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.7.input_layernorm.weight False
base_model.model.rinna.layers.7.input_layernorm.bias False
base_model.model.rinna.layers.7.post_attention_layernorm.weight False
base_model.model.rinna.layers.7.post_attention_layernorm.bias False
base_model.model.rinna.layers.7.attention.query_key_value.weight False
base_model.model.rinna.layers.7.attention.query_key_value.bias False
base_model.model.rinna.layers.7.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.7.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.7.attention.dense.weight False
base_model.model.rinna.layers.7.attention.dense.bias False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.8.input_layernorm.weight False
base_model.model.rinna.layers.8.input_layernorm.bias False
base_model.model.rinna.layers.8.post_attention_layernorm.weight False
base_model.model.rinna.layers.8.post_attention_layernorm.bias False
base_model.model.rinna.layers.8.attention.query_key_value.weight False
base_model.model.rinna.layers.8.attention.query_key_value.bias False
base_model.model.rinna.layers.8.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.8.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.8.attention.dense.weight False
base_model.model.rinna.layers.8.attention.dense.bias False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.9.input_layernorm.weight False
base_model.model.rinna.layers.9.input_layernorm.bias False
base_model.model.rinna.layers.9.post_attention_layernorm.weight False
base_model.model.rinna.layers.9.post_attention_layernorm.bias False
base_model.model.rinna.layers.9.attention.query_key_value.weight False
base_model.model.rinna.layers.9.attention.query_key_value.bias False
base_model.model.rinna.layers.9.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.9.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.9.attention.dense.weight False
base_model.model.rinna.layers.9.attention.dense.bias False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.10.input_layernorm.weight False
base_model.model.rinna.layers.10.input_layernorm.bias False
base_model.model.rinna.layers.10.post_attention_layernorm.weight False
base_model.model.rinna.layers.10.post_attention_layernorm.bias False
base_model.model.rinna.layers.10.attention.query_key_value.weight False
base_model.model.rinna.layers.10.attention.query_key_value.bias False
base_model.model.rinna.layers.10.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.10.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.10.attention.dense.weight False
base_model.model.rinna.layers.10.attention.dense.bias False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.11.input_layernorm.weight False
base_model.model.rinna.layers.11.input_layernorm.bias False
base_model.model.rinna.layers.11.post_attention_layernorm.weight False
base_model.model.rinna.layers.11.post_attention_layernorm.bias False
base_model.model.rinna.layers.11.attention.query_key_value.weight False
base_model.model.rinna.layers.11.attention.query_key_value.bias False
base_model.model.rinna.layers.11.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.11.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.11.attention.dense.weight False
base_model.model.rinna.layers.11.attention.dense.bias False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.12.input_layernorm.weight False
base_model.model.rinna.layers.12.input_layernorm.bias False
base_model.model.rinna.layers.12.post_attention_layernorm.weight False
base_model.model.rinna.layers.12.post_attention_layernorm.bias False
base_model.model.rinna.layers.12.attention.query_key_value.weight False
base_model.model.rinna.layers.12.attention.query_key_value.bias False
base_model.model.rinna.layers.12.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.12.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.12.attention.dense.weight False
base_model.model.rinna.layers.12.attention.dense.bias False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.13.input_layernorm.weight False
base_model.model.rinna.layers.13.input_layernorm.bias False
base_model.model.rinna.layers.13.post_attention_layernorm.weight False
base_model.model.rinna.layers.13.post_attention_layernorm.bias False
base_model.model.rinna.layers.13.attention.query_key_value.weight False
base_model.model.rinna.layers.13.attention.query_key_value.bias False
base_model.model.rinna.layers.13.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.13.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.13.attention.dense.weight False
base_model.model.rinna.layers.13.attention.dense.bias False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.14.input_layernorm.weight False
base_model.model.rinna.layers.14.input_layernorm.bias False
base_model.model.rinna.layers.14.post_attention_layernorm.weight False
base_model.model.rinna.layers.14.post_attention_layernorm.bias False
base_model.model.rinna.layers.14.attention.query_key_value.weight False
base_model.model.rinna.layers.14.attention.query_key_value.bias False
base_model.model.rinna.layers.14.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.14.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.14.attention.dense.weight False
base_model.model.rinna.layers.14.attention.dense.bias False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.15.input_layernorm.weight False
base_model.model.rinna.layers.15.input_layernorm.bias False
base_model.model.rinna.layers.15.post_attention_layernorm.weight False
base_model.model.rinna.layers.15.post_attention_layernorm.bias False
base_model.model.rinna.layers.15.attention.query_key_value.weight False
base_model.model.rinna.layers.15.attention.query_key_value.bias False
base_model.model.rinna.layers.15.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.15.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.15.attention.dense.weight False
base_model.model.rinna.layers.15.attention.dense.bias False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.16.input_layernorm.weight False
base_model.model.rinna.layers.16.input_layernorm.bias False
base_model.model.rinna.layers.16.post_attention_layernorm.weight False
base_model.model.rinna.layers.16.post_attention_layernorm.bias False
base_model.model.rinna.layers.16.attention.query_key_value.weight False
base_model.model.rinna.layers.16.attention.query_key_value.bias False
base_model.model.rinna.layers.16.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.16.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.16.attention.dense.weight False
base_model.model.rinna.layers.16.attention.dense.bias False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.17.input_layernorm.weight False
base_model.model.rinna.layers.17.input_layernorm.bias False
base_model.model.rinna.layers.17.post_attention_layernorm.weight False
base_model.model.rinna.layers.17.post_attention_layernorm.bias False
base_model.model.rinna.layers.17.attention.query_key_value.weight False
base_model.model.rinna.layers.17.attention.query_key_value.bias False
base_model.model.rinna.layers.17.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.17.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.17.attention.dense.weight False
base_model.model.rinna.layers.17.attention.dense.bias False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.18.input_layernorm.weight False
base_model.model.rinna.layers.18.input_layernorm.bias False
base_model.model.rinna.layers.18.post_attention_layernorm.weight False
base_model.model.rinna.layers.18.post_attention_layernorm.bias False
base_model.model.rinna.layers.18.attention.query_key_value.weight False
base_model.model.rinna.layers.18.attention.query_key_value.bias False
base_model.model.rinna.layers.18.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.18.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.18.attention.dense.weight False
base_model.model.rinna.layers.18.attention.dense.bias False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.19.input_layernorm.weight False
base_model.model.rinna.layers.19.input_layernorm.bias False
base_model.model.rinna.layers.19.post_attention_layernorm.weight False
base_model.model.rinna.layers.19.post_attention_layernorm.bias False
base_model.model.rinna.layers.19.attention.query_key_value.weight False
base_model.model.rinna.layers.19.attention.query_key_value.bias False
base_model.model.rinna.layers.19.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.19.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.19.attention.dense.weight False
base_model.model.rinna.layers.19.attention.dense.bias False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.20.input_layernorm.weight False
base_model.model.rinna.layers.20.input_layernorm.bias False
base_model.model.rinna.layers.20.post_attention_layernorm.weight False
base_model.model.rinna.layers.20.post_attention_layernorm.bias False
base_model.model.rinna.layers.20.attention.query_key_value.weight False
base_model.model.rinna.layers.20.attention.query_key_value.bias False
base_model.model.rinna.layers.20.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.20.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.20.attention.dense.weight False
base_model.model.rinna.layers.20.attention.dense.bias False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.21.input_layernorm.weight False
base_model.model.rinna.layers.21.input_layernorm.bias False
base_model.model.rinna.layers.21.post_attention_layernorm.weight False
base_model.model.rinna.layers.21.post_attention_layernorm.bias False
base_model.model.rinna.layers.21.attention.query_key_value.weight False
base_model.model.rinna.layers.21.attention.query_key_value.bias False
base_model.model.rinna.layers.21.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.21.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.21.attention.dense.weight False
base_model.model.rinna.layers.21.attention.dense.bias False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.22.input_layernorm.weight False
base_model.model.rinna.layers.22.input_layernorm.bias False
base_model.model.rinna.layers.22.post_attention_layernorm.weight False
base_model.model.rinna.layers.22.post_attention_layernorm.bias False
base_model.model.rinna.layers.22.attention.query_key_value.weight False
base_model.model.rinna.layers.22.attention.query_key_value.bias False
base_model.model.rinna.layers.22.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.22.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.22.attention.dense.weight False
base_model.model.rinna.layers.22.attention.dense.bias False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.23.input_layernorm.weight False
base_model.model.rinna.layers.23.input_layernorm.bias False
base_model.model.rinna.layers.23.post_attention_layernorm.weight False
base_model.model.rinna.layers.23.post_attention_layernorm.bias False
base_model.model.rinna.layers.23.attention.query_key_value.weight False
base_model.model.rinna.layers.23.attention.query_key_value.bias False
base_model.model.rinna.layers.23.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.23.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.23.attention.dense.weight False
base_model.model.rinna.layers.23.attention.dense.bias False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.24.input_layernorm.weight False
base_model.model.rinna.layers.24.input_layernorm.bias False
base_model.model.rinna.layers.24.post_attention_layernorm.weight False
base_model.model.rinna.layers.24.post_attention_layernorm.bias False
base_model.model.rinna.layers.24.attention.query_key_value.weight False
base_model.model.rinna.layers.24.attention.query_key_value.bias False
base_model.model.rinna.layers.24.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.24.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.24.attention.dense.weight False
base_model.model.rinna.layers.24.attention.dense.bias False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.25.input_layernorm.weight False
base_model.model.rinna.layers.25.input_layernorm.bias False
base_model.model.rinna.layers.25.post_attention_layernorm.weight False
base_model.model.rinna.layers.25.post_attention_layernorm.bias False
base_model.model.rinna.layers.25.attention.query_key_value.weight False
base_model.model.rinna.layers.25.attention.query_key_value.bias False
base_model.model.rinna.layers.25.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.25.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.25.attention.dense.weight False
base_model.model.rinna.layers.25.attention.dense.bias False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.26.input_layernorm.weight False
base_model.model.rinna.layers.26.input_layernorm.bias False
base_model.model.rinna.layers.26.post_attention_layernorm.weight False
base_model.model.rinna.layers.26.post_attention_layernorm.bias False
base_model.model.rinna.layers.26.attention.query_key_value.weight False
base_model.model.rinna.layers.26.attention.query_key_value.bias False
base_model.model.rinna.layers.26.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.26.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.26.attention.dense.weight False
base_model.model.rinna.layers.26.attention.dense.bias False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.27.input_layernorm.weight False
base_model.model.rinna.layers.27.input_layernorm.bias False
base_model.model.rinna.layers.27.post_attention_layernorm.weight False
base_model.model.rinna.layers.27.post_attention_layernorm.bias False
base_model.model.rinna.layers.27.attention.query_key_value.weight False
base_model.model.rinna.layers.27.attention.query_key_value.bias False
base_model.model.rinna.layers.27.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.27.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.27.attention.dense.weight False
base_model.model.rinna.layers.27.attention.dense.bias False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.28.input_layernorm.weight False
base_model.model.rinna.layers.28.input_layernorm.bias False
base_model.model.rinna.layers.28.post_attention_layernorm.weight False
base_model.model.rinna.layers.28.post_attention_layernorm.bias False
base_model.model.rinna.layers.28.attention.query_key_value.weight False
base_model.model.rinna.layers.28.attention.query_key_value.bias False
base_model.model.rinna.layers.28.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.28.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.28.attention.dense.weight False
base_model.model.rinna.layers.28.attention.dense.bias False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.29.input_layernorm.weight False
base_model.model.rinna.layers.29.input_layernorm.bias False
base_model.model.rinna.layers.29.post_attention_layernorm.weight False
base_model.model.rinna.layers.29.post_attention_layernorm.bias False
base_model.model.rinna.layers.29.attention.query_key_value.weight False
base_model.model.rinna.layers.29.attention.query_key_value.bias False
base_model.model.rinna.layers.29.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.29.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.29.attention.dense.weight False
base_model.model.rinna.layers.29.attention.dense.bias False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.30.input_layernorm.weight False
base_model.model.rinna.layers.30.input_layernorm.bias False
base_model.model.rinna.layers.30.post_attention_layernorm.weight False
base_model.model.rinna.layers.30.post_attention_layernorm.bias False
base_model.model.rinna.layers.30.attention.query_key_value.weight False
base_model.model.rinna.layers.30.attention.query_key_value.bias False
base_model.model.rinna.layers.30.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.30.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.30.attention.dense.weight False
base_model.model.rinna.layers.30.attention.dense.bias False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.31.input_layernorm.weight False
base_model.model.rinna.layers.31.input_layernorm.bias False
base_model.model.rinna.layers.31.post_attention_layernorm.weight False
base_model.model.rinna.layers.31.post_attention_layernorm.bias False
base_model.model.rinna.layers.31.attention.query_key_value.weight False
base_model.model.rinna.layers.31.attention.query_key_value.bias False
base_model.model.rinna.layers.31.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.31.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.31.attention.dense.weight False
base_model.model.rinna.layers.31.attention.dense.bias False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.32.input_layernorm.weight False
base_model.model.rinna.layers.32.input_layernorm.bias False
base_model.model.rinna.layers.32.post_attention_layernorm.weight False
base_model.model.rinna.layers.32.post_attention_layernorm.bias False
base_model.model.rinna.layers.32.attention.query_key_value.weight False
base_model.model.rinna.layers.32.attention.query_key_value.bias False
base_model.model.rinna.layers.32.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.32.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.32.attention.dense.weight False
base_model.model.rinna.layers.32.attention.dense.bias False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.33.input_layernorm.weight False
base_model.model.rinna.layers.33.input_layernorm.bias False
base_model.model.rinna.layers.33.post_attention_layernorm.weight False
base_model.model.rinna.layers.33.post_attention_layernorm.bias False
base_model.model.rinna.layers.33.attention.query_key_value.weight False
base_model.model.rinna.layers.33.attention.query_key_value.bias False
base_model.model.rinna.layers.33.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.33.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.33.attention.dense.weight False
base_model.model.rinna.layers.33.attention.dense.bias False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.34.input_layernorm.weight False
base_model.model.rinna.layers.34.input_layernorm.bias False
base_model.model.rinna.layers.34.post_attention_layernorm.weight False
base_model.model.rinna.layers.34.post_attention_layernorm.bias False
base_model.model.rinna.layers.34.attention.query_key_value.weight False
base_model.model.rinna.layers.34.attention.query_key_value.bias False
base_model.model.rinna.layers.34.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.34.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.34.attention.dense.weight False
base_model.model.rinna.layers.34.attention.dense.bias False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.35.input_layernorm.weight False
base_model.model.rinna.layers.35.input_layernorm.bias False
base_model.model.rinna.layers.35.post_attention_layernorm.weight False
base_model.model.rinna.layers.35.post_attention_layernorm.bias False
base_model.model.rinna.layers.35.attention.query_key_value.weight False
base_model.model.rinna.layers.35.attention.query_key_value.bias False
base_model.model.rinna.layers.35.attention.query_key_value.lora_A.default.weight False
base_model.model.rinna.layers.35.attention.query_key_value.lora_B.default.weight False
base_model.model.rinna.layers.35.attention.dense.weight False
base_model.model.rinna.layers.35.attention.dense.bias False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.bias False
base_model.model.rinna.final_layer_norm.weight False
base_model.model.rinna.final_layer_norm.bias False
base_model.model.my_linear.original_module.weight True
base_model.model.my_linear.original_module.bias True
base_model.model.my_linear.modules_to_save.default.weight True
base_model.model.my_linear.modules_to_save.default.bias True
base_model.model.my_linear2.original_module.weight True
base_model.model.my_linear2.original_module.bias True
base_model.model.my_linear2.modules_to_save.default.weight True
base_model.model.my_linear2.modules_to_save.default.bias True
Progress 0 / 832
epoch 0 	 loss 32.16906160861254
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           17.0000        4.0         21.0  ...     0.8095  0.4250  0.5574
Arg1            8.0000        3.0         11.0  ...     0.7273  0.1026  0.1798
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            25.0000        7.0         32.0  ...     1.5368  0.5276  0.7372
precision       0.7812        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1330        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2273        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0476        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.2273 

Progress 0 / 832
epoch 1 	 loss 14.344885855913162
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        4.0         24.0  ...     0.8333  0.5000  0.6250
Arg1            7.0000        2.0          9.0  ...     0.7778  0.0897  0.1609
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            27.0000        6.0         33.0  ...     1.6111  0.5897  0.7859
precision       0.8182        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1436        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2443        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0507        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.2443 

Progress 0 / 832
epoch 2 	 loss 10.66909322142601
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           26.0000       20.0         46.0  ...     0.5652  0.6500  0.6047
Arg1           10.0000        3.0         13.0  ...     0.7692  0.1282  0.2198
Arg2            1.0000        0.0          1.0  ...     1.0000  0.0294  0.0571
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            37.0000       23.0         60.0  ...     2.3344  0.8076  0.8816
precision       0.6167        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1968        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2984        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0569        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.2984 

Progress 0 / 832
epoch 3 	 loss 7.288830950856209
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           23.0000        6.0         29.0  ...     0.7931  0.5750  0.6667
Arg1           17.0000        6.0         23.0  ...     0.7391  0.2179  0.3366
Arg2            1.0000        3.0          4.0  ...     0.2500  0.0294  0.0526
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            41.0000       15.0         56.0  ...     1.7822  0.8224  1.0559
precision       0.7321        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2181        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3361        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0681        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.3361 

Progress 0 / 832
epoch 4 	 loss 6.206414952874184
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           19.0000        4.0         23.0  ...     0.8261  0.4750  0.6032
Arg1           18.0000       12.0         30.0  ...     0.6000  0.2308  0.3333
Arg2            3.0000        5.0          8.0  ...     0.3750  0.0882  0.1429
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       21.0         61.0  ...     1.8011  0.7940  1.0794
precision       0.6557        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3213        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0696        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 5 	 loss 4.89661261998117
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           23.0000        6.0         29.0  ...     0.7931  0.5750  0.6667
Arg1           20.0000        9.0         29.0  ...     0.6897  0.2564  0.3738
Arg2            1.0000        2.0          3.0  ...     0.3333  0.0294  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            44.0000       17.0         61.0  ...     1.8161  0.8608  1.0946
precision       0.7213        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2340        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3534        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0706        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.3534 

Progress 0 / 832
epoch 6 	 loss 4.953428941313177
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           22.0000        5.0         27.0  ...     0.8148  0.5500  0.6567
Arg1           17.0000        8.0         25.0  ...     0.6800  0.2179  0.3301
Arg2            1.0000        1.0          2.0  ...     0.5000  0.0294  0.0556
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       14.0         54.0  ...     1.9948  0.7974  1.0424
precision       0.7407        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3306        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0672        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 7 	 loss 5.427643119357526
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        4.0         24.0  ...     0.8333  0.5000  0.6250
Arg1           19.0000       13.0         32.0  ...     0.5938  0.2436  0.3455
Arg2            1.0000        0.0          1.0  ...     1.0000  0.0294  0.0571
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       17.0         57.0  ...     2.4271  0.7730  1.0276
precision       0.7018        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3265        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0663        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 8 	 loss 6.296878885943443
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        5.0         25.0  ...     0.8000  0.5000  0.6154
Arg1           13.0000        5.0         18.0  ...     0.7222  0.1667  0.2708
Arg2            1.0000        2.0          3.0  ...     0.3333  0.0294  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            34.0000       12.0         46.0  ...     1.8556  0.6961  0.9403
precision       0.7391        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1809        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2906        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0607        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 9 	 loss 6.504960596561432
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           21.0000        6.0         27.0  ...     0.7778  0.5250  0.6269
Arg1           14.0000        6.0         20.0  ...     0.7000  0.1795  0.2857
Arg2            1.0000        2.0          3.0  ...     0.3333  0.0294  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            36.0000       14.0         50.0  ...     1.8111  0.7339  0.9666
precision       0.7200        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1915        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3025        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0624        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../preprocess/base/mk_dataset.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch 10 	 loss 5.219316974282265
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           21.0000        6.0         27.0  ...     0.7778  0.5250  0.6269
Arg1           18.0000        8.0         26.0  ...     0.6923  0.2308  0.3462
Arg2            1.0000        2.0          3.0  ...     0.3333  0.0294  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       16.0         56.0  ...     1.8034  0.7852  1.0271
precision       0.7143        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3279        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0663        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Stop. No change in valid f1


 0:20:50.619239 

(0.3057,            correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           18.0000        3.0         21.0  ...     0.8571  0.4737  0.6102
Arg1           16.0000        8.0         24.0  ...     0.6667  0.2025  0.3107
Arg2            1.0000        3.0          4.0  ...     0.2500  0.0303  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            35.0000       14.0         49.0  ...     1.7738  0.7065  0.9749
precision       0.7143        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1944        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3057        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0629        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns])
