Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 206, MAX_LENGTH = 192, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
840
base_model.model.rinna.embed_in.weight False
base_model.model.rinna.layers.0.input_layernorm.weight False
base_model.model.rinna.layers.0.input_layernorm.bias False
base_model.model.rinna.layers.0.post_attention_layernorm.weight False
base_model.model.rinna.layers.0.post_attention_layernorm.bias False
base_model.model.rinna.layers.0.attention.query_key_value.weight False
base_model.model.rinna.layers.0.attention.query_key_value.bias False
base_model.model.rinna.layers.0.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.0.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.0.attention.dense.weight False
base_model.model.rinna.layers.0.attention.dense.bias False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.1.input_layernorm.weight False
base_model.model.rinna.layers.1.input_layernorm.bias False
base_model.model.rinna.layers.1.post_attention_layernorm.weight False
base_model.model.rinna.layers.1.post_attention_layernorm.bias False
base_model.model.rinna.layers.1.attention.query_key_value.weight False
base_model.model.rinna.layers.1.attention.query_key_value.bias False
base_model.model.rinna.layers.1.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.1.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.1.attention.dense.weight False
base_model.model.rinna.layers.1.attention.dense.bias False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.2.input_layernorm.weight False
base_model.model.rinna.layers.2.input_layernorm.bias False
base_model.model.rinna.layers.2.post_attention_layernorm.weight False
base_model.model.rinna.layers.2.post_attention_layernorm.bias False
base_model.model.rinna.layers.2.attention.query_key_value.weight False
base_model.model.rinna.layers.2.attention.query_key_value.bias False
base_model.model.rinna.layers.2.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.2.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.2.attention.dense.weight False
base_model.model.rinna.layers.2.attention.dense.bias False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.3.input_layernorm.weight False
base_model.model.rinna.layers.3.input_layernorm.bias False
base_model.model.rinna.layers.3.post_attention_layernorm.weight False
base_model.model.rinna.layers.3.post_attention_layernorm.bias False
base_model.model.rinna.layers.3.attention.query_key_value.weight False
base_model.model.rinna.layers.3.attention.query_key_value.bias False
base_model.model.rinna.layers.3.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.3.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.3.attention.dense.weight False
base_model.model.rinna.layers.3.attention.dense.bias False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.4.input_layernorm.weight False
base_model.model.rinna.layers.4.input_layernorm.bias False
base_model.model.rinna.layers.4.post_attention_layernorm.weight False
base_model.model.rinna.layers.4.post_attention_layernorm.bias False
base_model.model.rinna.layers.4.attention.query_key_value.weight False
base_model.model.rinna.layers.4.attention.query_key_value.bias False
base_model.model.rinna.layers.4.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.4.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.4.attention.dense.weight False
base_model.model.rinna.layers.4.attention.dense.bias False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.5.input_layernorm.weight False
base_model.model.rinna.layers.5.input_layernorm.bias False
base_model.model.rinna.layers.5.post_attention_layernorm.weight False
base_model.model.rinna.layers.5.post_attention_layernorm.bias False
base_model.model.rinna.layers.5.attention.query_key_value.weight False
base_model.model.rinna.layers.5.attention.query_key_value.bias False
base_model.model.rinna.layers.5.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.5.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.5.attention.dense.weight False
base_model.model.rinna.layers.5.attention.dense.bias False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.6.input_layernorm.weight False
base_model.model.rinna.layers.6.input_layernorm.bias False
base_model.model.rinna.layers.6.post_attention_layernorm.weight False
base_model.model.rinna.layers.6.post_attention_layernorm.bias False
base_model.model.rinna.layers.6.attention.query_key_value.weight False
base_model.model.rinna.layers.6.attention.query_key_value.bias False
base_model.model.rinna.layers.6.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.6.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.6.attention.dense.weight False
base_model.model.rinna.layers.6.attention.dense.bias False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.7.input_layernorm.weight False
base_model.model.rinna.layers.7.input_layernorm.bias False
base_model.model.rinna.layers.7.post_attention_layernorm.weight False
base_model.model.rinna.layers.7.post_attention_layernorm.bias False
base_model.model.rinna.layers.7.attention.query_key_value.weight False
base_model.model.rinna.layers.7.attention.query_key_value.bias False
base_model.model.rinna.layers.7.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.7.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.7.attention.dense.weight False
base_model.model.rinna.layers.7.attention.dense.bias False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.8.input_layernorm.weight False
base_model.model.rinna.layers.8.input_layernorm.bias False
base_model.model.rinna.layers.8.post_attention_layernorm.weight False
base_model.model.rinna.layers.8.post_attention_layernorm.bias False
base_model.model.rinna.layers.8.attention.query_key_value.weight False
base_model.model.rinna.layers.8.attention.query_key_value.bias False
base_model.model.rinna.layers.8.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.8.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.8.attention.dense.weight False
base_model.model.rinna.layers.8.attention.dense.bias False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.9.input_layernorm.weight False
base_model.model.rinna.layers.9.input_layernorm.bias False
base_model.model.rinna.layers.9.post_attention_layernorm.weight False
base_model.model.rinna.layers.9.post_attention_layernorm.bias False
base_model.model.rinna.layers.9.attention.query_key_value.weight False
base_model.model.rinna.layers.9.attention.query_key_value.bias False
base_model.model.rinna.layers.9.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.9.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.9.attention.dense.weight False
base_model.model.rinna.layers.9.attention.dense.bias False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.10.input_layernorm.weight False
base_model.model.rinna.layers.10.input_layernorm.bias False
base_model.model.rinna.layers.10.post_attention_layernorm.weight False
base_model.model.rinna.layers.10.post_attention_layernorm.bias False
base_model.model.rinna.layers.10.attention.query_key_value.weight False
base_model.model.rinna.layers.10.attention.query_key_value.bias False
base_model.model.rinna.layers.10.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.10.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.10.attention.dense.weight False
base_model.model.rinna.layers.10.attention.dense.bias False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.11.input_layernorm.weight False
base_model.model.rinna.layers.11.input_layernorm.bias False
base_model.model.rinna.layers.11.post_attention_layernorm.weight False
base_model.model.rinna.layers.11.post_attention_layernorm.bias False
base_model.model.rinna.layers.11.attention.query_key_value.weight False
base_model.model.rinna.layers.11.attention.query_key_value.bias False
base_model.model.rinna.layers.11.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.11.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.11.attention.dense.weight False
base_model.model.rinna.layers.11.attention.dense.bias False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.12.input_layernorm.weight False
base_model.model.rinna.layers.12.input_layernorm.bias False
base_model.model.rinna.layers.12.post_attention_layernorm.weight False
base_model.model.rinna.layers.12.post_attention_layernorm.bias False
base_model.model.rinna.layers.12.attention.query_key_value.weight False
base_model.model.rinna.layers.12.attention.query_key_value.bias False
base_model.model.rinna.layers.12.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.12.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.12.attention.dense.weight False
base_model.model.rinna.layers.12.attention.dense.bias False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.13.input_layernorm.weight False
base_model.model.rinna.layers.13.input_layernorm.bias False
base_model.model.rinna.layers.13.post_attention_layernorm.weight False
base_model.model.rinna.layers.13.post_attention_layernorm.bias False
base_model.model.rinna.layers.13.attention.query_key_value.weight False
base_model.model.rinna.layers.13.attention.query_key_value.bias False
base_model.model.rinna.layers.13.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.13.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.13.attention.dense.weight False
base_model.model.rinna.layers.13.attention.dense.bias False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.14.input_layernorm.weight False
base_model.model.rinna.layers.14.input_layernorm.bias False
base_model.model.rinna.layers.14.post_attention_layernorm.weight False
base_model.model.rinna.layers.14.post_attention_layernorm.bias False
base_model.model.rinna.layers.14.attention.query_key_value.weight False
base_model.model.rinna.layers.14.attention.query_key_value.bias False
base_model.model.rinna.layers.14.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.14.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.14.attention.dense.weight False
base_model.model.rinna.layers.14.attention.dense.bias False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.15.input_layernorm.weight False
base_model.model.rinna.layers.15.input_layernorm.bias False
base_model.model.rinna.layers.15.post_attention_layernorm.weight False
base_model.model.rinna.layers.15.post_attention_layernorm.bias False
base_model.model.rinna.layers.15.attention.query_key_value.weight False
base_model.model.rinna.layers.15.attention.query_key_value.bias False
base_model.model.rinna.layers.15.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.15.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.15.attention.dense.weight False
base_model.model.rinna.layers.15.attention.dense.bias False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.16.input_layernorm.weight False
base_model.model.rinna.layers.16.input_layernorm.bias False
base_model.model.rinna.layers.16.post_attention_layernorm.weight False
base_model.model.rinna.layers.16.post_attention_layernorm.bias False
base_model.model.rinna.layers.16.attention.query_key_value.weight False
base_model.model.rinna.layers.16.attention.query_key_value.bias False
base_model.model.rinna.layers.16.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.16.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.16.attention.dense.weight False
base_model.model.rinna.layers.16.attention.dense.bias False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.17.input_layernorm.weight False
base_model.model.rinna.layers.17.input_layernorm.bias False
base_model.model.rinna.layers.17.post_attention_layernorm.weight False
base_model.model.rinna.layers.17.post_attention_layernorm.bias False
base_model.model.rinna.layers.17.attention.query_key_value.weight False
base_model.model.rinna.layers.17.attention.query_key_value.bias False
base_model.model.rinna.layers.17.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.17.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.17.attention.dense.weight False
base_model.model.rinna.layers.17.attention.dense.bias False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.18.input_layernorm.weight False
base_model.model.rinna.layers.18.input_layernorm.bias False
base_model.model.rinna.layers.18.post_attention_layernorm.weight False
base_model.model.rinna.layers.18.post_attention_layernorm.bias False
base_model.model.rinna.layers.18.attention.query_key_value.weight False
base_model.model.rinna.layers.18.attention.query_key_value.bias False
base_model.model.rinna.layers.18.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.18.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.18.attention.dense.weight False
base_model.model.rinna.layers.18.attention.dense.bias False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.19.input_layernorm.weight False
base_model.model.rinna.layers.19.input_layernorm.bias False
base_model.model.rinna.layers.19.post_attention_layernorm.weight False
base_model.model.rinna.layers.19.post_attention_layernorm.bias False
base_model.model.rinna.layers.19.attention.query_key_value.weight False
base_model.model.rinna.layers.19.attention.query_key_value.bias False
base_model.model.rinna.layers.19.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.19.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.19.attention.dense.weight False
base_model.model.rinna.layers.19.attention.dense.bias False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.20.input_layernorm.weight False
base_model.model.rinna.layers.20.input_layernorm.bias False
base_model.model.rinna.layers.20.post_attention_layernorm.weight False
base_model.model.rinna.layers.20.post_attention_layernorm.bias False
base_model.model.rinna.layers.20.attention.query_key_value.weight False
base_model.model.rinna.layers.20.attention.query_key_value.bias False
base_model.model.rinna.layers.20.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.20.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.20.attention.dense.weight False
base_model.model.rinna.layers.20.attention.dense.bias False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.21.input_layernorm.weight False
base_model.model.rinna.layers.21.input_layernorm.bias False
base_model.model.rinna.layers.21.post_attention_layernorm.weight False
base_model.model.rinna.layers.21.post_attention_layernorm.bias False
base_model.model.rinna.layers.21.attention.query_key_value.weight False
base_model.model.rinna.layers.21.attention.query_key_value.bias False
base_model.model.rinna.layers.21.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.21.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.21.attention.dense.weight False
base_model.model.rinna.layers.21.attention.dense.bias False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.22.input_layernorm.weight False
base_model.model.rinna.layers.22.input_layernorm.bias False
base_model.model.rinna.layers.22.post_attention_layernorm.weight False
base_model.model.rinna.layers.22.post_attention_layernorm.bias False
base_model.model.rinna.layers.22.attention.query_key_value.weight False
base_model.model.rinna.layers.22.attention.query_key_value.bias False
base_model.model.rinna.layers.22.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.22.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.22.attention.dense.weight False
base_model.model.rinna.layers.22.attention.dense.bias False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.23.input_layernorm.weight False
base_model.model.rinna.layers.23.input_layernorm.bias False
base_model.model.rinna.layers.23.post_attention_layernorm.weight False
base_model.model.rinna.layers.23.post_attention_layernorm.bias False
base_model.model.rinna.layers.23.attention.query_key_value.weight False
base_model.model.rinna.layers.23.attention.query_key_value.bias False
base_model.model.rinna.layers.23.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.23.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.23.attention.dense.weight False
base_model.model.rinna.layers.23.attention.dense.bias False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.24.input_layernorm.weight False
base_model.model.rinna.layers.24.input_layernorm.bias False
base_model.model.rinna.layers.24.post_attention_layernorm.weight False
base_model.model.rinna.layers.24.post_attention_layernorm.bias False
base_model.model.rinna.layers.24.attention.query_key_value.weight False
base_model.model.rinna.layers.24.attention.query_key_value.bias False
base_model.model.rinna.layers.24.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.24.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.24.attention.dense.weight False
base_model.model.rinna.layers.24.attention.dense.bias False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.25.input_layernorm.weight False
base_model.model.rinna.layers.25.input_layernorm.bias False
base_model.model.rinna.layers.25.post_attention_layernorm.weight False
base_model.model.rinna.layers.25.post_attention_layernorm.bias False
base_model.model.rinna.layers.25.attention.query_key_value.weight False
base_model.model.rinna.layers.25.attention.query_key_value.bias False
base_model.model.rinna.layers.25.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.25.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.25.attention.dense.weight False
base_model.model.rinna.layers.25.attention.dense.bias False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.26.input_layernorm.weight False
base_model.model.rinna.layers.26.input_layernorm.bias False
base_model.model.rinna.layers.26.post_attention_layernorm.weight False
base_model.model.rinna.layers.26.post_attention_layernorm.bias False
base_model.model.rinna.layers.26.attention.query_key_value.weight False
base_model.model.rinna.layers.26.attention.query_key_value.bias False
base_model.model.rinna.layers.26.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.26.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.26.attention.dense.weight False
base_model.model.rinna.layers.26.attention.dense.bias False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.27.input_layernorm.weight False
base_model.model.rinna.layers.27.input_layernorm.bias False
base_model.model.rinna.layers.27.post_attention_layernorm.weight False
base_model.model.rinna.layers.27.post_attention_layernorm.bias False
base_model.model.rinna.layers.27.attention.query_key_value.weight False
base_model.model.rinna.layers.27.attention.query_key_value.bias False
base_model.model.rinna.layers.27.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.27.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.27.attention.dense.weight False
base_model.model.rinna.layers.27.attention.dense.bias False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.28.input_layernorm.weight False
base_model.model.rinna.layers.28.input_layernorm.bias False
base_model.model.rinna.layers.28.post_attention_layernorm.weight False
base_model.model.rinna.layers.28.post_attention_layernorm.bias False
base_model.model.rinna.layers.28.attention.query_key_value.weight False
base_model.model.rinna.layers.28.attention.query_key_value.bias False
base_model.model.rinna.layers.28.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.28.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.28.attention.dense.weight False
base_model.model.rinna.layers.28.attention.dense.bias False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.29.input_layernorm.weight False
base_model.model.rinna.layers.29.input_layernorm.bias False
base_model.model.rinna.layers.29.post_attention_layernorm.weight False
base_model.model.rinna.layers.29.post_attention_layernorm.bias False
base_model.model.rinna.layers.29.attention.query_key_value.weight False
base_model.model.rinna.layers.29.attention.query_key_value.bias False
base_model.model.rinna.layers.29.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.29.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.29.attention.dense.weight False
base_model.model.rinna.layers.29.attention.dense.bias False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.30.input_layernorm.weight False
base_model.model.rinna.layers.30.input_layernorm.bias False
base_model.model.rinna.layers.30.post_attention_layernorm.weight False
base_model.model.rinna.layers.30.post_attention_layernorm.bias False
base_model.model.rinna.layers.30.attention.query_key_value.weight False
base_model.model.rinna.layers.30.attention.query_key_value.bias False
base_model.model.rinna.layers.30.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.30.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.30.attention.dense.weight False
base_model.model.rinna.layers.30.attention.dense.bias False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.31.input_layernorm.weight False
base_model.model.rinna.layers.31.input_layernorm.bias False
base_model.model.rinna.layers.31.post_attention_layernorm.weight False
base_model.model.rinna.layers.31.post_attention_layernorm.bias False
base_model.model.rinna.layers.31.attention.query_key_value.weight False
base_model.model.rinna.layers.31.attention.query_key_value.bias False
base_model.model.rinna.layers.31.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.31.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.31.attention.dense.weight False
base_model.model.rinna.layers.31.attention.dense.bias False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.32.input_layernorm.weight False
base_model.model.rinna.layers.32.input_layernorm.bias False
base_model.model.rinna.layers.32.post_attention_layernorm.weight False
base_model.model.rinna.layers.32.post_attention_layernorm.bias False
base_model.model.rinna.layers.32.attention.query_key_value.weight False
base_model.model.rinna.layers.32.attention.query_key_value.bias False
base_model.model.rinna.layers.32.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.32.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.32.attention.dense.weight False
base_model.model.rinna.layers.32.attention.dense.bias False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.33.input_layernorm.weight False
base_model.model.rinna.layers.33.input_layernorm.bias False
base_model.model.rinna.layers.33.post_attention_layernorm.weight False
base_model.model.rinna.layers.33.post_attention_layernorm.bias False
base_model.model.rinna.layers.33.attention.query_key_value.weight False
base_model.model.rinna.layers.33.attention.query_key_value.bias False
base_model.model.rinna.layers.33.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.33.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.33.attention.dense.weight False
base_model.model.rinna.layers.33.attention.dense.bias False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.34.input_layernorm.weight False
base_model.model.rinna.layers.34.input_layernorm.bias False
base_model.model.rinna.layers.34.post_attention_layernorm.weight False
base_model.model.rinna.layers.34.post_attention_layernorm.bias False
base_model.model.rinna.layers.34.attention.query_key_value.weight False
base_model.model.rinna.layers.34.attention.query_key_value.bias False
base_model.model.rinna.layers.34.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.34.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.34.attention.dense.weight False
base_model.model.rinna.layers.34.attention.dense.bias False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.35.input_layernorm.weight False
base_model.model.rinna.layers.35.input_layernorm.bias False
base_model.model.rinna.layers.35.post_attention_layernorm.weight False
base_model.model.rinna.layers.35.post_attention_layernorm.bias False
base_model.model.rinna.layers.35.attention.query_key_value.weight False
base_model.model.rinna.layers.35.attention.query_key_value.bias False
base_model.model.rinna.layers.35.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.35.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.35.attention.dense.weight False
base_model.model.rinna.layers.35.attention.dense.bias False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.bias False
base_model.model.rinna.final_layer_norm.weight False
base_model.model.rinna.final_layer_norm.bias False
base_model.model.my_linear.original_module.weight True
base_model.model.my_linear.original_module.bias True
base_model.model.my_linear.modules_to_save.default.weight True
base_model.model.my_linear.modules_to_save.default.bias True
base_model.model.my_linear2.original_module.weight True
base_model.model.my_linear2.original_module.bias True
base_model.model.my_linear2.modules_to_save.default.weight True
base_model.model.my_linear2.modules_to_save.default.bias True
Progress 0 / 832
epoch 0 	 loss 126.77330243587494
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  3 token_num = 3
3 :  180  ->  3 token_num = 3
4 :  252  ->  1 token_num = 4
5 :  252  ->  2 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  3 token_num = 4
8 :  252  ->  1 token_num = 4
9 :  252  ->  2 token_num = 4
10 :  252  ->  3 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  3 token_num = 4
13 :  252  ->  1 token_num = 4
14 :  252  ->  3 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  2 token_num = 5
17 :  432  ->  3 token_num = 5
18 :  432  ->  6 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  3 token_num = 5
21 :  432  ->  3 token_num = 5
22 :  432  ->  3 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  4 token_num = 5
25 :  540  ->  2 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  6 token_num = 6
29 :  540  ->  7 token_num = 6
30 :  540  ->  8 token_num = 6
31 :  540  ->  8 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  7 token_num = 7
34 :  792  ->  9 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  9 token_num = 7
37 :  792  ->  3 token_num = 7
38 :  792  ->  10 token_num = 7
39 :  792  ->  6 token_num = 7
40 :  792  ->  8 token_num = 7
41 :  792  ->  10 token_num = 7
42 :  936  ->  8 token_num = 8
43 :  936  ->  9 token_num = 8
44 :  1260  ->  10 token_num = 9
45 :  1260  ->  15 token_num = 9
46 :  1260  ->  18 token_num = 9
47 :  1440  ->  25 token_num = 10
48 :  1440  ->  10 token_num = 10
49 :  2052  ->  32 token_num = 12
50 :  2520  ->  45 token_num = 13
51 :  2520  ->  48 token_num = 13
52 :  2520  ->  30 token_num = 13
53 :  2772  ->  53 token_num = 14
54 :  3312  ->  28 token_num = 15
55 :  3600  ->  54 token_num = 16
56 :  4212  ->  35 token_num = 17
57 :  4212  ->  62 token_num = 17
58 :  4536  ->  86 token_num = 18
59 :  4536  ->  103 token_num = 18
60 :  4536  ->  50 token_num = 18
61 :  5220  ->  107 token_num = 19
62 :  5220  ->  63 token_num = 19
63 :  5220  ->  48 token_num = 19
64 :  5220  ->  102 token_num = 19
65 :  5580  ->  63 token_num = 20
66 :  6732  ->  60 token_num = 22
67 :  6732  ->  119 token_num = 22
68 :  6732  ->  112 token_num = 22
69 :  7560  ->  76 token_num = 23
70 :  7992  ->  93 token_num = 24
71 :  9360  ->  66 token_num = 26
72 :  9360  ->  168 token_num = 26
73 :  9360  ->  100 token_num = 26
74 :  10332  ->  234 token_num = 27
75 :  10836  ->  121 token_num = 28
76 :  11880  ->  132 token_num = 29
77 :  11880  ->  131 token_num = 29
78 :  11880  ->  127 token_num = 29
79 :  11880  ->  235 token_num = 29
80 :  12420  ->  117 token_num = 30
81 :  13536  ->  301 token_num = 31
82 :  14076  ->  305 token_num = 32
83 :  14076  ->  136 token_num = 32
84 :  14076  ->  235 token_num = 32
85 :  14076  ->  124 token_num = 32
86 :  17280  ->  383 token_num = 36
87 :  17280  ->  142 token_num = 36
88 :  17712  ->  331 token_num = 37
89 :  17712  ->  311 token_num = 37
90 :  18504  ->  358 token_num = 38
91 :  19224  ->  241 token_num = 39
92 :  19224  ->  231 token_num = 39
93 :  21312  ->  442 token_num = 43
94 :  21312  ->  359 token_num = 43
95 :  22140  ->  381 token_num = 45
96 :  22140  ->  323 token_num = 45
97 :  22716  ->  403 token_num = 46
98 :  22716  ->  187 token_num = 46
99 :  23472  ->  140 token_num = 49
100 :  23904  ->  597 token_num = 50
101 :  24840  ->  435 token_num = 55
102 :  25164  ->  616 token_num = 56
103 :  26244  ->  674 token_num = 63
104 :  27072  ->  706 token_num = 71
105 :  27900  ->  539 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 1 	 loss 54.0353579223156
0 :  72  ->  1 token_num = 2
1 :  180  ->  0 token_num = 3
2 :  180  ->  0 token_num = 3
3 :  180  ->  2 token_num = 3
4 :  252  ->  1 token_num = 4
5 :  252  ->  0 token_num = 4
6 :  252  ->  2 token_num = 4
7 :  252  ->  1 token_num = 4
8 :  252  ->  1 token_num = 4
9 :  252  ->  0 token_num = 4
10 :  252  ->  0 token_num = 4
11 :  252  ->  0 token_num = 4
12 :  252  ->  1 token_num = 4
13 :  252  ->  1 token_num = 4
14 :  252  ->  0 token_num = 4
15 :  252  ->  0 token_num = 4
16 :  432  ->  2 token_num = 5
17 :  432  ->  2 token_num = 5
18 :  432  ->  1 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  1 token_num = 5
21 :  432  ->  0 token_num = 5
22 :  432  ->  2 token_num = 5
23 :  432  ->  0 token_num = 5
24 :  432  ->  1 token_num = 5
25 :  540  ->  2 token_num = 6
26 :  540  ->  4 token_num = 6
27 :  540  ->  0 token_num = 6
28 :  540  ->  4 token_num = 6
29 :  540  ->  2 token_num = 6
30 :  540  ->  2 token_num = 6
31 :  540  ->  2 token_num = 6
32 :  540  ->  2 token_num = 6
33 :  792  ->  1 token_num = 7
34 :  792  ->  2 token_num = 7
35 :  792  ->  1 token_num = 7
36 :  792  ->  3 token_num = 7
37 :  792  ->  2 token_num = 7
38 :  792  ->  2 token_num = 7
39 :  792  ->  0 token_num = 7
40 :  792  ->  2 token_num = 7
41 :  792  ->  0 token_num = 7
42 :  936  ->  1 token_num = 8
43 :  936  ->  0 token_num = 8
44 :  1260  ->  4 token_num = 9
45 :  1260  ->  7 token_num = 9
46 :  1260  ->  4 token_num = 9
47 :  1440  ->  2 token_num = 10
48 :  1440  ->  1 token_num = 10
49 :  2052  ->  10 token_num = 12
50 :  2520  ->  10 token_num = 13
51 :  2520  ->  10 token_num = 13
52 :  2520  ->  10 token_num = 13
53 :  2772  ->  11 token_num = 14
54 :  3312  ->  8 token_num = 15
55 :  3600  ->  7 token_num = 16
56 :  4212  ->  8 token_num = 17
57 :  4212  ->  14 token_num = 17
58 :  4536  ->  36 token_num = 18
59 :  4536  ->  43 token_num = 18
60 :  4536  ->  7 token_num = 18
61 :  5220  ->  20 token_num = 19
62 :  5220  ->  30 token_num = 19
63 :  5220  ->  35 token_num = 19
64 :  5220  ->  33 token_num = 19
65 :  5580  ->  14 token_num = 20
66 :  6732  ->  14 token_num = 22
67 :  6732  ->  33 token_num = 22
68 :  6732  ->  22 token_num = 22
69 :  7560  ->  20 token_num = 23
70 :  7992  ->  18 token_num = 24
71 :  9360  ->  15 token_num = 26
72 :  9360  ->  77 token_num = 26
73 :  9360  ->  64 token_num = 26
74 :  10332  ->  25 token_num = 27
75 :  10836  ->  40 token_num = 28
76 :  11880  ->  24 token_num = 29
77 :  11880  ->  65 token_num = 29
78 :  11880  ->  49 token_num = 29
79 :  11880  ->  27 token_num = 29
80 :  12420  ->  18 token_num = 30
81 :  13536  ->  179 token_num = 31
82 :  14076  ->  69 token_num = 32
83 :  14076  ->  53 token_num = 32
84 :  14076  ->  46 token_num = 32
85 :  14076  ->  32 token_num = 32
86 :  17280  ->  192 token_num = 36
87 :  17280  ->  62 token_num = 36
88 :  17712  ->  104 token_num = 37
89 :  17712  ->  174 token_num = 37
90 :  18504  ->  156 token_num = 38
91 :  19224  ->  150 token_num = 39
92 :  19224  ->  132 token_num = 39
93 :  21312  ->  312 token_num = 43
94 :  21312  ->  98 token_num = 43
95 :  22140  ->  46 token_num = 45
96 :  22140  ->  115 token_num = 45
97 :  22716  ->  300 token_num = 46
98 :  22716  ->  125 token_num = 46
99 :  23472  ->  21 token_num = 49
100 :  23904  ->  366 token_num = 50
101 :  24840  ->  141 token_num = 55
102 :  25164  ->  329 token_num = 56
103 :  26244  ->  483 token_num = 63
104 :  27072  ->  403 token_num = 71
105 :  27900  ->  323 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 2 	 loss 44.67414475977421
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  3 token_num = 3
3 :  180  ->  3 token_num = 3
4 :  252  ->  2 token_num = 4
5 :  252  ->  3 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  3 token_num = 4
8 :  252  ->  2 token_num = 4
9 :  252  ->  2 token_num = 4
10 :  252  ->  2 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  3 token_num = 4
13 :  252  ->  2 token_num = 4
14 :  252  ->  2 token_num = 4
15 :  252  ->  2 token_num = 4
16 :  432  ->  2 token_num = 5
17 :  432  ->  2 token_num = 5
18 :  432  ->  3 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  3 token_num = 5
22 :  432  ->  3 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  4 token_num = 5
25 :  540  ->  3 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  4 token_num = 6
28 :  540  ->  5 token_num = 6
29 :  540  ->  5 token_num = 6
30 :  540  ->  7 token_num = 6
31 :  540  ->  6 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  6 token_num = 7
34 :  792  ->  6 token_num = 7
35 :  792  ->  3 token_num = 7
36 :  792  ->  6 token_num = 7
37 :  792  ->  3 token_num = 7
38 :  792  ->  6 token_num = 7
39 :  792  ->  6 token_num = 7
40 :  792  ->  7 token_num = 7
41 :  792  ->  8 token_num = 7
42 :  936  ->  6 token_num = 8
43 :  936  ->  4 token_num = 8
44 :  1260  ->  8 token_num = 9
45 :  1260  ->  15 token_num = 9
46 :  1260  ->  12 token_num = 9
47 :  1440  ->  17 token_num = 10
48 :  1440  ->  6 token_num = 10
49 :  2052  ->  22 token_num = 12
50 :  2520  ->  37 token_num = 13
51 :  2520  ->  33 token_num = 13
52 :  2520  ->  23 token_num = 13
53 :  2772  ->  32 token_num = 14
54 :  3312  ->  22 token_num = 15
55 :  3600  ->  38 token_num = 16
56 :  4212  ->  27 token_num = 17
57 :  4212  ->  43 token_num = 17
58 :  4536  ->  58 token_num = 18
59 :  4536  ->  65 token_num = 18
60 :  4536  ->  36 token_num = 18
61 :  5220  ->  78 token_num = 19
62 :  5220  ->  63 token_num = 19
63 :  5220  ->  44 token_num = 19
64 :  5220  ->  82 token_num = 19
65 :  5580  ->  35 token_num = 20
66 :  6732  ->  43 token_num = 22
67 :  6732  ->  77 token_num = 22
68 :  6732  ->  64 token_num = 22
69 :  7560  ->  40 token_num = 23
70 :  7992  ->  59 token_num = 24
71 :  9360  ->  58 token_num = 26
72 :  9360  ->  155 token_num = 26
73 :  9360  ->  90 token_num = 26
74 :  10332  ->  86 token_num = 27
75 :  10836  ->  78 token_num = 28
76 :  11880  ->  84 token_num = 29
77 :  11880  ->  108 token_num = 29
78 :  11880  ->  115 token_num = 29
79 :  11880  ->  118 token_num = 29
80 :  12420  ->  48 token_num = 30
81 :  13536  ->  256 token_num = 31
82 :  14076  ->  179 token_num = 32
83 :  14076  ->  93 token_num = 32
84 :  14076  ->  131 token_num = 32
85 :  14076  ->  59 token_num = 32
86 :  17280  ->  313 token_num = 36
87 :  17280  ->  111 token_num = 36
88 :  17712  ->  243 token_num = 37
89 :  17712  ->  242 token_num = 37
90 :  18504  ->  286 token_num = 38
91 :  19224  ->  211 token_num = 39
92 :  19224  ->  214 token_num = 39
93 :  21312  ->  434 token_num = 43
94 :  21312  ->  240 token_num = 43
95 :  22140  ->  307 token_num = 45
96 :  22140  ->  259 token_num = 45
97 :  22716  ->  396 token_num = 46
98 :  22716  ->  164 token_num = 46
99 :  23472  ->  123 token_num = 49
100 :  23904  ->  540 token_num = 50
101 :  24840  ->  357 token_num = 55
102 :  25164  ->  568 token_num = 56
103 :  26244  ->  629 token_num = 63
104 :  27072  ->  675 token_num = 71
105 :  27900  ->  528 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            4.0000        0.0          4.0  ...        1.0     0.1  0.1818
Arg1            0.0000        0.0          0.0  ...        NaN     0.0     NaN
Arg2            0.0000        0.0          0.0  ...        NaN     0.0     NaN
Arg3            0.0000        0.0          0.0  ...        NaN     0.0     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgA            0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN     0.0     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN     0.0     NaN
sum             4.0000        0.0          4.0  ...        1.0     0.1  0.1818
precision       1.0000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0213        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0417        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0117        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.0417 

Progress 0 / 832
epoch 3 	 loss 36.713044196367264
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  3 token_num = 3
3 :  180  ->  4 token_num = 3
4 :  252  ->  2 token_num = 4
5 :  252  ->  3 token_num = 4
6 :  252  ->  4 token_num = 4
7 :  252  ->  3 token_num = 4
8 :  252  ->  2 token_num = 4
9 :  252  ->  3 token_num = 4
10 :  252  ->  2 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  3 token_num = 4
13 :  252  ->  2 token_num = 4
14 :  252  ->  2 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  3 token_num = 5
17 :  432  ->  4 token_num = 5
18 :  432  ->  5 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  1 token_num = 5
21 :  432  ->  3 token_num = 5
22 :  432  ->  4 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  5 token_num = 5
25 :  540  ->  3 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  3 token_num = 6
28 :  540  ->  5 token_num = 6
29 :  540  ->  8 token_num = 6
30 :  540  ->  8 token_num = 6
31 :  540  ->  8 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  8 token_num = 7
34 :  792  ->  9 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  6 token_num = 7
37 :  792  ->  3 token_num = 7
38 :  792  ->  9 token_num = 7
39 :  792  ->  8 token_num = 7
40 :  792  ->  11 token_num = 7
41 :  792  ->  10 token_num = 7
42 :  936  ->  9 token_num = 8
43 :  936  ->  9 token_num = 8
44 :  1260  ->  8 token_num = 9
45 :  1260  ->  16 token_num = 9
46 :  1260  ->  13 token_num = 9
47 :  1440  ->  25 token_num = 10
48 :  1440  ->  9 token_num = 10
49 :  2052  ->  28 token_num = 12
50 :  2520  ->  44 token_num = 13
51 :  2520  ->  46 token_num = 13
52 :  2520  ->  24 token_num = 13
53 :  2772  ->  37 token_num = 14
54 :  3312  ->  18 token_num = 15
55 :  3600  ->  44 token_num = 16
56 :  4212  ->  34 token_num = 17
57 :  4212  ->  44 token_num = 17
58 :  4536  ->  60 token_num = 18
59 :  4536  ->  89 token_num = 18
60 :  4536  ->  36 token_num = 18
61 :  5220  ->  81 token_num = 19
62 :  5220  ->  63 token_num = 19
63 :  5220  ->  45 token_num = 19
64 :  5220  ->  87 token_num = 19
65 :  5580  ->  41 token_num = 20
66 :  6732  ->  46 token_num = 22
67 :  6732  ->  90 token_num = 22
68 :  6732  ->  88 token_num = 22
69 :  7560  ->  46 token_num = 23
70 :  7992  ->  79 token_num = 24
71 :  9360  ->  63 token_num = 26
72 :  9360  ->  178 token_num = 26
73 :  9360  ->  92 token_num = 26
74 :  10332  ->  133 token_num = 27
75 :  10836  ->  89 token_num = 28
76 :  11880  ->  101 token_num = 29
77 :  11880  ->  110 token_num = 29
78 :  11880  ->  128 token_num = 29
79 :  11880  ->  124 token_num = 29
80 :  12420  ->  86 token_num = 30
81 :  13536  ->  263 token_num = 31
82 :  14076  ->  184 token_num = 32
83 :  14076  ->  110 token_num = 32
84 :  14076  ->  169 token_num = 32
85 :  14076  ->  64 token_num = 32
86 :  17280  ->  342 token_num = 36
87 :  17280  ->  142 token_num = 36
88 :  17712  ->  290 token_num = 37
89 :  17712  ->  253 token_num = 37
90 :  18504  ->  295 token_num = 38
91 :  19224  ->  223 token_num = 39
92 :  19224  ->  219 token_num = 39
93 :  21312  ->  436 token_num = 43
94 :  21312  ->  245 token_num = 43
95 :  22140  ->  386 token_num = 45
96 :  22140  ->  271 token_num = 45
97 :  22716  ->  396 token_num = 46
98 :  22716  ->  166 token_num = 46
99 :  23472  ->  136 token_num = 49
100 :  23904  ->  574 token_num = 50
101 :  24840  ->  374 token_num = 55
102 :  25164  ->  589 token_num = 56
103 :  26244  ->  604 token_num = 63
104 :  27072  ->  660 token_num = 71
105 :  27900  ->  537 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            1.0000        0.0          1.0  ...        1.0   0.025  0.0488
Arg1            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg2            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
sum             1.0000        0.0          1.0  ...        1.0   0.025  0.0488
precision       1.0000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0053        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0106        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0031        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 4 	 loss 29.328792959451675
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  3 token_num = 3
3 :  180  ->  4 token_num = 3
4 :  252  ->  2 token_num = 4
5 :  252  ->  5 token_num = 4
6 :  252  ->  4 token_num = 4
7 :  252  ->  3 token_num = 4
8 :  252  ->  2 token_num = 4
9 :  252  ->  3 token_num = 4
10 :  252  ->  2 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  3 token_num = 4
13 :  252  ->  2 token_num = 4
14 :  252  ->  2 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  5 token_num = 5
17 :  432  ->  8 token_num = 5
18 :  432  ->  4 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  3 token_num = 5
21 :  432  ->  3 token_num = 5
22 :  432  ->  4 token_num = 5
23 :  432  ->  3 token_num = 5
24 :  432  ->  6 token_num = 5
25 :  540  ->  4 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  6 token_num = 6
28 :  540  ->  6 token_num = 6
29 :  540  ->  9 token_num = 6
30 :  540  ->  7 token_num = 6
31 :  540  ->  8 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  8 token_num = 7
34 :  792  ->  12 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  10 token_num = 7
37 :  792  ->  5 token_num = 7
38 :  792  ->  10 token_num = 7
39 :  792  ->  9 token_num = 7
40 :  792  ->  13 token_num = 7
41 :  792  ->  12 token_num = 7
42 :  936  ->  10 token_num = 8
43 :  936  ->  11 token_num = 8
44 :  1260  ->  11 token_num = 9
45 :  1260  ->  16 token_num = 9
46 :  1260  ->  19 token_num = 9
47 :  1440  ->  26 token_num = 10
48 :  1440  ->  9 token_num = 10
49 :  2052  ->  31 token_num = 12
50 :  2520  ->  47 token_num = 13
51 :  2520  ->  48 token_num = 13
52 :  2520  ->  30 token_num = 13
53 :  2772  ->  44 token_num = 14
54 :  3312  ->  23 token_num = 15
55 :  3600  ->  52 token_num = 16
56 :  4212  ->  36 token_num = 17
57 :  4212  ->  52 token_num = 17
58 :  4536  ->  67 token_num = 18
59 :  4536  ->  96 token_num = 18
60 :  4536  ->  44 token_num = 18
61 :  5220  ->  111 token_num = 19
62 :  5220  ->  63 token_num = 19
63 :  5220  ->  52 token_num = 19
64 :  5220  ->  111 token_num = 19
65 :  5580  ->  49 token_num = 20
66 :  6732  ->  51 token_num = 22
67 :  6732  ->  113 token_num = 22
68 :  6732  ->  94 token_num = 22
69 :  7560  ->  54 token_num = 23
70 :  7992  ->  86 token_num = 24
71 :  9360  ->  64 token_num = 26
72 :  9360  ->  178 token_num = 26
73 :  9360  ->  91 token_num = 26
74 :  10332  ->  150 token_num = 27
75 :  10836  ->  93 token_num = 28
76 :  11880  ->  132 token_num = 29
77 :  11880  ->  135 token_num = 29
78 :  11880  ->  127 token_num = 29
79 :  11880  ->  192 token_num = 29
80 :  12420  ->  103 token_num = 30
81 :  13536  ->  308 token_num = 31
82 :  14076  ->  191 token_num = 32
83 :  14076  ->  123 token_num = 32
84 :  14076  ->  189 token_num = 32
85 :  14076  ->  89 token_num = 32
86 :  17280  ->  356 token_num = 36
87 :  17280  ->  159 token_num = 36
88 :  17712  ->  313 token_num = 37
89 :  17712  ->  272 token_num = 37
90 :  18504  ->  312 token_num = 38
91 :  19224  ->  224 token_num = 39
92 :  19224  ->  241 token_num = 39
93 :  21312  ->  442 token_num = 43
94 :  21312  ->  310 token_num = 43
95 :  22140  ->  400 token_num = 45
96 :  22140  ->  275 token_num = 45
97 :  22716  ->  397 token_num = 46
98 :  22716  ->  170 token_num = 46
99 :  23472  ->  153 token_num = 49
100 :  23904  ->  590 token_num = 50
101 :  24840  ->  362 token_num = 55
102 :  25164  ->  605 token_num = 56
103 :  26244  ->  598 token_num = 63
104 :  27072  ->  618 token_num = 71
105 :  27900  ->  544 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 5 	 loss 23.791815288364887
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  5 token_num = 3
3 :  180  ->  5 token_num = 3
4 :  252  ->  2 token_num = 4
5 :  252  ->  6 token_num = 4
6 :  252  ->  4 token_num = 4
7 :  252  ->  4 token_num = 4
8 :  252  ->  2 token_num = 4
9 :  252  ->  3 token_num = 4
10 :  252  ->  3 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  5 token_num = 4
13 :  252  ->  2 token_num = 4
14 :  252  ->  6 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  4 token_num = 5
17 :  432  ->  8 token_num = 5
18 :  432  ->  5 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  4 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  7 token_num = 5
25 :  540  ->  6 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  7 token_num = 6
28 :  540  ->  11 token_num = 6
29 :  540  ->  6 token_num = 6
30 :  540  ->  9 token_num = 6
31 :  540  ->  12 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  11 token_num = 7
34 :  792  ->  11 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  7 token_num = 7
37 :  792  ->  4 token_num = 7
38 :  792  ->  7 token_num = 7
39 :  792  ->  9 token_num = 7
40 :  792  ->  15 token_num = 7
41 :  792  ->  14 token_num = 7
42 :  936  ->  7 token_num = 8
43 :  936  ->  9 token_num = 8
44 :  1260  ->  6 token_num = 9
45 :  1260  ->  14 token_num = 9
46 :  1260  ->  14 token_num = 9
47 :  1440  ->  24 token_num = 10
48 :  1440  ->  12 token_num = 10
49 :  2052  ->  34 token_num = 12
50 :  2520  ->  40 token_num = 13
51 :  2520  ->  42 token_num = 13
52 :  2520  ->  25 token_num = 13
53 :  2772  ->  37 token_num = 14
54 :  3312  ->  16 token_num = 15
55 :  3600  ->  36 token_num = 16
56 :  4212  ->  31 token_num = 17
57 :  4212  ->  42 token_num = 17
58 :  4536  ->  55 token_num = 18
59 :  4536  ->  92 token_num = 18
60 :  4536  ->  38 token_num = 18
61 :  5220  ->  75 token_num = 19
62 :  5220  ->  65 token_num = 19
63 :  5220  ->  47 token_num = 19
64 :  5220  ->  66 token_num = 19
65 :  5580  ->  42 token_num = 20
66 :  6732  ->  45 token_num = 22
67 :  6732  ->  96 token_num = 22
68 :  6732  ->  78 token_num = 22
69 :  7560  ->  36 token_num = 23
70 :  7992  ->  73 token_num = 24
71 :  9360  ->  66 token_num = 26
72 :  9360  ->  185 token_num = 26
73 :  9360  ->  92 token_num = 26
74 :  10332  ->  150 token_num = 27
75 :  10836  ->  57 token_num = 28
76 :  11880  ->  94 token_num = 29
77 :  11880  ->  105 token_num = 29
78 :  11880  ->  127 token_num = 29
79 :  11880  ->  113 token_num = 29
80 :  12420  ->  86 token_num = 30
81 :  13536  ->  285 token_num = 31
82 :  14076  ->  190 token_num = 32
83 :  14076  ->  85 token_num = 32
84 :  14076  ->  169 token_num = 32
85 :  14076  ->  55 token_num = 32
86 :  17280  ->  325 token_num = 36
87 :  17280  ->  125 token_num = 36
88 :  17712  ->  267 token_num = 37
89 :  17712  ->  258 token_num = 37
90 :  18504  ->  274 token_num = 38
91 :  19224  ->  213 token_num = 39
92 :  19224  ->  224 token_num = 39
93 :  21312  ->  444 token_num = 43
94 :  21312  ->  254 token_num = 43
95 :  22140  ->  359 token_num = 45
96 :  22140  ->  231 token_num = 45
97 :  22716  ->  388 token_num = 46
98 :  22716  ->  151 token_num = 46
99 :  23472  ->  117 token_num = 49
100 :  23904  ->  545 token_num = 50
101 :  24840  ->  313 token_num = 55
102 :  25164  ->  587 token_num = 56
103 :  26244  ->  582 token_num = 63
104 :  27072  ->  639 token_num = 71
105 :  27900  ->  528 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           12.0000        3.0         15.0  ...        0.8  0.3000  0.4364
Arg1            2.0000        0.0          2.0  ...        1.0  0.0256  0.0500
Arg2            0.0000        1.0          1.0  ...        0.0  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            14.0000        4.0         18.0  ...        1.8  0.3256  0.4864
precision       0.7778        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0745        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1359        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0314        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1359 

Progress 0 / 832
epoch 6 	 loss 20.45490726083517
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  6 token_num = 3
3 :  180  ->  4 token_num = 3
4 :  252  ->  2 token_num = 4
5 :  252  ->  5 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  4 token_num = 4
8 :  252  ->  2 token_num = 4
9 :  252  ->  3 token_num = 4
10 :  252  ->  3 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  4 token_num = 4
13 :  252  ->  2 token_num = 4
14 :  252  ->  5 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  4 token_num = 5
17 :  432  ->  6 token_num = 5
18 :  432  ->  5 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  5 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  3 token_num = 5
24 :  432  ->  5 token_num = 5
25 :  540  ->  4 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  2 token_num = 6
28 :  540  ->  9 token_num = 6
29 :  540  ->  6 token_num = 6
30 :  540  ->  5 token_num = 6
31 :  540  ->  12 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  5 token_num = 7
34 :  792  ->  6 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  5 token_num = 7
37 :  792  ->  4 token_num = 7
38 :  792  ->  7 token_num = 7
39 :  792  ->  5 token_num = 7
40 :  792  ->  12 token_num = 7
41 :  792  ->  8 token_num = 7
42 :  936  ->  5 token_num = 8
43 :  936  ->  6 token_num = 8
44 :  1260  ->  6 token_num = 9
45 :  1260  ->  12 token_num = 9
46 :  1260  ->  12 token_num = 9
47 :  1440  ->  18 token_num = 10
48 :  1440  ->  8 token_num = 10
49 :  2052  ->  29 token_num = 12
50 :  2520  ->  36 token_num = 13
51 :  2520  ->  35 token_num = 13
52 :  2520  ->  20 token_num = 13
53 :  2772  ->  34 token_num = 14
54 :  3312  ->  9 token_num = 15
55 :  3600  ->  32 token_num = 16
56 :  4212  ->  30 token_num = 17
57 :  4212  ->  35 token_num = 17
58 :  4536  ->  53 token_num = 18
59 :  4536  ->  76 token_num = 18
60 :  4536  ->  23 token_num = 18
61 :  5220  ->  65 token_num = 19
62 :  5220  ->  64 token_num = 19
63 :  5220  ->  36 token_num = 19
64 :  5220  ->  70 token_num = 19
65 :  5580  ->  41 token_num = 20
66 :  6732  ->  38 token_num = 22
67 :  6732  ->  87 token_num = 22
68 :  6732  ->  73 token_num = 22
69 :  7560  ->  35 token_num = 23
70 :  7992  ->  65 token_num = 24
71 :  9360  ->  59 token_num = 26
72 :  9360  ->  142 token_num = 26
73 :  9360  ->  82 token_num = 26
74 :  10332  ->  126 token_num = 27
75 :  10836  ->  43 token_num = 28
76 :  11880  ->  79 token_num = 29
77 :  11880  ->  100 token_num = 29
78 :  11880  ->  119 token_num = 29
79 :  11880  ->  89 token_num = 29
80 :  12420  ->  75 token_num = 30
81 :  13536  ->  237 token_num = 31
82 :  14076  ->  142 token_num = 32
83 :  14076  ->  61 token_num = 32
84 :  14076  ->  163 token_num = 32
85 :  14076  ->  50 token_num = 32
86 :  17280  ->  256 token_num = 36
87 :  17280  ->  45 token_num = 36
88 :  17712  ->  194 token_num = 37
89 :  17712  ->  233 token_num = 37
90 :  18504  ->  236 token_num = 38
91 :  19224  ->  201 token_num = 39
92 :  19224  ->  190 token_num = 39
93 :  21312  ->  433 token_num = 43
94 :  21312  ->  252 token_num = 43
95 :  22140  ->  396 token_num = 45
96 :  22140  ->  188 token_num = 45
97 :  22716  ->  393 token_num = 46
98 :  22716  ->  153 token_num = 46
99 :  23472  ->  106 token_num = 49
100 :  23904  ->  516 token_num = 50
101 :  24840  ->  241 token_num = 55
102 :  25164  ->  521 token_num = 56
103 :  26244  ->  557 token_num = 63
104 :  27072  ->  567 token_num = 71
105 :  27900  ->  532 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            6.0000        1.0          7.0  ...     0.8571  0.1500  0.2553
Arg1            6.0000        5.0         11.0  ...     0.5455  0.0769  0.1348
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            12.0000        6.0         18.0  ...     1.4026  0.2269  0.3902
precision       0.6667        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0638        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1165        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0252        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 7 	 loss 19.586561355739832
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  8 token_num = 3
3 :  180  ->  6 token_num = 3
4 :  252  ->  4 token_num = 4
5 :  252  ->  7 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  7 token_num = 4
8 :  252  ->  4 token_num = 4
9 :  252  ->  9 token_num = 4
10 :  252  ->  5 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  7 token_num = 4
13 :  252  ->  4 token_num = 4
14 :  252  ->  7 token_num = 4
15 :  252  ->  3 token_num = 4
16 :  432  ->  8 token_num = 5
17 :  432  ->  12 token_num = 5
18 :  432  ->  5 token_num = 5
19 :  432  ->  9 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  8 token_num = 5
22 :  432  ->  7 token_num = 5
23 :  432  ->  3 token_num = 5
24 :  432  ->  13 token_num = 5
25 :  540  ->  7 token_num = 6
26 :  540  ->  7 token_num = 6
27 :  540  ->  6 token_num = 6
28 :  540  ->  19 token_num = 6
29 :  540  ->  13 token_num = 6
30 :  540  ->  17 token_num = 6
31 :  540  ->  19 token_num = 6
32 :  540  ->  4 token_num = 6
33 :  792  ->  13 token_num = 7
34 :  792  ->  14 token_num = 7
35 :  792  ->  9 token_num = 7
36 :  792  ->  15 token_num = 7
37 :  792  ->  10 token_num = 7
38 :  792  ->  17 token_num = 7
39 :  792  ->  11 token_num = 7
40 :  792  ->  22 token_num = 7
41 :  792  ->  14 token_num = 7
42 :  936  ->  15 token_num = 8
43 :  936  ->  21 token_num = 8
44 :  1260  ->  8 token_num = 9
45 :  1260  ->  16 token_num = 9
46 :  1260  ->  24 token_num = 9
47 :  1440  ->  27 token_num = 10
48 :  1440  ->  16 token_num = 10
49 :  2052  ->  38 token_num = 12
50 :  2520  ->  30 token_num = 13
51 :  2520  ->  54 token_num = 13
52 :  2520  ->  32 token_num = 13
53 :  2772  ->  37 token_num = 14
54 :  3312  ->  29 token_num = 15
55 :  3600  ->  40 token_num = 16
56 :  4212  ->  26 token_num = 17
57 :  4212  ->  44 token_num = 17
58 :  4536  ->  70 token_num = 18
59 :  4536  ->  73 token_num = 18
60 :  4536  ->  46 token_num = 18
61 :  5220  ->  89 token_num = 19
62 :  5220  ->  36 token_num = 19
63 :  5220  ->  47 token_num = 19
64 :  5220  ->  118 token_num = 19
65 :  5580  ->  45 token_num = 20
66 :  6732  ->  51 token_num = 22
67 :  6732  ->  82 token_num = 22
68 :  6732  ->  77 token_num = 22
69 :  7560  ->  33 token_num = 23
70 :  7992  ->  81 token_num = 24
71 :  9360  ->  44 token_num = 26
72 :  9360  ->  87 token_num = 26
73 :  9360  ->  60 token_num = 26
74 :  10332  ->  141 token_num = 27
75 :  10836  ->  54 token_num = 28
76 :  11880  ->  90 token_num = 29
77 :  11880  ->  81 token_num = 29
78 :  11880  ->  103 token_num = 29
79 :  11880  ->  181 token_num = 29
80 :  12420  ->  89 token_num = 30
81 :  13536  ->  144 token_num = 31
82 :  14076  ->  81 token_num = 32
83 :  14076  ->  22 token_num = 32
84 :  14076  ->  88 token_num = 32
85 :  14076  ->  72 token_num = 32
86 :  17280  ->  153 token_num = 36
87 :  17280  ->  73 token_num = 36
88 :  17712  ->  159 token_num = 37
89 :  17712  ->  180 token_num = 37
90 :  18504  ->  157 token_num = 38
91 :  19224  ->  99 token_num = 39
92 :  19224  ->  149 token_num = 39
93 :  21312  ->  232 token_num = 43
94 :  21312  ->  176 token_num = 43
95 :  22140  ->  156 token_num = 45
96 :  22140  ->  82 token_num = 45
97 :  22716  ->  243 token_num = 46
98 :  22716  ->  97 token_num = 46
99 :  23472  ->  176 token_num = 49
100 :  23904  ->  257 token_num = 50
101 :  24840  ->  190 token_num = 55
102 :  25164  ->  225 token_num = 56
103 :  26244  ->  310 token_num = 63
104 :  27072  ->  192 token_num = 71
105 :  27900  ->  304 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            1.0000        1.0          2.0  ...        0.5   0.025  0.0476
Arg1            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg2            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
sum             1.0000        1.0          2.0  ...        0.5   0.025  0.0476
precision       0.5000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0053        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0105        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0031        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 8 	 loss 15.757663153111935
0 :  72  ->  2 token_num = 2
1 :  180  ->  2 token_num = 3
2 :  180  ->  7 token_num = 3
3 :  180  ->  8 token_num = 3
4 :  252  ->  5 token_num = 4
5 :  252  ->  4 token_num = 4
6 :  252  ->  5 token_num = 4
7 :  252  ->  7 token_num = 4
8 :  252  ->  5 token_num = 4
9 :  252  ->  7 token_num = 4
10 :  252  ->  5 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  8 token_num = 4
13 :  252  ->  5 token_num = 4
14 :  252  ->  7 token_num = 4
15 :  252  ->  7 token_num = 4
16 :  432  ->  7 token_num = 5
17 :  432  ->  9 token_num = 5
18 :  432  ->  9 token_num = 5
19 :  432  ->  9 token_num = 5
20 :  432  ->  3 token_num = 5
21 :  432  ->  6 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  3 token_num = 5
24 :  432  ->  11 token_num = 5
25 :  540  ->  4 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  15 token_num = 6
29 :  540  ->  12 token_num = 6
30 :  540  ->  19 token_num = 6
31 :  540  ->  24 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  9 token_num = 7
34 :  792  ->  11 token_num = 7
35 :  792  ->  7 token_num = 7
36 :  792  ->  13 token_num = 7
37 :  792  ->  4 token_num = 7
38 :  792  ->  14 token_num = 7
39 :  792  ->  5 token_num = 7
40 :  792  ->  20 token_num = 7
41 :  792  ->  14 token_num = 7
42 :  936  ->  11 token_num = 8
43 :  936  ->  21 token_num = 8
44 :  1260  ->  7 token_num = 9
45 :  1260  ->  20 token_num = 9
46 :  1260  ->  20 token_num = 9
47 :  1440  ->  30 token_num = 10
48 :  1440  ->  15 token_num = 10
49 :  2052  ->  42 token_num = 12
50 :  2520  ->  46 token_num = 13
51 :  2520  ->  67 token_num = 13
52 :  2520  ->  37 token_num = 13
53 :  2772  ->  44 token_num = 14
54 :  3312  ->  32 token_num = 15
55 :  3600  ->  41 token_num = 16
56 :  4212  ->  39 token_num = 17
57 :  4212  ->  44 token_num = 17
58 :  4536  ->  70 token_num = 18
59 :  4536  ->  102 token_num = 18
60 :  4536  ->  35 token_num = 18
61 :  5220  ->  130 token_num = 19
62 :  5220  ->  60 token_num = 19
63 :  5220  ->  58 token_num = 19
64 :  5220  ->  124 token_num = 19
65 :  5580  ->  51 token_num = 20
66 :  6732  ->  58 token_num = 22
67 :  6732  ->  89 token_num = 22
68 :  6732  ->  83 token_num = 22
69 :  7560  ->  44 token_num = 23
70 :  7992  ->  93 token_num = 24
71 :  9360  ->  83 token_num = 26
72 :  9360  ->  139 token_num = 26
73 :  9360  ->  91 token_num = 26
74 :  10332  ->  132 token_num = 27
75 :  10836  ->  71 token_num = 28
76 :  11880  ->  102 token_num = 29
77 :  11880  ->  137 token_num = 29
78 :  11880  ->  144 token_num = 29
79 :  11880  ->  163 token_num = 29
80 :  12420  ->  113 token_num = 30
81 :  13536  ->  240 token_num = 31
82 :  14076  ->  126 token_num = 32
83 :  14076  ->  68 token_num = 32
84 :  14076  ->  180 token_num = 32
85 :  14076  ->  85 token_num = 32
86 :  17280  ->  258 token_num = 36
87 :  17280  ->  76 token_num = 36
88 :  17712  ->  215 token_num = 37
89 :  17712  ->  253 token_num = 37
90 :  18504  ->  213 token_num = 38
91 :  19224  ->  211 token_num = 39
92 :  19224  ->  226 token_num = 39
93 :  21312  ->  428 token_num = 43
94 :  21312  ->  287 token_num = 43
95 :  22140  ->  314 token_num = 45
96 :  22140  ->  192 token_num = 45
97 :  22716  ->  398 token_num = 46
98 :  22716  ->  137 token_num = 46
99 :  23472  ->  159 token_num = 49
100 :  23904  ->  485 token_num = 50
101 :  24840  ->  258 token_num = 55
102 :  25164  ->  472 token_num = 56
103 :  26244  ->  592 token_num = 63
104 :  27072  ->  491 token_num = 71
105 :  27900  ->  513 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           12.0000        3.0         15.0  ...        0.8  0.3000  0.4364
Arg1            6.0000        0.0          6.0  ...        1.0  0.0769  0.1429
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            18.0000        3.0         21.0  ...        1.8  0.3769  0.5792
precision       0.8571        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0957        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1722        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0374        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1722 

Progress 0 / 832
epoch 9 	 loss 13.072117105126381
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  7 token_num = 3
3 :  180  ->  7 token_num = 3
4 :  252  ->  5 token_num = 4
5 :  252  ->  14 token_num = 4
6 :  252  ->  8 token_num = 4
7 :  252  ->  10 token_num = 4
8 :  252  ->  5 token_num = 4
9 :  252  ->  15 token_num = 4
10 :  252  ->  8 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  10 token_num = 4
13 :  252  ->  5 token_num = 4
14 :  252  ->  10 token_num = 4
15 :  252  ->  7 token_num = 4
16 :  432  ->  8 token_num = 5
17 :  432  ->  16 token_num = 5
18 :  432  ->  13 token_num = 5
19 :  432  ->  11 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  9 token_num = 5
22 :  432  ->  11 token_num = 5
23 :  432  ->  4 token_num = 5
24 :  432  ->  16 token_num = 5
25 :  540  ->  6 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  18 token_num = 6
29 :  540  ->  21 token_num = 6
30 :  540  ->  23 token_num = 6
31 :  540  ->  25 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  19 token_num = 7
34 :  792  ->  12 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  17 token_num = 7
37 :  792  ->  8 token_num = 7
38 :  792  ->  12 token_num = 7
39 :  792  ->  14 token_num = 7
40 :  792  ->  30 token_num = 7
41 :  792  ->  13 token_num = 7
42 :  936  ->  11 token_num = 8
43 :  936  ->  19 token_num = 8
44 :  1260  ->  9 token_num = 9
45 :  1260  ->  19 token_num = 9
46 :  1260  ->  20 token_num = 9
47 :  1440  ->  21 token_num = 10
48 :  1440  ->  15 token_num = 10
49 :  2052  ->  42 token_num = 12
50 :  2520  ->  32 token_num = 13
51 :  2520  ->  52 token_num = 13
52 :  2520  ->  24 token_num = 13
53 :  2772  ->  28 token_num = 14
54 :  3312  ->  29 token_num = 15
55 :  3600  ->  33 token_num = 16
56 :  4212  ->  34 token_num = 17
57 :  4212  ->  33 token_num = 17
58 :  4536  ->  55 token_num = 18
59 :  4536  ->  54 token_num = 18
60 :  4536  ->  47 token_num = 18
61 :  5220  ->  59 token_num = 19
62 :  5220  ->  63 token_num = 19
63 :  5220  ->  47 token_num = 19
64 :  5220  ->  86 token_num = 19
65 :  5580  ->  48 token_num = 20
66 :  6732  ->  34 token_num = 22
67 :  6732  ->  62 token_num = 22
68 :  6732  ->  75 token_num = 22
69 :  7560  ->  31 token_num = 23
70 :  7992  ->  70 token_num = 24
71 :  9360  ->  75 token_num = 26
72 :  9360  ->  108 token_num = 26
73 :  9360  ->  73 token_num = 26
74 :  10332  ->  127 token_num = 27
75 :  10836  ->  49 token_num = 28
76 :  11880  ->  102 token_num = 29
77 :  11880  ->  98 token_num = 29
78 :  11880  ->  127 token_num = 29
79 :  11880  ->  135 token_num = 29
80 :  12420  ->  87 token_num = 30
81 :  13536  ->  164 token_num = 31
82 :  14076  ->  97 token_num = 32
83 :  14076  ->  53 token_num = 32
84 :  14076  ->  150 token_num = 32
85 :  14076  ->  66 token_num = 32
86 :  17280  ->  254 token_num = 36
87 :  17280  ->  36 token_num = 36
88 :  17712  ->  134 token_num = 37
89 :  17712  ->  218 token_num = 37
90 :  18504  ->  197 token_num = 38
91 :  19224  ->  155 token_num = 39
92 :  19224  ->  196 token_num = 39
93 :  21312  ->  342 token_num = 43
94 :  21312  ->  246 token_num = 43
95 :  22140  ->  228 token_num = 45
96 :  22140  ->  157 token_num = 45
97 :  22716  ->  368 token_num = 46
98 :  22716  ->  130 token_num = 46
99 :  23472  ->  120 token_num = 49
100 :  23904  ->  367 token_num = 50
101 :  24840  ->  272 token_num = 55
102 :  25164  ->  313 token_num = 56
103 :  26244  ->  470 token_num = 63
104 :  27072  ->  306 token_num = 71
105 :  27900  ->  462 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           23.0000        8.0         31.0  ...     0.7419  0.5750  0.6479
Arg1           12.0000        3.0         15.0  ...     0.8000  0.1538  0.2581
Arg2            0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            35.0000       12.0         47.0  ...     1.5419  0.7288  0.9060
precision       0.7447        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1862        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2979        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0584        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.2979 

Progress 0 / 832
epoch 10 	 loss 10.035054424777627
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  6 token_num = 3
3 :  180  ->  7 token_num = 3
4 :  252  ->  5 token_num = 4
5 :  252  ->  8 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  6 token_num = 4
8 :  252  ->  5 token_num = 4
9 :  252  ->  10 token_num = 4
10 :  252  ->  8 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  7 token_num = 4
13 :  252  ->  5 token_num = 4
14 :  252  ->  7 token_num = 4
15 :  252  ->  6 token_num = 4
16 :  432  ->  7 token_num = 5
17 :  432  ->  11 token_num = 5
18 :  432  ->  9 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  1 token_num = 5
21 :  432  ->  8 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  3 token_num = 5
24 :  432  ->  7 token_num = 5
25 :  540  ->  4 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  11 token_num = 6
29 :  540  ->  7 token_num = 6
30 :  540  ->  12 token_num = 6
31 :  540  ->  18 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  8 token_num = 7
34 :  792  ->  8 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  9 token_num = 7
37 :  792  ->  6 token_num = 7
38 :  792  ->  7 token_num = 7
39 :  792  ->  8 token_num = 7
40 :  792  ->  20 token_num = 7
41 :  792  ->  9 token_num = 7
42 :  936  ->  9 token_num = 8
43 :  936  ->  20 token_num = 8
44 :  1260  ->  8 token_num = 9
45 :  1260  ->  10 token_num = 9
46 :  1260  ->  9 token_num = 9
47 :  1440  ->  13 token_num = 10
48 :  1440  ->  12 token_num = 10
49 :  2052  ->  30 token_num = 12
50 :  2520  ->  33 token_num = 13
51 :  2520  ->  34 token_num = 13
52 :  2520  ->  21 token_num = 13
53 :  2772  ->  34 token_num = 14
54 :  3312  ->  13 token_num = 15
55 :  3600  ->  25 token_num = 16
56 :  4212  ->  16 token_num = 17
57 :  4212  ->  28 token_num = 17
58 :  4536  ->  49 token_num = 18
59 :  4536  ->  55 token_num = 18
60 :  4536  ->  24 token_num = 18
61 :  5220  ->  51 token_num = 19
62 :  5220  ->  45 token_num = 19
63 :  5220  ->  39 token_num = 19
64 :  5220  ->  47 token_num = 19
65 :  5580  ->  44 token_num = 20
66 :  6732  ->  25 token_num = 22
67 :  6732  ->  70 token_num = 22
68 :  6732  ->  70 token_num = 22
69 :  7560  ->  33 token_num = 23
70 :  7992  ->  71 token_num = 24
71 :  9360  ->  57 token_num = 26
72 :  9360  ->  149 token_num = 26
73 :  9360  ->  82 token_num = 26
74 :  10332  ->  128 token_num = 27
75 :  10836  ->  38 token_num = 28
76 :  11880  ->  69 token_num = 29
77 :  11880  ->  75 token_num = 29
78 :  11880  ->  126 token_num = 29
79 :  11880  ->  122 token_num = 29
80 :  12420  ->  70 token_num = 30
81 :  13536  ->  199 token_num = 31
82 :  14076  ->  134 token_num = 32
83 :  14076  ->  59 token_num = 32
84 :  14076  ->  171 token_num = 32
85 :  14076  ->  68 token_num = 32
86 :  17280  ->  258 token_num = 36
87 :  17280  ->  26 token_num = 36
88 :  17712  ->  187 token_num = 37
89 :  17712  ->  227 token_num = 37
90 :  18504  ->  255 token_num = 38
91 :  19224  ->  187 token_num = 39
92 :  19224  ->  188 token_num = 39
93 :  21312  ->  354 token_num = 43
94 :  21312  ->  220 token_num = 43
95 :  22140  ->  363 token_num = 45
96 :  22140  ->  169 token_num = 45
97 :  22716  ->  393 token_num = 46
98 :  22716  ->  130 token_num = 46
99 :  23472  ->  83 token_num = 49
100 :  23904  ->  435 token_num = 50
101 :  24840  ->  241 token_num = 55
102 :  25164  ->  414 token_num = 56
103 :  26244  ->  482 token_num = 63
104 :  27072  ->  436 token_num = 71
105 :  27900  ->  475 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           23.0000        6.0         29.0  ...     0.7931  0.5750  0.6667
Arg1           17.0000        5.0         22.0  ...     0.7727  0.2179  0.3400
Arg2            1.0000        1.0          2.0  ...     0.5000  0.0294  0.0556
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            41.0000       12.0         53.0  ...     2.0658  0.8224  1.0622
precision       0.7736        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2181        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3402        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0685        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.3402 

Progress 0 / 832
epoch 11 	 loss 7.2024407282005996
0 :  72  ->  2 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  6 token_num = 3
3 :  180  ->  8 token_num = 3
4 :  252  ->  6 token_num = 4
5 :  252  ->  6 token_num = 4
6 :  252  ->  5 token_num = 4
7 :  252  ->  6 token_num = 4
8 :  252  ->  6 token_num = 4
9 :  252  ->  7 token_num = 4
10 :  252  ->  6 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  8 token_num = 4
13 :  252  ->  6 token_num = 4
14 :  252  ->  8 token_num = 4
15 :  252  ->  5 token_num = 4
16 :  432  ->  6 token_num = 5
17 :  432  ->  7 token_num = 5
18 :  432  ->  7 token_num = 5
19 :  432  ->  6 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  6 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  4 token_num = 5
24 :  432  ->  7 token_num = 5
25 :  540  ->  4 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  14 token_num = 6
29 :  540  ->  8 token_num = 6
30 :  540  ->  13 token_num = 6
31 :  540  ->  17 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  6 token_num = 7
34 :  792  ->  9 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  15 token_num = 7
37 :  792  ->  8 token_num = 7
38 :  792  ->  7 token_num = 7
39 :  792  ->  6 token_num = 7
40 :  792  ->  17 token_num = 7
41 :  792  ->  10 token_num = 7
42 :  936  ->  7 token_num = 8
43 :  936  ->  17 token_num = 8
44 :  1260  ->  10 token_num = 9
45 :  1260  ->  15 token_num = 9
46 :  1260  ->  11 token_num = 9
47 :  1440  ->  19 token_num = 10
48 :  1440  ->  11 token_num = 10
49 :  2052  ->  34 token_num = 12
50 :  2520  ->  41 token_num = 13
51 :  2520  ->  62 token_num = 13
52 :  2520  ->  31 token_num = 13
53 :  2772  ->  43 token_num = 14
54 :  3312  ->  26 token_num = 15
55 :  3600  ->  41 token_num = 16
56 :  4212  ->  33 token_num = 17
57 :  4212  ->  53 token_num = 17
58 :  4536  ->  67 token_num = 18
59 :  4536  ->  80 token_num = 18
60 :  4536  ->  28 token_num = 18
61 :  5220  ->  88 token_num = 19
62 :  5220  ->  66 token_num = 19
63 :  5220  ->  54 token_num = 19
64 :  5220  ->  141 token_num = 19
65 :  5580  ->  52 token_num = 20
66 :  6732  ->  58 token_num = 22
67 :  6732  ->  93 token_num = 22
68 :  6732  ->  74 token_num = 22
69 :  7560  ->  46 token_num = 23
70 :  7992  ->  91 token_num = 24
71 :  9360  ->  80 token_num = 26
72 :  9360  ->  182 token_num = 26
73 :  9360  ->  86 token_num = 26
74 :  10332  ->  163 token_num = 27
75 :  10836  ->  62 token_num = 28
76 :  11880  ->  112 token_num = 29
77 :  11880  ->  130 token_num = 29
78 :  11880  ->  146 token_num = 29
79 :  11880  ->  235 token_num = 29
80 :  12420  ->  110 token_num = 30
81 :  13536  ->  245 token_num = 31
82 :  14076  ->  167 token_num = 32
83 :  14076  ->  74 token_num = 32
84 :  14076  ->  190 token_num = 32
85 :  14076  ->  96 token_num = 32
86 :  17280  ->  272 token_num = 36
87 :  17280  ->  60 token_num = 36
88 :  17712  ->  250 token_num = 37
89 :  17712  ->  290 token_num = 37
90 :  18504  ->  331 token_num = 38
91 :  19224  ->  233 token_num = 39
92 :  19224  ->  199 token_num = 39
93 :  21312  ->  465 token_num = 43
94 :  21312  ->  336 token_num = 43
95 :  22140  ->  389 token_num = 45
96 :  22140  ->  196 token_num = 45
97 :  22716  ->  415 token_num = 46
98 :  22716  ->  137 token_num = 46
99 :  23472  ->  164 token_num = 49
100 :  23904  ->  559 token_num = 50
101 :  24840  ->  315 token_num = 55
102 :  25164  ->  469 token_num = 56
103 :  26244  ->  618 token_num = 63
104 :  27072  ->  502 token_num = 71
105 :  27900  ->  503 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           18.0000        4.0         22.0  ...     0.8182  0.4500  0.5806
Arg1           13.0000        6.0         19.0  ...     0.6842  0.1667  0.2680
Arg2            0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            31.0000       11.0         42.0  ...     1.5024  0.6167  0.8487
precision       0.7381        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1649        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2696        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0548        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 12 	 loss 5.590995389968157
0 :  72  ->  2 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  6 token_num = 3
3 :  180  ->  6 token_num = 3
4 :  252  ->  4 token_num = 4
5 :  252  ->  5 token_num = 4
6 :  252  ->  5 token_num = 4
7 :  252  ->  7 token_num = 4
8 :  252  ->  4 token_num = 4
9 :  252  ->  9 token_num = 4
10 :  252  ->  8 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  9 token_num = 4
13 :  252  ->  4 token_num = 4
14 :  252  ->  9 token_num = 4
15 :  252  ->  7 token_num = 4
16 :  432  ->  6 token_num = 5
17 :  432  ->  7 token_num = 5
18 :  432  ->  10 token_num = 5
19 :  432  ->  7 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  6 token_num = 5
22 :  432  ->  8 token_num = 5
23 :  432  ->  5 token_num = 5
24 :  432  ->  11 token_num = 5
25 :  540  ->  5 token_num = 6
26 :  540  ->  7 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  12 token_num = 6
29 :  540  ->  8 token_num = 6
30 :  540  ->  11 token_num = 6
31 :  540  ->  20 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  9 token_num = 7
34 :  792  ->  9 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  13 token_num = 7
37 :  792  ->  6 token_num = 7
38 :  792  ->  10 token_num = 7
39 :  792  ->  5 token_num = 7
40 :  792  ->  25 token_num = 7
41 :  792  ->  9 token_num = 7
42 :  936  ->  8 token_num = 8
43 :  936  ->  15 token_num = 8
44 :  1260  ->  9 token_num = 9
45 :  1260  ->  14 token_num = 9
46 :  1260  ->  9 token_num = 9
47 :  1440  ->  16 token_num = 10
48 :  1440  ->  13 token_num = 10
49 :  2052  ->  35 token_num = 12
50 :  2520  ->  37 token_num = 13
51 :  2520  ->  55 token_num = 13
52 :  2520  ->  22 token_num = 13
53 :  2772  ->  40 token_num = 14
54 :  3312  ->  16 token_num = 15
55 :  3600  ->  35 token_num = 16
56 :  4212  ->  28 token_num = 17
57 :  4212  ->  30 token_num = 17
58 :  4536  ->  49 token_num = 18
59 :  4536  ->  53 token_num = 18
60 :  4536  ->  29 token_num = 18
61 :  5220  ->  69 token_num = 19
62 :  5220  ->  41 token_num = 19
63 :  5220  ->  46 token_num = 19
64 :  5220  ->  82 token_num = 19
65 :  5580  ->  47 token_num = 20
66 :  6732  ->  37 token_num = 22
67 :  6732  ->  73 token_num = 22
68 :  6732  ->  72 token_num = 22
69 :  7560  ->  48 token_num = 23
70 :  7992  ->  98 token_num = 24
71 :  9360  ->  60 token_num = 26
72 :  9360  ->  159 token_num = 26
73 :  9360  ->  58 token_num = 26
74 :  10332  ->  127 token_num = 27
75 :  10836  ->  48 token_num = 28
76 :  11880  ->  80 token_num = 29
77 :  11880  ->  96 token_num = 29
78 :  11880  ->  143 token_num = 29
79 :  11880  ->  166 token_num = 29
80 :  12420  ->  103 token_num = 30
81 :  13536  ->  175 token_num = 31
82 :  14076  ->  114 token_num = 32
83 :  14076  ->  34 token_num = 32
84 :  14076  ->  183 token_num = 32
85 :  14076  ->  95 token_num = 32
86 :  17280  ->  216 token_num = 36
87 :  17280  ->  44 token_num = 36
88 :  17712  ->  190 token_num = 37
89 :  17712  ->  224 token_num = 37
90 :  18504  ->  232 token_num = 38
91 :  19224  ->  154 token_num = 39
92 :  19224  ->  192 token_num = 39
93 :  21312  ->  330 token_num = 43
94 :  21312  ->  261 token_num = 43
95 :  22140  ->  326 token_num = 45
96 :  22140  ->  150 token_num = 45
97 :  22716  ->  394 token_num = 46
98 :  22716  ->  119 token_num = 46
99 :  23472  ->  156 token_num = 49
100 :  23904  ->  372 token_num = 50
101 :  24840  ->  287 token_num = 55
102 :  25164  ->  381 token_num = 56
103 :  26244  ->  559 token_num = 63
104 :  27072  ->  357 token_num = 71
105 :  27900  ->  444 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        5.0         25.0  ...       0.80  0.5000  0.6154
Arg1           17.0000        8.0         25.0  ...       0.68  0.2179  0.3301
Arg2            1.0000        4.0          5.0  ...       0.20  0.0294  0.0513
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            38.0000       17.0         55.0  ...       1.68  0.7474  0.9968
precision       0.6909        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2021        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3128        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0643        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 13 	 loss 4.805862294975668
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  4 token_num = 3
3 :  180  ->  5 token_num = 3
4 :  252  ->  6 token_num = 4
5 :  252  ->  6 token_num = 4
6 :  252  ->  5 token_num = 4
7 :  252  ->  7 token_num = 4
8 :  252  ->  6 token_num = 4
9 :  252  ->  6 token_num = 4
10 :  252  ->  7 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  7 token_num = 4
13 :  252  ->  6 token_num = 4
14 :  252  ->  8 token_num = 4
15 :  252  ->  7 token_num = 4
16 :  432  ->  6 token_num = 5
17 :  432  ->  9 token_num = 5
18 :  432  ->  9 token_num = 5
19 :  432  ->  5 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  7 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  8 token_num = 5
25 :  540  ->  7 token_num = 6
26 :  540  ->  6 token_num = 6
27 :  540  ->  6 token_num = 6
28 :  540  ->  12 token_num = 6
29 :  540  ->  11 token_num = 6
30 :  540  ->  11 token_num = 6
31 :  540  ->  19 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  9 token_num = 7
34 :  792  ->  8 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  10 token_num = 7
37 :  792  ->  6 token_num = 7
38 :  792  ->  7 token_num = 7
39 :  792  ->  6 token_num = 7
40 :  792  ->  22 token_num = 7
41 :  792  ->  7 token_num = 7
42 :  936  ->  7 token_num = 8
43 :  936  ->  16 token_num = 8
44 :  1260  ->  9 token_num = 9
45 :  1260  ->  10 token_num = 9
46 :  1260  ->  9 token_num = 9
47 :  1440  ->  12 token_num = 10
48 :  1440  ->  12 token_num = 10
49 :  2052  ->  18 token_num = 12
50 :  2520  ->  29 token_num = 13
51 :  2520  ->  33 token_num = 13
52 :  2520  ->  23 token_num = 13
53 :  2772  ->  28 token_num = 14
54 :  3312  ->  17 token_num = 15
55 :  3600  ->  27 token_num = 16
56 :  4212  ->  18 token_num = 17
57 :  4212  ->  30 token_num = 17
58 :  4536  ->  46 token_num = 18
59 :  4536  ->  38 token_num = 18
60 :  4536  ->  32 token_num = 18
61 :  5220  ->  44 token_num = 19
62 :  5220  ->  38 token_num = 19
63 :  5220  ->  40 token_num = 19
64 :  5220  ->  64 token_num = 19
65 :  5580  ->  35 token_num = 20
66 :  6732  ->  35 token_num = 22
67 :  6732  ->  47 token_num = 22
68 :  6732  ->  68 token_num = 22
69 :  7560  ->  36 token_num = 23
70 :  7992  ->  54 token_num = 24
71 :  9360  ->  61 token_num = 26
72 :  9360  ->  106 token_num = 26
73 :  9360  ->  72 token_num = 26
74 :  10332  ->  118 token_num = 27
75 :  10836  ->  32 token_num = 28
76 :  11880  ->  68 token_num = 29
77 :  11880  ->  75 token_num = 29
78 :  11880  ->  141 token_num = 29
79 :  11880  ->  123 token_num = 29
80 :  12420  ->  76 token_num = 30
81 :  13536  ->  138 token_num = 31
82 :  14076  ->  130 token_num = 32
83 :  14076  ->  27 token_num = 32
84 :  14076  ->  125 token_num = 32
85 :  14076  ->  68 token_num = 32
86 :  17280  ->  225 token_num = 36
87 :  17280  ->  32 token_num = 36
88 :  17712  ->  129 token_num = 37
89 :  17712  ->  165 token_num = 37
90 :  18504  ->  188 token_num = 38
91 :  19224  ->  174 token_num = 39
92 :  19224  ->  189 token_num = 39
93 :  21312  ->  335 token_num = 43
94 :  21312  ->  218 token_num = 43
95 :  22140  ->  329 token_num = 45
96 :  22140  ->  148 token_num = 45
97 :  22716  ->  373 token_num = 46
98 :  22716  ->  109 token_num = 46
99 :  23472  ->  114 token_num = 49
100 :  23904  ->  324 token_num = 50
101 :  24840  ->  223 token_num = 55
102 :  25164  ->  345 token_num = 56
103 :  26244  ->  479 token_num = 63
104 :  27072  ->  419 token_num = 71
105 :  27900  ->  404 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        5.0         25.0  ...     0.8000  0.5000  0.6154
Arg1           19.0000       13.0         32.0  ...     0.5938  0.2436  0.3455
Arg2            1.0000        1.0          2.0  ...     0.5000  0.0294  0.0556
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       20.0         60.0  ...     1.8938  0.7730  1.0164
precision       0.6667        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3226        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0656        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 14 	 loss 3.7397086117998697
0 :  72  ->  1 token_num = 2
1 :  180  ->  1 token_num = 3
2 :  180  ->  4 token_num = 3
3 :  180  ->  5 token_num = 3
4 :  252  ->  4 token_num = 4
5 :  252  ->  5 token_num = 4
6 :  252  ->  3 token_num = 4
7 :  252  ->  5 token_num = 4
8 :  252  ->  4 token_num = 4
9 :  252  ->  5 token_num = 4
10 :  252  ->  8 token_num = 4
11 :  252  ->  1 token_num = 4
12 :  252  ->  6 token_num = 4
13 :  252  ->  4 token_num = 4
14 :  252  ->  10 token_num = 4
15 :  252  ->  4 token_num = 4
16 :  432  ->  5 token_num = 5
17 :  432  ->  7 token_num = 5
18 :  432  ->  8 token_num = 5
19 :  432  ->  4 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  6 token_num = 5
22 :  432  ->  6 token_num = 5
23 :  432  ->  2 token_num = 5
24 :  432  ->  7 token_num = 5
25 :  540  ->  5 token_num = 6
26 :  540  ->  3 token_num = 6
27 :  540  ->  3 token_num = 6
28 :  540  ->  11 token_num = 6
29 :  540  ->  7 token_num = 6
30 :  540  ->  11 token_num = 6
31 :  540  ->  16 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  8 token_num = 7
34 :  792  ->  7 token_num = 7
35 :  792  ->  5 token_num = 7
36 :  792  ->  7 token_num = 7
37 :  792  ->  5 token_num = 7
38 :  792  ->  6 token_num = 7
39 :  792  ->  5 token_num = 7
40 :  792  ->  16 token_num = 7
41 :  792  ->  6 token_num = 7
42 :  936  ->  7 token_num = 8
43 :  936  ->  9 token_num = 8
44 :  1260  ->  5 token_num = 9
45 :  1260  ->  6 token_num = 9
46 :  1260  ->  8 token_num = 9
47 :  1440  ->  7 token_num = 10
48 :  1440  ->  10 token_num = 10
49 :  2052  ->  16 token_num = 12
50 :  2520  ->  26 token_num = 13
51 :  2520  ->  24 token_num = 13
52 :  2520  ->  17 token_num = 13
53 :  2772  ->  29 token_num = 14
54 :  3312  ->  17 token_num = 15
55 :  3600  ->  16 token_num = 16
56 :  4212  ->  10 token_num = 17
57 :  4212  ->  17 token_num = 17
58 :  4536  ->  46 token_num = 18
59 :  4536  ->  21 token_num = 18
60 :  4536  ->  22 token_num = 18
61 :  5220  ->  29 token_num = 19
62 :  5220  ->  34 token_num = 19
63 :  5220  ->  32 token_num = 19
64 :  5220  ->  38 token_num = 19
65 :  5580  ->  28 token_num = 20
66 :  6732  ->  19 token_num = 22
67 :  6732  ->  44 token_num = 22
68 :  6732  ->  65 token_num = 22
69 :  7560  ->  30 token_num = 23
70 :  7992  ->  44 token_num = 24
71 :  9360  ->  44 token_num = 26
72 :  9360  ->  118 token_num = 26
73 :  9360  ->  70 token_num = 26
74 :  10332  ->  101 token_num = 27
75 :  10836  ->  22 token_num = 28
76 :  11880  ->  45 token_num = 29
77 :  11880  ->  60 token_num = 29
78 :  11880  ->  124 token_num = 29
79 :  11880  ->  97 token_num = 29
80 :  12420  ->  58 token_num = 30
81 :  13536  ->  134 token_num = 31
82 :  14076  ->  136 token_num = 32
83 :  14076  ->  47 token_num = 32
84 :  14076  ->  165 token_num = 32
85 :  14076  ->  61 token_num = 32
86 :  17280  ->  222 token_num = 36
87 :  17280  ->  24 token_num = 36
88 :  17712  ->  119 token_num = 37
89 :  17712  ->  144 token_num = 37
90 :  18504  ->  181 token_num = 38
91 :  19224  ->  149 token_num = 39
92 :  19224  ->  184 token_num = 39
93 :  21312  ->  247 token_num = 43
94 :  21312  ->  208 token_num = 43
95 :  22140  ->  289 token_num = 45
96 :  22140  ->  157 token_num = 45
97 :  22716  ->  386 token_num = 46
98 :  22716  ->  99 token_num = 46
99 :  23472  ->  86 token_num = 49
100 :  23904  ->  359 token_num = 50
101 :  24840  ->  243 token_num = 55
102 :  25164  ->  289 token_num = 56
103 :  26244  ->  482 token_num = 63
104 :  27072  ->  415 token_num = 71
105 :  27900  ->  421 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           20.0000        7.0         27.0  ...     0.7407  0.5000  0.5970
Arg1           19.0000       14.0         33.0  ...     0.5758  0.2436  0.3423
Arg2            1.0000        2.0          3.0  ...     0.3333  0.0294  0.0541
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            40.0000       24.0         64.0  ...     1.6498  0.7730  0.9934
precision       0.6250        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2128        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3175        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0641        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../preprocess/base/mk_dataset.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch 15 	 loss 3.2177942969137803
0 :  72  ->  3 token_num = 2
1 :  180  ->  2 token_num = 3
2 :  180  ->  6 token_num = 3
3 :  180  ->  7 token_num = 3
4 :  252  ->  6 token_num = 4
5 :  252  ->  7 token_num = 4
6 :  252  ->  6 token_num = 4
7 :  252  ->  8 token_num = 4
8 :  252  ->  6 token_num = 4
9 :  252  ->  15 token_num = 4
10 :  252  ->  8 token_num = 4
11 :  252  ->  2 token_num = 4
12 :  252  ->  9 token_num = 4
13 :  252  ->  6 token_num = 4
14 :  252  ->  13 token_num = 4
15 :  252  ->  11 token_num = 4
16 :  432  ->  7 token_num = 5
17 :  432  ->  10 token_num = 5
18 :  432  ->  15 token_num = 5
19 :  432  ->  7 token_num = 5
20 :  432  ->  2 token_num = 5
21 :  432  ->  6 token_num = 5
22 :  432  ->  10 token_num = 5
23 :  432  ->  6 token_num = 5
24 :  432  ->  13 token_num = 5
25 :  540  ->  5 token_num = 6
26 :  540  ->  5 token_num = 6
27 :  540  ->  5 token_num = 6
28 :  540  ->  14 token_num = 6
29 :  540  ->  7 token_num = 6
30 :  540  ->  16 token_num = 6
31 :  540  ->  22 token_num = 6
32 :  540  ->  3 token_num = 6
33 :  792  ->  12 token_num = 7
34 :  792  ->  8 token_num = 7
35 :  792  ->  6 token_num = 7
36 :  792  ->  15 token_num = 7
37 :  792  ->  5 token_num = 7
38 :  792  ->  9 token_num = 7
39 :  792  ->  7 token_num = 7
40 :  792  ->  22 token_num = 7
41 :  792  ->  8 token_num = 7
42 :  936  ->  9 token_num = 8
43 :  936  ->  25 token_num = 8
44 :  1260  ->  6 token_num = 9
45 :  1260  ->  12 token_num = 9
46 :  1260  ->  10 token_num = 9
47 :  1440  ->  18 token_num = 10
48 :  1440  ->  15 token_num = 10
49 :  2052  ->  32 token_num = 12
50 :  2520  ->  39 token_num = 13
51 :  2520  ->  71 token_num = 13
52 :  2520  ->  26 token_num = 13
53 :  2772  ->  39 token_num = 14
54 :  3312  ->  25 token_num = 15
55 :  3600  ->  31 token_num = 16
56 :  4212  ->  31 token_num = 17
57 :  4212  ->  40 token_num = 17
58 :  4536  ->  57 token_num = 18
59 :  4536  ->  71 token_num = 18
60 :  4536  ->  34 token_num = 18
61 :  5220  ->  67 token_num = 19
62 :  5220  ->  68 token_num = 19
63 :  5220  ->  52 token_num = 19
64 :  5220  ->  96 token_num = 19
65 :  5580  ->  53 token_num = 20
66 :  6732  ->  44 token_num = 22
67 :  6732  ->  75 token_num = 22
68 :  6732  ->  69 token_num = 22
69 :  7560  ->  48 token_num = 23
70 :  7992  ->  84 token_num = 24
71 :  9360  ->  76 token_num = 26
72 :  9360  ->  188 token_num = 26
73 :  9360  ->  79 token_num = 26
74 :  10332  ->  150 token_num = 27
75 :  10836  ->  64 token_num = 28
76 :  11880  ->  99 token_num = 29
77 :  11880  ->  100 token_num = 29
78 :  11880  ->  161 token_num = 29
79 :  11880  ->  175 token_num = 29
80 :  12420  ->  118 token_num = 30
81 :  13536  ->  220 token_num = 31
82 :  14076  ->  132 token_num = 32
83 :  14076  ->  81 token_num = 32
84 :  14076  ->  162 token_num = 32
85 :  14076  ->  104 token_num = 32
86 :  17280  ->  319 token_num = 36
87 :  17280  ->  45 token_num = 36
88 :  17712  ->  217 token_num = 37
89 :  17712  ->  319 token_num = 37
90 :  18504  ->  321 token_num = 38
91 :  19224  ->  205 token_num = 39
92 :  19224  ->  206 token_num = 39
93 :  21312  ->  446 token_num = 43
94 :  21312  ->  296 token_num = 43
95 :  22140  ->  410 token_num = 45
96 :  22140  ->  191 token_num = 45
97 :  22716  ->  393 token_num = 46
98 :  22716  ->  129 token_num = 46
99 :  23472  ->  160 token_num = 49
100 :  23904  ->  468 token_num = 50
101 :  24840  ->  343 token_num = 55
102 :  25164  ->  436 token_num = 56
103 :  26244  ->  688 token_num = 63
104 :  27072  ->  462 token_num = 71
105 :  27900  ->  493 token_num = 75
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           21.0000        7.0         28.0  ...     0.7500  0.5250  0.6176
Arg1           17.0000       12.0         29.0  ...     0.5862  0.2179  0.3178
Arg2            1.0000        5.0          6.0  ...     0.1667  0.0294  0.0500
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            39.0000       24.0         63.0  ...     1.5029  0.7724  0.9854
precision       0.6190        NaN          NaN  ...        NaN     NaN     NaN
recall          0.2074        NaN          NaN  ...        NaN     NaN     NaN
f1              0.3108        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0636        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Stop. No change in valid f1


 0:51:47.044539 

0 :  252  ->  5 token_num = 4
1 :  252  ->  8 token_num = 4
2 :  252  ->  4 token_num = 4
3 :  252  ->  13 token_num = 4
4 :  252  ->  3 token_num = 4
5 :  252  ->  7 token_num = 4
6 :  252  ->  5 token_num = 4
7 :  252  ->  3 token_num = 4
8 :  252  ->  7 token_num = 4
9 :  432  ->  4 token_num = 5
10 :  432  ->  11 token_num = 5
11 :  432  ->  5 token_num = 5
12 :  432  ->  5 token_num = 5
13 :  432  ->  7 token_num = 5
14 :  432  ->  6 token_num = 5
15 :  432  ->  9 token_num = 5
16 :  432  ->  5 token_num = 5
17 :  432  ->  10 token_num = 5
18 :  432  ->  4 token_num = 5
19 :  432  ->  3 token_num = 5
20 :  432  ->  5 token_num = 5
21 :  540  ->  8 token_num = 6
22 :  540  ->  8 token_num = 6
23 :  540  ->  11 token_num = 6
24 :  540  ->  6 token_num = 6
25 :  540  ->  5 token_num = 6
26 :  540  ->  13 token_num = 6
27 :  540  ->  4 token_num = 6
28 :  540  ->  5 token_num = 6
29 :  540  ->  3 token_num = 6
30 :  540  ->  7 token_num = 6
31 :  540  ->  10 token_num = 6
32 :  792  ->  5 token_num = 7
33 :  792  ->  11 token_num = 7
34 :  792  ->  7 token_num = 7
35 :  1260  ->  14 token_num = 9
36 :  1260  ->  25 token_num = 9
37 :  1440  ->  11 token_num = 10
38 :  1836  ->  18 token_num = 11
39 :  1836  ->  13 token_num = 11
40 :  2052  ->  32 token_num = 12
41 :  2520  ->  22 token_num = 13
42 :  2520  ->  22 token_num = 13
43 :  2520  ->  22 token_num = 13
44 :  3312  ->  39 token_num = 15
45 :  3312  ->  37 token_num = 15
46 :  3312  ->  36 token_num = 15
47 :  3312  ->  24 token_num = 15
48 :  3312  ->  21 token_num = 15
49 :  3600  ->  38 token_num = 16
50 :  3600  ->  27 token_num = 16
51 :  4212  ->  14 token_num = 17
52 :  4536  ->  56 token_num = 18
53 :  4536  ->  78 token_num = 18
54 :  4536  ->  40 token_num = 18
55 :  5220  ->  51 token_num = 19
56 :  5220  ->  31 token_num = 19
57 :  5220  ->  89 token_num = 19
58 :  5220  ->  49 token_num = 19
59 :  5580  ->  35 token_num = 20
60 :  5580  ->  41 token_num = 20
61 :  6336  ->  27 token_num = 21
62 :  6336  ->  56 token_num = 21
63 :  6732  ->  31 token_num = 22
64 :  6732  ->  56 token_num = 22
65 :  6732  ->  40 token_num = 22
66 :  6732  ->  25 token_num = 22
67 :  6732  ->  43 token_num = 22
68 :  7560  ->  49 token_num = 23
69 :  7560  ->  66 token_num = 23
70 :  7992  ->  60 token_num = 24
71 :  7992  ->  70 token_num = 24
72 :  8892  ->  100 token_num = 25
73 :  8892  ->  92 token_num = 25
74 :  8892  ->  121 token_num = 25
75 :  9360  ->  54 token_num = 26
76 :  9360  ->  75 token_num = 26
77 :  10332  ->  50 token_num = 27
78 :  10332  ->  73 token_num = 27
79 :  10836  ->  77 token_num = 28
80 :  10836  ->  40 token_num = 28
81 :  10836  ->  123 token_num = 28
82 :  12420  ->  84 token_num = 30
83 :  12420  ->  114 token_num = 30
84 :  13536  ->  114 token_num = 31
85 :  13536  ->  153 token_num = 31
86 :  13536  ->  108 token_num = 31
87 :  14076  ->  63 token_num = 32
88 :  15084  ->  167 token_num = 33
89 :  15696  ->  101 token_num = 34
90 :  17280  ->  76 token_num = 36
91 :  17712  ->  76 token_num = 37
92 :  18504  ->  48 token_num = 38
93 :  19224  ->  113 token_num = 39
94 :  19584  ->  156 token_num = 40
95 :  20196  ->  452 token_num = 41
96 :  21312  ->  282 token_num = 43
97 :  21564  ->  107 token_num = 44
98 :  21564  ->  118 token_num = 44
99 :  23220  ->  275 token_num = 48
100 :  25488  ->  223 token_num = 57
101 :  25668  ->  589 token_num = 61
102 :  26784  ->  369 token_num = 67
103 :  27576  ->  601 token_num = 73
104 :  29016  ->  632 token_num = 92
(0.2909,            correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           18.0000        3.0         21.0  ...     0.8571  0.4737  0.6102
Arg1           14.0000        3.0         17.0  ...     0.8235  0.1772  0.2917
Arg2            0.0000        2.0          2.0  ...     0.0000  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            32.0000        8.0         40.0  ...     1.6807  0.6509  0.9018
precision       0.8000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1778        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2909        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0582        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns])
