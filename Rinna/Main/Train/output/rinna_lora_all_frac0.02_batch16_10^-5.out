Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 206, MAX_LENGTH = 192, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
840
base_model.model.rinna.embed_in.weight False
base_model.model.rinna.layers.0.input_layernorm.weight False
base_model.model.rinna.layers.0.input_layernorm.bias False
base_model.model.rinna.layers.0.post_attention_layernorm.weight False
base_model.model.rinna.layers.0.post_attention_layernorm.bias False
base_model.model.rinna.layers.0.attention.query_key_value.weight False
base_model.model.rinna.layers.0.attention.query_key_value.bias False
base_model.model.rinna.layers.0.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.0.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.0.attention.dense.weight False
base_model.model.rinna.layers.0.attention.dense.bias False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.1.input_layernorm.weight False
base_model.model.rinna.layers.1.input_layernorm.bias False
base_model.model.rinna.layers.1.post_attention_layernorm.weight False
base_model.model.rinna.layers.1.post_attention_layernorm.bias False
base_model.model.rinna.layers.1.attention.query_key_value.weight False
base_model.model.rinna.layers.1.attention.query_key_value.bias False
base_model.model.rinna.layers.1.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.1.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.1.attention.dense.weight False
base_model.model.rinna.layers.1.attention.dense.bias False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.2.input_layernorm.weight False
base_model.model.rinna.layers.2.input_layernorm.bias False
base_model.model.rinna.layers.2.post_attention_layernorm.weight False
base_model.model.rinna.layers.2.post_attention_layernorm.bias False
base_model.model.rinna.layers.2.attention.query_key_value.weight False
base_model.model.rinna.layers.2.attention.query_key_value.bias False
base_model.model.rinna.layers.2.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.2.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.2.attention.dense.weight False
base_model.model.rinna.layers.2.attention.dense.bias False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.3.input_layernorm.weight False
base_model.model.rinna.layers.3.input_layernorm.bias False
base_model.model.rinna.layers.3.post_attention_layernorm.weight False
base_model.model.rinna.layers.3.post_attention_layernorm.bias False
base_model.model.rinna.layers.3.attention.query_key_value.weight False
base_model.model.rinna.layers.3.attention.query_key_value.bias False
base_model.model.rinna.layers.3.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.3.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.3.attention.dense.weight False
base_model.model.rinna.layers.3.attention.dense.bias False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.4.input_layernorm.weight False
base_model.model.rinna.layers.4.input_layernorm.bias False
base_model.model.rinna.layers.4.post_attention_layernorm.weight False
base_model.model.rinna.layers.4.post_attention_layernorm.bias False
base_model.model.rinna.layers.4.attention.query_key_value.weight False
base_model.model.rinna.layers.4.attention.query_key_value.bias False
base_model.model.rinna.layers.4.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.4.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.4.attention.dense.weight False
base_model.model.rinna.layers.4.attention.dense.bias False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.5.input_layernorm.weight False
base_model.model.rinna.layers.5.input_layernorm.bias False
base_model.model.rinna.layers.5.post_attention_layernorm.weight False
base_model.model.rinna.layers.5.post_attention_layernorm.bias False
base_model.model.rinna.layers.5.attention.query_key_value.weight False
base_model.model.rinna.layers.5.attention.query_key_value.bias False
base_model.model.rinna.layers.5.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.5.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.5.attention.dense.weight False
base_model.model.rinna.layers.5.attention.dense.bias False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.6.input_layernorm.weight False
base_model.model.rinna.layers.6.input_layernorm.bias False
base_model.model.rinna.layers.6.post_attention_layernorm.weight False
base_model.model.rinna.layers.6.post_attention_layernorm.bias False
base_model.model.rinna.layers.6.attention.query_key_value.weight False
base_model.model.rinna.layers.6.attention.query_key_value.bias False
base_model.model.rinna.layers.6.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.6.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.6.attention.dense.weight False
base_model.model.rinna.layers.6.attention.dense.bias False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.7.input_layernorm.weight False
base_model.model.rinna.layers.7.input_layernorm.bias False
base_model.model.rinna.layers.7.post_attention_layernorm.weight False
base_model.model.rinna.layers.7.post_attention_layernorm.bias False
base_model.model.rinna.layers.7.attention.query_key_value.weight False
base_model.model.rinna.layers.7.attention.query_key_value.bias False
base_model.model.rinna.layers.7.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.7.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.7.attention.dense.weight False
base_model.model.rinna.layers.7.attention.dense.bias False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.8.input_layernorm.weight False
base_model.model.rinna.layers.8.input_layernorm.bias False
base_model.model.rinna.layers.8.post_attention_layernorm.weight False
base_model.model.rinna.layers.8.post_attention_layernorm.bias False
base_model.model.rinna.layers.8.attention.query_key_value.weight False
base_model.model.rinna.layers.8.attention.query_key_value.bias False
base_model.model.rinna.layers.8.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.8.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.8.attention.dense.weight False
base_model.model.rinna.layers.8.attention.dense.bias False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.9.input_layernorm.weight False
base_model.model.rinna.layers.9.input_layernorm.bias False
base_model.model.rinna.layers.9.post_attention_layernorm.weight False
base_model.model.rinna.layers.9.post_attention_layernorm.bias False
base_model.model.rinna.layers.9.attention.query_key_value.weight False
base_model.model.rinna.layers.9.attention.query_key_value.bias False
base_model.model.rinna.layers.9.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.9.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.9.attention.dense.weight False
base_model.model.rinna.layers.9.attention.dense.bias False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.10.input_layernorm.weight False
base_model.model.rinna.layers.10.input_layernorm.bias False
base_model.model.rinna.layers.10.post_attention_layernorm.weight False
base_model.model.rinna.layers.10.post_attention_layernorm.bias False
base_model.model.rinna.layers.10.attention.query_key_value.weight False
base_model.model.rinna.layers.10.attention.query_key_value.bias False
base_model.model.rinna.layers.10.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.10.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.10.attention.dense.weight False
base_model.model.rinna.layers.10.attention.dense.bias False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.11.input_layernorm.weight False
base_model.model.rinna.layers.11.input_layernorm.bias False
base_model.model.rinna.layers.11.post_attention_layernorm.weight False
base_model.model.rinna.layers.11.post_attention_layernorm.bias False
base_model.model.rinna.layers.11.attention.query_key_value.weight False
base_model.model.rinna.layers.11.attention.query_key_value.bias False
base_model.model.rinna.layers.11.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.11.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.11.attention.dense.weight False
base_model.model.rinna.layers.11.attention.dense.bias False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.12.input_layernorm.weight False
base_model.model.rinna.layers.12.input_layernorm.bias False
base_model.model.rinna.layers.12.post_attention_layernorm.weight False
base_model.model.rinna.layers.12.post_attention_layernorm.bias False
base_model.model.rinna.layers.12.attention.query_key_value.weight False
base_model.model.rinna.layers.12.attention.query_key_value.bias False
base_model.model.rinna.layers.12.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.12.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.12.attention.dense.weight False
base_model.model.rinna.layers.12.attention.dense.bias False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.13.input_layernorm.weight False
base_model.model.rinna.layers.13.input_layernorm.bias False
base_model.model.rinna.layers.13.post_attention_layernorm.weight False
base_model.model.rinna.layers.13.post_attention_layernorm.bias False
base_model.model.rinna.layers.13.attention.query_key_value.weight False
base_model.model.rinna.layers.13.attention.query_key_value.bias False
base_model.model.rinna.layers.13.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.13.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.13.attention.dense.weight False
base_model.model.rinna.layers.13.attention.dense.bias False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.14.input_layernorm.weight False
base_model.model.rinna.layers.14.input_layernorm.bias False
base_model.model.rinna.layers.14.post_attention_layernorm.weight False
base_model.model.rinna.layers.14.post_attention_layernorm.bias False
base_model.model.rinna.layers.14.attention.query_key_value.weight False
base_model.model.rinna.layers.14.attention.query_key_value.bias False
base_model.model.rinna.layers.14.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.14.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.14.attention.dense.weight False
base_model.model.rinna.layers.14.attention.dense.bias False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.15.input_layernorm.weight False
base_model.model.rinna.layers.15.input_layernorm.bias False
base_model.model.rinna.layers.15.post_attention_layernorm.weight False
base_model.model.rinna.layers.15.post_attention_layernorm.bias False
base_model.model.rinna.layers.15.attention.query_key_value.weight False
base_model.model.rinna.layers.15.attention.query_key_value.bias False
base_model.model.rinna.layers.15.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.15.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.15.attention.dense.weight False
base_model.model.rinna.layers.15.attention.dense.bias False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.16.input_layernorm.weight False
base_model.model.rinna.layers.16.input_layernorm.bias False
base_model.model.rinna.layers.16.post_attention_layernorm.weight False
base_model.model.rinna.layers.16.post_attention_layernorm.bias False
base_model.model.rinna.layers.16.attention.query_key_value.weight False
base_model.model.rinna.layers.16.attention.query_key_value.bias False
base_model.model.rinna.layers.16.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.16.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.16.attention.dense.weight False
base_model.model.rinna.layers.16.attention.dense.bias False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.17.input_layernorm.weight False
base_model.model.rinna.layers.17.input_layernorm.bias False
base_model.model.rinna.layers.17.post_attention_layernorm.weight False
base_model.model.rinna.layers.17.post_attention_layernorm.bias False
base_model.model.rinna.layers.17.attention.query_key_value.weight False
base_model.model.rinna.layers.17.attention.query_key_value.bias False
base_model.model.rinna.layers.17.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.17.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.17.attention.dense.weight False
base_model.model.rinna.layers.17.attention.dense.bias False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.18.input_layernorm.weight False
base_model.model.rinna.layers.18.input_layernorm.bias False
base_model.model.rinna.layers.18.post_attention_layernorm.weight False
base_model.model.rinna.layers.18.post_attention_layernorm.bias False
base_model.model.rinna.layers.18.attention.query_key_value.weight False
base_model.model.rinna.layers.18.attention.query_key_value.bias False
base_model.model.rinna.layers.18.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.18.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.18.attention.dense.weight False
base_model.model.rinna.layers.18.attention.dense.bias False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.19.input_layernorm.weight False
base_model.model.rinna.layers.19.input_layernorm.bias False
base_model.model.rinna.layers.19.post_attention_layernorm.weight False
base_model.model.rinna.layers.19.post_attention_layernorm.bias False
base_model.model.rinna.layers.19.attention.query_key_value.weight False
base_model.model.rinna.layers.19.attention.query_key_value.bias False
base_model.model.rinna.layers.19.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.19.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.19.attention.dense.weight False
base_model.model.rinna.layers.19.attention.dense.bias False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.20.input_layernorm.weight False
base_model.model.rinna.layers.20.input_layernorm.bias False
base_model.model.rinna.layers.20.post_attention_layernorm.weight False
base_model.model.rinna.layers.20.post_attention_layernorm.bias False
base_model.model.rinna.layers.20.attention.query_key_value.weight False
base_model.model.rinna.layers.20.attention.query_key_value.bias False
base_model.model.rinna.layers.20.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.20.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.20.attention.dense.weight False
base_model.model.rinna.layers.20.attention.dense.bias False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.21.input_layernorm.weight False
base_model.model.rinna.layers.21.input_layernorm.bias False
base_model.model.rinna.layers.21.post_attention_layernorm.weight False
base_model.model.rinna.layers.21.post_attention_layernorm.bias False
base_model.model.rinna.layers.21.attention.query_key_value.weight False
base_model.model.rinna.layers.21.attention.query_key_value.bias False
base_model.model.rinna.layers.21.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.21.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.21.attention.dense.weight False
base_model.model.rinna.layers.21.attention.dense.bias False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.22.input_layernorm.weight False
base_model.model.rinna.layers.22.input_layernorm.bias False
base_model.model.rinna.layers.22.post_attention_layernorm.weight False
base_model.model.rinna.layers.22.post_attention_layernorm.bias False
base_model.model.rinna.layers.22.attention.query_key_value.weight False
base_model.model.rinna.layers.22.attention.query_key_value.bias False
base_model.model.rinna.layers.22.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.22.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.22.attention.dense.weight False
base_model.model.rinna.layers.22.attention.dense.bias False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.23.input_layernorm.weight False
base_model.model.rinna.layers.23.input_layernorm.bias False
base_model.model.rinna.layers.23.post_attention_layernorm.weight False
base_model.model.rinna.layers.23.post_attention_layernorm.bias False
base_model.model.rinna.layers.23.attention.query_key_value.weight False
base_model.model.rinna.layers.23.attention.query_key_value.bias False
base_model.model.rinna.layers.23.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.23.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.23.attention.dense.weight False
base_model.model.rinna.layers.23.attention.dense.bias False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.24.input_layernorm.weight False
base_model.model.rinna.layers.24.input_layernorm.bias False
base_model.model.rinna.layers.24.post_attention_layernorm.weight False
base_model.model.rinna.layers.24.post_attention_layernorm.bias False
base_model.model.rinna.layers.24.attention.query_key_value.weight False
base_model.model.rinna.layers.24.attention.query_key_value.bias False
base_model.model.rinna.layers.24.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.24.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.24.attention.dense.weight False
base_model.model.rinna.layers.24.attention.dense.bias False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.25.input_layernorm.weight False
base_model.model.rinna.layers.25.input_layernorm.bias False
base_model.model.rinna.layers.25.post_attention_layernorm.weight False
base_model.model.rinna.layers.25.post_attention_layernorm.bias False
base_model.model.rinna.layers.25.attention.query_key_value.weight False
base_model.model.rinna.layers.25.attention.query_key_value.bias False
base_model.model.rinna.layers.25.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.25.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.25.attention.dense.weight False
base_model.model.rinna.layers.25.attention.dense.bias False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.26.input_layernorm.weight False
base_model.model.rinna.layers.26.input_layernorm.bias False
base_model.model.rinna.layers.26.post_attention_layernorm.weight False
base_model.model.rinna.layers.26.post_attention_layernorm.bias False
base_model.model.rinna.layers.26.attention.query_key_value.weight False
base_model.model.rinna.layers.26.attention.query_key_value.bias False
base_model.model.rinna.layers.26.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.26.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.26.attention.dense.weight False
base_model.model.rinna.layers.26.attention.dense.bias False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.27.input_layernorm.weight False
base_model.model.rinna.layers.27.input_layernorm.bias False
base_model.model.rinna.layers.27.post_attention_layernorm.weight False
base_model.model.rinna.layers.27.post_attention_layernorm.bias False
base_model.model.rinna.layers.27.attention.query_key_value.weight False
base_model.model.rinna.layers.27.attention.query_key_value.bias False
base_model.model.rinna.layers.27.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.27.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.27.attention.dense.weight False
base_model.model.rinna.layers.27.attention.dense.bias False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.28.input_layernorm.weight False
base_model.model.rinna.layers.28.input_layernorm.bias False
base_model.model.rinna.layers.28.post_attention_layernorm.weight False
base_model.model.rinna.layers.28.post_attention_layernorm.bias False
base_model.model.rinna.layers.28.attention.query_key_value.weight False
base_model.model.rinna.layers.28.attention.query_key_value.bias False
base_model.model.rinna.layers.28.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.28.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.28.attention.dense.weight False
base_model.model.rinna.layers.28.attention.dense.bias False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.29.input_layernorm.weight False
base_model.model.rinna.layers.29.input_layernorm.bias False
base_model.model.rinna.layers.29.post_attention_layernorm.weight False
base_model.model.rinna.layers.29.post_attention_layernorm.bias False
base_model.model.rinna.layers.29.attention.query_key_value.weight False
base_model.model.rinna.layers.29.attention.query_key_value.bias False
base_model.model.rinna.layers.29.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.29.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.29.attention.dense.weight False
base_model.model.rinna.layers.29.attention.dense.bias False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.30.input_layernorm.weight False
base_model.model.rinna.layers.30.input_layernorm.bias False
base_model.model.rinna.layers.30.post_attention_layernorm.weight False
base_model.model.rinna.layers.30.post_attention_layernorm.bias False
base_model.model.rinna.layers.30.attention.query_key_value.weight False
base_model.model.rinna.layers.30.attention.query_key_value.bias False
base_model.model.rinna.layers.30.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.30.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.30.attention.dense.weight False
base_model.model.rinna.layers.30.attention.dense.bias False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.31.input_layernorm.weight False
base_model.model.rinna.layers.31.input_layernorm.bias False
base_model.model.rinna.layers.31.post_attention_layernorm.weight False
base_model.model.rinna.layers.31.post_attention_layernorm.bias False
base_model.model.rinna.layers.31.attention.query_key_value.weight False
base_model.model.rinna.layers.31.attention.query_key_value.bias False
base_model.model.rinna.layers.31.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.31.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.31.attention.dense.weight False
base_model.model.rinna.layers.31.attention.dense.bias False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.32.input_layernorm.weight False
base_model.model.rinna.layers.32.input_layernorm.bias False
base_model.model.rinna.layers.32.post_attention_layernorm.weight False
base_model.model.rinna.layers.32.post_attention_layernorm.bias False
base_model.model.rinna.layers.32.attention.query_key_value.weight False
base_model.model.rinna.layers.32.attention.query_key_value.bias False
base_model.model.rinna.layers.32.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.32.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.32.attention.dense.weight False
base_model.model.rinna.layers.32.attention.dense.bias False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.33.input_layernorm.weight False
base_model.model.rinna.layers.33.input_layernorm.bias False
base_model.model.rinna.layers.33.post_attention_layernorm.weight False
base_model.model.rinna.layers.33.post_attention_layernorm.bias False
base_model.model.rinna.layers.33.attention.query_key_value.weight False
base_model.model.rinna.layers.33.attention.query_key_value.bias False
base_model.model.rinna.layers.33.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.33.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.33.attention.dense.weight False
base_model.model.rinna.layers.33.attention.dense.bias False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.34.input_layernorm.weight False
base_model.model.rinna.layers.34.input_layernorm.bias False
base_model.model.rinna.layers.34.post_attention_layernorm.weight False
base_model.model.rinna.layers.34.post_attention_layernorm.bias False
base_model.model.rinna.layers.34.attention.query_key_value.weight False
base_model.model.rinna.layers.34.attention.query_key_value.bias False
base_model.model.rinna.layers.34.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.34.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.34.attention.dense.weight False
base_model.model.rinna.layers.34.attention.dense.bias False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.35.input_layernorm.weight False
base_model.model.rinna.layers.35.input_layernorm.bias False
base_model.model.rinna.layers.35.post_attention_layernorm.weight False
base_model.model.rinna.layers.35.post_attention_layernorm.bias False
base_model.model.rinna.layers.35.attention.query_key_value.weight False
base_model.model.rinna.layers.35.attention.query_key_value.bias False
base_model.model.rinna.layers.35.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.35.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.35.attention.dense.weight False
base_model.model.rinna.layers.35.attention.dense.bias False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.bias False
base_model.model.rinna.final_layer_norm.weight False
base_model.model.rinna.final_layer_norm.bias False
base_model.model.my_linear.original_module.weight True
base_model.model.my_linear.original_module.bias True
base_model.model.my_linear.modules_to_save.default.weight True
base_model.model.my_linear.modules_to_save.default.bias True
base_model.model.my_linear2.original_module.weight True
base_model.model.my_linear2.original_module.bias True
base_model.model.my_linear2.modules_to_save.default.weight True
base_model.model.my_linear2.modules_to_save.default.bias True
Progress 0 / 832
epoch 0 	 loss 60.43006730079651
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 1 	 loss 54.24684911966324
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 2 	 loss 46.12636870145798
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            1.0000        1.0          2.0  ...     0.5000  0.0250  0.0476
Arg1            4.0000        7.0         11.0  ...     0.3636  0.0513  0.0899
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum             5.0000        8.0         13.0  ...     0.8636  0.0763  0.1375
precision       0.3846        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0266        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0498        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0089        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.0498 

Progress 0 / 832
epoch 3 	 loss 36.46507728099823
           correct_num  wrong_num  predict_num  ...  precision  recall    f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN   NaN
Arg0            9.0000        1.0         10.0  ...        0.9   0.225  0.36
Arg1            0.0000        0.0          0.0  ...        NaN   0.000   NaN
Arg2            0.0000        0.0          0.0  ...        NaN   0.000   NaN
Arg3            0.0000        0.0          0.0  ...        NaN   0.000   NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN   NaN
Arg5            0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgA            0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN   0.000   NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN   NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN   0.000   NaN
sum             9.0000        1.0         10.0  ...        0.9   0.225  0.36
precision       0.9000        NaN          NaN  ...        NaN     NaN   NaN
recall          0.0479        NaN          NaN  ...        NaN     NaN   NaN
f1              0.0909        NaN          NaN  ...        NaN     NaN   NaN
f1_macro        0.0232        NaN          NaN  ...        NaN     NaN   NaN

[36 rows x 8 columns]
Valid f1 =  0.0909 

Progress 0 / 832
epoch 4 	 loss 34.33006493747234
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            3.0000        0.0          3.0  ...        1.0  0.0750  0.1395
Arg1            1.0000        0.0          1.0  ...        1.0  0.0128  0.0253
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum             4.0000        0.0          4.0  ...        2.0  0.0878  0.1649
precision       1.0000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0213        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0417        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0106        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 5 	 loss 27.56501054763794
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0            7.0000        2.0          9.0  ...     0.7778  0.1750  0.2857
Arg1            4.0000        4.0          8.0  ...     0.5000  0.0513  0.0930
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            11.0000        6.0         17.0  ...     1.2778  0.2263  0.3787
precision       0.6471        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0585        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1073        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0244        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1073 

Progress 0 / 832
epoch 6 	 loss 21.897595316171646
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           19.0000        4.0         23.0  ...     0.8261  0.4750  0.6032
Arg1           12.0000        4.0         16.0  ...     0.7500  0.1538  0.2553
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            31.0000        8.0         39.0  ...     1.5761  0.6288  0.8585
precision       0.7949        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1649        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2731        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0554        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.2731 

Progress 0 / 832
epoch 7 	 loss 19.94112602621317
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           13.0000        2.0         15.0  ...     0.8667  0.3250  0.4727
Arg1           12.0000        3.0         15.0  ...     0.8000  0.1538  0.2581
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            25.0000        5.0         30.0  ...     1.6667  0.4788  0.7308
precision       0.8333        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1330        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2294        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0471        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 8 	 loss 17.569016240537167
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           10.0000        3.0         13.0  ...     0.7692  0.2500  0.3774
Arg1            5.0000        1.0          6.0  ...     0.8333  0.0641  0.1190
Arg2            0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            15.0000        5.0         20.0  ...     1.6026  0.3141  0.4964
precision       0.7500        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0798        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1442        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0320        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 9 	 loss 13.882200442254543
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           17.0000        4.0         21.0  ...     0.8095   0.425  0.5574
Arg1           11.0000        4.0         15.0  ...     0.7333   0.141  0.2366
Arg2            0.0000        1.0          1.0  ...     0.0000   0.000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
sum            28.0000        9.0         37.0  ...     1.5429   0.566  0.7939
precision       0.7568        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1489        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2489        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0512        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
epoch 10 	 loss 10.802897427231073
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           17.0000        3.0         20.0  ...     0.8500   0.425  0.5667
Arg1           11.0000        4.0         15.0  ...     0.7333   0.141  0.2366
Arg2            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN   0.000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN   0.000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN   0.000     NaN
sum            28.0000        7.0         35.0  ...     1.5833   0.566  0.8032
precision       0.8000        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1489        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2511        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0518        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 832
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../preprocess/base/mk_dataset.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
/media/takeuchi/HDPH-UT/callum/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch 11 	 loss 7.995130703784525
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           19.0000        6.0         25.0  ...       0.76  0.4750  0.5846
Arg1            9.0000        3.0         12.0  ...       0.75  0.1154  0.2000
Arg2            0.0000        2.0          2.0  ...       0.00  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            28.0000       11.0         39.0  ...       1.51  0.5904  0.7846
precision       0.7179        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1489        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2467        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0506        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Stop. No change in valid f1


 0:38:37.531042 

(0.2629,            correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0           16.0000        1.0         17.0  ...     0.9412  0.4211  0.5818
Arg1           12.0000        4.0         16.0  ...     0.7500  0.1519  0.2526
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum            28.0000        5.0         33.0  ...     1.6912  0.5730  0.8344
precision       0.8485        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1556        NaN          NaN  ...        NaN     NaN     NaN
f1              0.2629        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0538        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns])
