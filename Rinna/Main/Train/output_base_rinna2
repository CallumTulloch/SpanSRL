Some weights of the model checkpoint at rinna/japanese-gpt-neox-3.6b were not used when initializing GPTNeoXModel: ['embed_out.weight']
- This IS expected if you are initializing GPTNeoXModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoXModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'Arg': 0, 'Arg0': 1, 'Arg1': 2, 'Arg2': 3, 'Arg3': 4, 'Arg4': 5, 'Arg5': 6, 'ArgA': 7, 'ArgM': 8, 'ArgM2': 9, 'ArgM_ADV': 10, 'ArgM_AND': 11, 'ArgM_BUT': 12, 'ArgM_CAU': 13, 'ArgM_CMP': 14, 'ArgM_CND': 15, 'ArgM_CRT': 16, 'ArgM_DIR': 17, 'ArgM_EXT': 18, 'ArgM_GOL': 19, 'ArgM_LOC': 20, 'ArgM_MDF': 21, 'ArgM_MNR': 22, 'ArgM_MNS': 23, 'ArgM_NEG': 24, 'ArgM_PRP': 25, 'ArgM_PRX': 26, 'ArgM_REC': 27, 'ArgM_SCP': 28, 'ArgM_SPK': 29, 'ArgM_TMP': 30, 'F-A': 31, 'F-P': 32, 'V': 33, 'O': 34, 'N': 35} 

MAX_TOKEN = 268, MAX_LENGTH = 254, MAX_ARGUMENT_SEQUENCE_LENGTH = 30


No dependency data
42022
base_model.model.rinna.embed_in.weight False
base_model.model.rinna.layers.0.input_layernorm.weight False
base_model.model.rinna.layers.0.input_layernorm.bias False
base_model.model.rinna.layers.0.post_attention_layernorm.weight False
base_model.model.rinna.layers.0.post_attention_layernorm.bias False
base_model.model.rinna.layers.0.attention.query_key_value.weight False
base_model.model.rinna.layers.0.attention.query_key_value.bias False
base_model.model.rinna.layers.0.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.0.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.0.attention.dense.weight False
base_model.model.rinna.layers.0.attention.dense.bias False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.0.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.0.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.1.input_layernorm.weight False
base_model.model.rinna.layers.1.input_layernorm.bias False
base_model.model.rinna.layers.1.post_attention_layernorm.weight False
base_model.model.rinna.layers.1.post_attention_layernorm.bias False
base_model.model.rinna.layers.1.attention.query_key_value.weight False
base_model.model.rinna.layers.1.attention.query_key_value.bias False
base_model.model.rinna.layers.1.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.1.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.1.attention.dense.weight False
base_model.model.rinna.layers.1.attention.dense.bias False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.1.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.1.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.2.input_layernorm.weight False
base_model.model.rinna.layers.2.input_layernorm.bias False
base_model.model.rinna.layers.2.post_attention_layernorm.weight False
base_model.model.rinna.layers.2.post_attention_layernorm.bias False
base_model.model.rinna.layers.2.attention.query_key_value.weight False
base_model.model.rinna.layers.2.attention.query_key_value.bias False
base_model.model.rinna.layers.2.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.2.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.2.attention.dense.weight False
base_model.model.rinna.layers.2.attention.dense.bias False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.2.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.2.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.3.input_layernorm.weight False
base_model.model.rinna.layers.3.input_layernorm.bias False
base_model.model.rinna.layers.3.post_attention_layernorm.weight False
base_model.model.rinna.layers.3.post_attention_layernorm.bias False
base_model.model.rinna.layers.3.attention.query_key_value.weight False
base_model.model.rinna.layers.3.attention.query_key_value.bias False
base_model.model.rinna.layers.3.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.3.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.3.attention.dense.weight False
base_model.model.rinna.layers.3.attention.dense.bias False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.3.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.3.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.4.input_layernorm.weight False
base_model.model.rinna.layers.4.input_layernorm.bias False
base_model.model.rinna.layers.4.post_attention_layernorm.weight False
base_model.model.rinna.layers.4.post_attention_layernorm.bias False
base_model.model.rinna.layers.4.attention.query_key_value.weight False
base_model.model.rinna.layers.4.attention.query_key_value.bias False
base_model.model.rinna.layers.4.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.4.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.4.attention.dense.weight False
base_model.model.rinna.layers.4.attention.dense.bias False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.4.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.4.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.5.input_layernorm.weight False
base_model.model.rinna.layers.5.input_layernorm.bias False
base_model.model.rinna.layers.5.post_attention_layernorm.weight False
base_model.model.rinna.layers.5.post_attention_layernorm.bias False
base_model.model.rinna.layers.5.attention.query_key_value.weight False
base_model.model.rinna.layers.5.attention.query_key_value.bias False
base_model.model.rinna.layers.5.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.5.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.5.attention.dense.weight False
base_model.model.rinna.layers.5.attention.dense.bias False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.5.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.5.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.6.input_layernorm.weight False
base_model.model.rinna.layers.6.input_layernorm.bias False
base_model.model.rinna.layers.6.post_attention_layernorm.weight False
base_model.model.rinna.layers.6.post_attention_layernorm.bias False
base_model.model.rinna.layers.6.attention.query_key_value.weight False
base_model.model.rinna.layers.6.attention.query_key_value.bias False
base_model.model.rinna.layers.6.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.6.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.6.attention.dense.weight False
base_model.model.rinna.layers.6.attention.dense.bias False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.6.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.6.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.7.input_layernorm.weight False
base_model.model.rinna.layers.7.input_layernorm.bias False
base_model.model.rinna.layers.7.post_attention_layernorm.weight False
base_model.model.rinna.layers.7.post_attention_layernorm.bias False
base_model.model.rinna.layers.7.attention.query_key_value.weight False
base_model.model.rinna.layers.7.attention.query_key_value.bias False
base_model.model.rinna.layers.7.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.7.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.7.attention.dense.weight False
base_model.model.rinna.layers.7.attention.dense.bias False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.7.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.7.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.8.input_layernorm.weight False
base_model.model.rinna.layers.8.input_layernorm.bias False
base_model.model.rinna.layers.8.post_attention_layernorm.weight False
base_model.model.rinna.layers.8.post_attention_layernorm.bias False
base_model.model.rinna.layers.8.attention.query_key_value.weight False
base_model.model.rinna.layers.8.attention.query_key_value.bias False
base_model.model.rinna.layers.8.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.8.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.8.attention.dense.weight False
base_model.model.rinna.layers.8.attention.dense.bias False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.8.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.8.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.9.input_layernorm.weight False
base_model.model.rinna.layers.9.input_layernorm.bias False
base_model.model.rinna.layers.9.post_attention_layernorm.weight False
base_model.model.rinna.layers.9.post_attention_layernorm.bias False
base_model.model.rinna.layers.9.attention.query_key_value.weight False
base_model.model.rinna.layers.9.attention.query_key_value.bias False
base_model.model.rinna.layers.9.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.9.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.9.attention.dense.weight False
base_model.model.rinna.layers.9.attention.dense.bias False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.9.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.9.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.10.input_layernorm.weight False
base_model.model.rinna.layers.10.input_layernorm.bias False
base_model.model.rinna.layers.10.post_attention_layernorm.weight False
base_model.model.rinna.layers.10.post_attention_layernorm.bias False
base_model.model.rinna.layers.10.attention.query_key_value.weight False
base_model.model.rinna.layers.10.attention.query_key_value.bias False
base_model.model.rinna.layers.10.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.10.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.10.attention.dense.weight False
base_model.model.rinna.layers.10.attention.dense.bias False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.10.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.10.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.11.input_layernorm.weight False
base_model.model.rinna.layers.11.input_layernorm.bias False
base_model.model.rinna.layers.11.post_attention_layernorm.weight False
base_model.model.rinna.layers.11.post_attention_layernorm.bias False
base_model.model.rinna.layers.11.attention.query_key_value.weight False
base_model.model.rinna.layers.11.attention.query_key_value.bias False
base_model.model.rinna.layers.11.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.11.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.11.attention.dense.weight False
base_model.model.rinna.layers.11.attention.dense.bias False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.11.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.11.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.12.input_layernorm.weight False
base_model.model.rinna.layers.12.input_layernorm.bias False
base_model.model.rinna.layers.12.post_attention_layernorm.weight False
base_model.model.rinna.layers.12.post_attention_layernorm.bias False
base_model.model.rinna.layers.12.attention.query_key_value.weight False
base_model.model.rinna.layers.12.attention.query_key_value.bias False
base_model.model.rinna.layers.12.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.12.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.12.attention.dense.weight False
base_model.model.rinna.layers.12.attention.dense.bias False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.12.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.12.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.13.input_layernorm.weight False
base_model.model.rinna.layers.13.input_layernorm.bias False
base_model.model.rinna.layers.13.post_attention_layernorm.weight False
base_model.model.rinna.layers.13.post_attention_layernorm.bias False
base_model.model.rinna.layers.13.attention.query_key_value.weight False
base_model.model.rinna.layers.13.attention.query_key_value.bias False
base_model.model.rinna.layers.13.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.13.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.13.attention.dense.weight False
base_model.model.rinna.layers.13.attention.dense.bias False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.13.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.13.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.14.input_layernorm.weight False
base_model.model.rinna.layers.14.input_layernorm.bias False
base_model.model.rinna.layers.14.post_attention_layernorm.weight False
base_model.model.rinna.layers.14.post_attention_layernorm.bias False
base_model.model.rinna.layers.14.attention.query_key_value.weight False
base_model.model.rinna.layers.14.attention.query_key_value.bias False
base_model.model.rinna.layers.14.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.14.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.14.attention.dense.weight False
base_model.model.rinna.layers.14.attention.dense.bias False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.14.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.14.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.15.input_layernorm.weight False
base_model.model.rinna.layers.15.input_layernorm.bias False
base_model.model.rinna.layers.15.post_attention_layernorm.weight False
base_model.model.rinna.layers.15.post_attention_layernorm.bias False
base_model.model.rinna.layers.15.attention.query_key_value.weight False
base_model.model.rinna.layers.15.attention.query_key_value.bias False
base_model.model.rinna.layers.15.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.15.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.15.attention.dense.weight False
base_model.model.rinna.layers.15.attention.dense.bias False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.15.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.15.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.16.input_layernorm.weight False
base_model.model.rinna.layers.16.input_layernorm.bias False
base_model.model.rinna.layers.16.post_attention_layernorm.weight False
base_model.model.rinna.layers.16.post_attention_layernorm.bias False
base_model.model.rinna.layers.16.attention.query_key_value.weight False
base_model.model.rinna.layers.16.attention.query_key_value.bias False
base_model.model.rinna.layers.16.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.16.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.16.attention.dense.weight False
base_model.model.rinna.layers.16.attention.dense.bias False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.16.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.16.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.17.input_layernorm.weight False
base_model.model.rinna.layers.17.input_layernorm.bias False
base_model.model.rinna.layers.17.post_attention_layernorm.weight False
base_model.model.rinna.layers.17.post_attention_layernorm.bias False
base_model.model.rinna.layers.17.attention.query_key_value.weight False
base_model.model.rinna.layers.17.attention.query_key_value.bias False
base_model.model.rinna.layers.17.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.17.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.17.attention.dense.weight False
base_model.model.rinna.layers.17.attention.dense.bias False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.17.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.17.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.18.input_layernorm.weight False
base_model.model.rinna.layers.18.input_layernorm.bias False
base_model.model.rinna.layers.18.post_attention_layernorm.weight False
base_model.model.rinna.layers.18.post_attention_layernorm.bias False
base_model.model.rinna.layers.18.attention.query_key_value.weight False
base_model.model.rinna.layers.18.attention.query_key_value.bias False
base_model.model.rinna.layers.18.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.18.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.18.attention.dense.weight False
base_model.model.rinna.layers.18.attention.dense.bias False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.18.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.18.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.19.input_layernorm.weight False
base_model.model.rinna.layers.19.input_layernorm.bias False
base_model.model.rinna.layers.19.post_attention_layernorm.weight False
base_model.model.rinna.layers.19.post_attention_layernorm.bias False
base_model.model.rinna.layers.19.attention.query_key_value.weight False
base_model.model.rinna.layers.19.attention.query_key_value.bias False
base_model.model.rinna.layers.19.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.19.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.19.attention.dense.weight False
base_model.model.rinna.layers.19.attention.dense.bias False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.19.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.19.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.20.input_layernorm.weight False
base_model.model.rinna.layers.20.input_layernorm.bias False
base_model.model.rinna.layers.20.post_attention_layernorm.weight False
base_model.model.rinna.layers.20.post_attention_layernorm.bias False
base_model.model.rinna.layers.20.attention.query_key_value.weight False
base_model.model.rinna.layers.20.attention.query_key_value.bias False
base_model.model.rinna.layers.20.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.20.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.20.attention.dense.weight False
base_model.model.rinna.layers.20.attention.dense.bias False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.20.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.20.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.21.input_layernorm.weight False
base_model.model.rinna.layers.21.input_layernorm.bias False
base_model.model.rinna.layers.21.post_attention_layernorm.weight False
base_model.model.rinna.layers.21.post_attention_layernorm.bias False
base_model.model.rinna.layers.21.attention.query_key_value.weight False
base_model.model.rinna.layers.21.attention.query_key_value.bias False
base_model.model.rinna.layers.21.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.21.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.21.attention.dense.weight False
base_model.model.rinna.layers.21.attention.dense.bias False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.21.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.21.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.22.input_layernorm.weight False
base_model.model.rinna.layers.22.input_layernorm.bias False
base_model.model.rinna.layers.22.post_attention_layernorm.weight False
base_model.model.rinna.layers.22.post_attention_layernorm.bias False
base_model.model.rinna.layers.22.attention.query_key_value.weight False
base_model.model.rinna.layers.22.attention.query_key_value.bias False
base_model.model.rinna.layers.22.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.22.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.22.attention.dense.weight False
base_model.model.rinna.layers.22.attention.dense.bias False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.22.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.22.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.23.input_layernorm.weight False
base_model.model.rinna.layers.23.input_layernorm.bias False
base_model.model.rinna.layers.23.post_attention_layernorm.weight False
base_model.model.rinna.layers.23.post_attention_layernorm.bias False
base_model.model.rinna.layers.23.attention.query_key_value.weight False
base_model.model.rinna.layers.23.attention.query_key_value.bias False
base_model.model.rinna.layers.23.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.23.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.23.attention.dense.weight False
base_model.model.rinna.layers.23.attention.dense.bias False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.23.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.23.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.24.input_layernorm.weight False
base_model.model.rinna.layers.24.input_layernorm.bias False
base_model.model.rinna.layers.24.post_attention_layernorm.weight False
base_model.model.rinna.layers.24.post_attention_layernorm.bias False
base_model.model.rinna.layers.24.attention.query_key_value.weight False
base_model.model.rinna.layers.24.attention.query_key_value.bias False
base_model.model.rinna.layers.24.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.24.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.24.attention.dense.weight False
base_model.model.rinna.layers.24.attention.dense.bias False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.24.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.24.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.25.input_layernorm.weight False
base_model.model.rinna.layers.25.input_layernorm.bias False
base_model.model.rinna.layers.25.post_attention_layernorm.weight False
base_model.model.rinna.layers.25.post_attention_layernorm.bias False
base_model.model.rinna.layers.25.attention.query_key_value.weight False
base_model.model.rinna.layers.25.attention.query_key_value.bias False
base_model.model.rinna.layers.25.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.25.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.25.attention.dense.weight False
base_model.model.rinna.layers.25.attention.dense.bias False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.25.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.25.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.26.input_layernorm.weight False
base_model.model.rinna.layers.26.input_layernorm.bias False
base_model.model.rinna.layers.26.post_attention_layernorm.weight False
base_model.model.rinna.layers.26.post_attention_layernorm.bias False
base_model.model.rinna.layers.26.attention.query_key_value.weight False
base_model.model.rinna.layers.26.attention.query_key_value.bias False
base_model.model.rinna.layers.26.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.26.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.26.attention.dense.weight False
base_model.model.rinna.layers.26.attention.dense.bias False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.26.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.26.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.27.input_layernorm.weight False
base_model.model.rinna.layers.27.input_layernorm.bias False
base_model.model.rinna.layers.27.post_attention_layernorm.weight False
base_model.model.rinna.layers.27.post_attention_layernorm.bias False
base_model.model.rinna.layers.27.attention.query_key_value.weight False
base_model.model.rinna.layers.27.attention.query_key_value.bias False
base_model.model.rinna.layers.27.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.27.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.27.attention.dense.weight False
base_model.model.rinna.layers.27.attention.dense.bias False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.27.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.27.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.28.input_layernorm.weight False
base_model.model.rinna.layers.28.input_layernorm.bias False
base_model.model.rinna.layers.28.post_attention_layernorm.weight False
base_model.model.rinna.layers.28.post_attention_layernorm.bias False
base_model.model.rinna.layers.28.attention.query_key_value.weight False
base_model.model.rinna.layers.28.attention.query_key_value.bias False
base_model.model.rinna.layers.28.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.28.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.28.attention.dense.weight False
base_model.model.rinna.layers.28.attention.dense.bias False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.28.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.28.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.29.input_layernorm.weight False
base_model.model.rinna.layers.29.input_layernorm.bias False
base_model.model.rinna.layers.29.post_attention_layernorm.weight False
base_model.model.rinna.layers.29.post_attention_layernorm.bias False
base_model.model.rinna.layers.29.attention.query_key_value.weight False
base_model.model.rinna.layers.29.attention.query_key_value.bias False
base_model.model.rinna.layers.29.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.29.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.29.attention.dense.weight False
base_model.model.rinna.layers.29.attention.dense.bias False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.29.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.29.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.30.input_layernorm.weight False
base_model.model.rinna.layers.30.input_layernorm.bias False
base_model.model.rinna.layers.30.post_attention_layernorm.weight False
base_model.model.rinna.layers.30.post_attention_layernorm.bias False
base_model.model.rinna.layers.30.attention.query_key_value.weight False
base_model.model.rinna.layers.30.attention.query_key_value.bias False
base_model.model.rinna.layers.30.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.30.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.30.attention.dense.weight False
base_model.model.rinna.layers.30.attention.dense.bias False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.30.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.30.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.31.input_layernorm.weight False
base_model.model.rinna.layers.31.input_layernorm.bias False
base_model.model.rinna.layers.31.post_attention_layernorm.weight False
base_model.model.rinna.layers.31.post_attention_layernorm.bias False
base_model.model.rinna.layers.31.attention.query_key_value.weight False
base_model.model.rinna.layers.31.attention.query_key_value.bias False
base_model.model.rinna.layers.31.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.31.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.31.attention.dense.weight False
base_model.model.rinna.layers.31.attention.dense.bias False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.31.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.31.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.32.input_layernorm.weight False
base_model.model.rinna.layers.32.input_layernorm.bias False
base_model.model.rinna.layers.32.post_attention_layernorm.weight False
base_model.model.rinna.layers.32.post_attention_layernorm.bias False
base_model.model.rinna.layers.32.attention.query_key_value.weight False
base_model.model.rinna.layers.32.attention.query_key_value.bias False
base_model.model.rinna.layers.32.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.32.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.32.attention.dense.weight False
base_model.model.rinna.layers.32.attention.dense.bias False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.32.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.32.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.33.input_layernorm.weight False
base_model.model.rinna.layers.33.input_layernorm.bias False
base_model.model.rinna.layers.33.post_attention_layernorm.weight False
base_model.model.rinna.layers.33.post_attention_layernorm.bias False
base_model.model.rinna.layers.33.attention.query_key_value.weight False
base_model.model.rinna.layers.33.attention.query_key_value.bias False
base_model.model.rinna.layers.33.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.33.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.33.attention.dense.weight False
base_model.model.rinna.layers.33.attention.dense.bias False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.33.mlp.dense_h_to_4h.bias False
/media/takeuchi/HDPH-UT/callum/SpanSRL/Rinna/Main/Train/../../preprocess/base/mk_dataset.py:35: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)
  sentences = torch.tensor(sentences, dtype=torch.long)
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.33.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.34.input_layernorm.weight False
base_model.model.rinna.layers.34.input_layernorm.bias False
base_model.model.rinna.layers.34.post_attention_layernorm.weight False
base_model.model.rinna.layers.34.post_attention_layernorm.bias False
base_model.model.rinna.layers.34.attention.query_key_value.weight False
base_model.model.rinna.layers.34.attention.query_key_value.bias False
base_model.model.rinna.layers.34.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.34.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.34.attention.dense.weight False
base_model.model.rinna.layers.34.attention.dense.bias False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.34.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.34.mlp.dense_4h_to_h.bias False
base_model.model.rinna.layers.35.input_layernorm.weight False
base_model.model.rinna.layers.35.input_layernorm.bias False
base_model.model.rinna.layers.35.post_attention_layernorm.weight False
base_model.model.rinna.layers.35.post_attention_layernorm.bias False
base_model.model.rinna.layers.35.attention.query_key_value.weight False
base_model.model.rinna.layers.35.attention.query_key_value.bias False
base_model.model.rinna.layers.35.attention.query_key_value.lora_A.default.weight True
base_model.model.rinna.layers.35.attention.query_key_value.lora_B.default.weight True
base_model.model.rinna.layers.35.attention.dense.weight False
base_model.model.rinna.layers.35.attention.dense.bias False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.weight False
base_model.model.rinna.layers.35.mlp.dense_h_to_4h.bias False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.weight False
base_model.model.rinna.layers.35.mlp.dense_4h_to_h.bias False
base_model.model.rinna.final_layer_norm.weight False
base_model.model.rinna.final_layer_norm.bias False
base_model.model.my_linear.original_module.weight True
base_model.model.my_linear.original_module.bias True
base_model.model.my_linear.modules_to_save.default.weight True
base_model.model.my_linear.modules_to_save.default.bias True
base_model.model.my_linear2.original_module.weight True
base_model.model.my_linear2.original_module.bias True
base_model.model.my_linear2.modules_to_save.default.weight True
base_model.model.my_linear2.modules_to_save.default.bias True
Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
/media/takeuchi/HDPH-UT/callum/SpanSRL/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
epoch 0 	 loss 1214.6424109777436
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
/media/takeuchi/HDPH-UT/callum/SpanSRL/Rinna/Main/Train/../../utils/evaluate.py:83: RuntimeWarning: invalid value encountered in scalar divide
  df.loc['f1','correct_num'] = 2*df.loc['precision','correct_num']*df.loc['recall','correct_num'] / (df.loc['precision','correct_num'] + df.loc['recall','correct_num'])
epoch 1 	 loss 1031.894218233414
           correct_num  wrong_num  predict_num  ...  precision  recall   f1
Arg                0.0        0.0          0.0  ...        NaN     NaN  NaN
Arg0               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg1               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg2               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg3               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg4               0.0        0.0          0.0  ...        NaN     0.0  NaN
Arg5               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgA               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM               0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM2              0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_ADV           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_AND           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_BUT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CAU           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CND           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_CRT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_DIR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_EXT           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_GOL           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_LOC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MDF           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNR           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_MNS           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_NEG           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_PRX           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_REC           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_SCP           0.0        0.0          0.0  ...        NaN     0.0  NaN
ArgM_SPK           0.0        0.0          0.0  ...        NaN     NaN  NaN
ArgM_TMP           0.0        0.0          0.0  ...        NaN     0.0  NaN
sum                0.0        0.0          1.0  ...        0.0     0.0  0.0
precision          0.0        NaN          NaN  ...        NaN     NaN  NaN
recall             0.0        NaN          NaN  ...        NaN     NaN  NaN
f1                 NaN        NaN          NaN  ...        NaN     NaN  NaN
f1_macro           0.0        NaN          NaN  ...        NaN     NaN  NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 2 	 loss 966.3921085663605
           correct_num  wrong_num  predict_num  ...  precision  recall     f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN    NaN
Arg0            6.0000        0.0          6.0  ...        1.0  0.0035  0.007
Arg1            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN    NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN    NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000    NaN
sum             6.0000        0.0          6.0  ...        1.0  0.0035  0.007
precision       1.0000        NaN          NaN  ...        NaN     NaN    NaN
recall          0.0007        NaN          NaN  ...        NaN     NaN    NaN
f1              0.0013        NaN          NaN  ...        NaN     NaN    NaN
f1_macro        0.0005        NaN          NaN  ...        NaN     NaN    NaN

[36 rows x 8 columns]
Valid f1 =  0.0013 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 3 	 loss 911.8426907201065
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          179.0000       10.0        189.0  ...     0.9471  0.1053  0.1895
Arg1           34.0000        1.0         35.0  ...     0.9714  0.0084  0.0167
Arg2            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum           213.0000       11.0        224.0  ...     1.9185  0.1137  0.2062
precision       0.9509        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0237        NaN          NaN  ...        NaN     NaN     NaN
f1              0.0463        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0133        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.0463 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 4 	 loss 863.6564292678377
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          449.0000       23.0        472.0  ...     0.9513  0.2641  0.4134
Arg1          184.0000       18.0        202.0  ...     0.9109  0.0456  0.0869
Arg2            1.0000        6.0          7.0  ...     0.1429  0.0006  0.0012
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum           634.0000       47.0        681.0  ...     2.0050  0.3104  0.5016
precision       0.9310        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0707        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1314        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0324        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1314 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 5 	 loss 824.5540274665109
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          474.0000       27.0        501.0  ...     0.9461  0.2788  0.4307
Arg1          229.0000       21.0        250.0  ...     0.9160  0.0568  0.1069
Arg2            1.0000        6.0          7.0  ...     0.1429  0.0006  0.0012
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum           704.0000       54.0        758.0  ...     2.0050  0.3362  0.5389
precision       0.9288        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0785        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1448        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0348        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1448 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 6 	 loss 793.88005134993
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          534.0000       24.0        558.0  ...     0.9570  0.3141  0.4730
Arg1          290.0000       30.0        320.0  ...     0.9062  0.0719  0.1332
Arg2            2.0000        7.0          9.0  ...     0.2222  0.0012  0.0025
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
sum           826.0000       61.0        887.0  ...     2.0855  0.3873  0.6087
precision       0.9312        NaN          NaN  ...        NaN     NaN     NaN
recall          0.0921        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1676        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0393        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1676 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 7 	 loss 772.4254099109676
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          604.0000       47.0        651.0  ...     0.9278  0.3553  0.5138
Arg1          379.0000       61.0        440.0  ...     0.8614  0.0940  0.1695
Arg2            5.0000       13.0         18.0  ...     0.2778  0.0031  0.0061
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        2.0000        1.0          3.0  ...     0.6667  0.0123  0.0241
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_MNS        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_NEG        1.0000        0.0          1.0  ...     1.0000  0.0127  0.0250
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
sum           991.0000      126.0       1117.0  ...     3.7336  0.4773  0.7385
precision       0.8872        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1105        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1965        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0476        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
Valid f1 =  0.1965 

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
Progress 14400 / 42016
Progress 16000 / 42016
Progress 17600 / 42016
Progress 19200 / 42016
Progress 20800 / 42016
Progress 22400 / 42016
Progress 24000 / 42016
Progress 25600 / 42016
Progress 27200 / 42016
Progress 28800 / 42016
Progress 30400 / 42016
Progress 32000 / 42016
Progress 33600 / 42016
Progress 35200 / 42016
Progress 36800 / 42016
Progress 38400 / 42016
Progress 40000 / 42016
Progress 41600 / 42016
epoch 8 	 loss 755.160013378365
           correct_num  wrong_num  predict_num  ...  precision  recall      f1
Arg             0.0000        0.0          0.0  ...        NaN     NaN     NaN
Arg0          597.0000       43.0        640.0  ...     0.9328  0.3512  0.5103
Arg1          353.0000       52.0        405.0  ...     0.8716  0.0875  0.1591
Arg2            7.0000        6.0         13.0  ...     0.5385  0.0043  0.0086
Arg3            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg4            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
Arg5            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgA            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM            0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM2           0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_ADV        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_AND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_BUT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CAU        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CMP        0.0000        1.0          1.0  ...     0.0000  0.0000     NaN
ArgM_CND        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_CRT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_DIR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_EXT        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_GOL        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_LOC        1.0000        1.0          2.0  ...     0.5000  0.0061  0.0121
ArgM_MDF        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNR        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_MNS        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_NEG        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_PRX        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_REC        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SCP        0.0000        0.0          0.0  ...        NaN  0.0000     NaN
ArgM_SPK        0.0000        0.0          0.0  ...        NaN     NaN     NaN
ArgM_TMP        1.0000        1.0          2.0  ...     0.5000  0.0046  0.0091
sum           959.0000      105.0       1064.0  ...     3.3429  0.4538  0.6991
precision       0.9013        NaN          NaN  ...        NaN     NaN     NaN
recall          0.1069        NaN          NaN  ...        NaN     NaN     NaN
f1              0.1912        NaN          NaN  ...        NaN     NaN     NaN
f1_macro        0.0451        NaN          NaN  ...        NaN     NaN     NaN

[36 rows x 8 columns]
No change in valid f1

Progress 0 / 42016
Progress 1600 / 42016
Progress 3200 / 42016
Progress 4800 / 42016
Progress 6400 / 42016
Progress 8000 / 42016
Progress 9600 / 42016
Progress 11200 / 42016
Progress 12800 / 42016
